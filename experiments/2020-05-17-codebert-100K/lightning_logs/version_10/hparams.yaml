lr: 0.0001
num_attention_heads: 12
num_hidden_layers: 2
power: 1.0
tokenizer_path: ./dataset
train_batch_size: 8
train_data_path: ./dataset/all.src
val_data_path: ./dataset/valid.src
warmup_steps: 1000
weight_decay: 0.1
