{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydriller import RepositoryMining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "commit #1, \n",
      "message: Initial commit\n",
      "__Before__: \n",
      "None\n",
      "__After__: \n",
      "# TensorFlow Models\n",
      "\n",
      "@@ -0,0 +1 @@\n",
      "+# TensorFlow Models\n",
      "\n",
      "\n",
      "commit #2, \n",
      "message: Adding Neural GPU code.\n",
      "__Before__: \n",
      "None\n",
      "__After__: \n",
      "py_library(\n",
      "    name = \"data_utils\",\n",
      "    srcs = [\n",
      "        \"data_utils.py\",\n",
      "    ],\n",
      "    deps = [\n",
      "        \"//file/colossus/public:cns\",\n",
      "        \"//third_party/py/numpy\",\n",
      "        \"//third_party/py/tensorflow\",\n",
      "    ],\n",
      ")\n",
      "\n",
      "py_library(\n",
      "    name = \"neural_gpu\",\n",
      "    srcs = [\n",
      "        \"neural_gpu.py\",\n",
      "    ],\n",
      "    deps = [\n",
      "        \":data_utils\",\n",
      "        \"//third_party/py/numpy\",\n",
      "        \"//third_party/py/tensorflow\",\n",
      "    ],\n",
      ")\n",
      "\n",
      "py_binary(\n",
      "    name = \"neural_gpu_trainer\",\n",
      "    srcs = [\n",
      "        \"neural_gpu_trainer.py\",\n",
      "    ],\n",
      "    launcher = \"//devtools/python/launcher\",\n",
      "    malloc = \"//tcmalloc:tcmalloc_or_debug\",\n",
      "    deps = [\n",
      "        \":neural_gpu\",\n",
      "        \"//file/colossus/public:cns\",\n",
      "        \"//net/proto2/python/public:use_fast_cpp_protos\",\n",
      "        \"//third_party/py/Tkinter\",\n",
      "        \"//third_party/py/matplotlib\",\n",
      "        \"//third_party/py/numpy\",\n",
      "        \"//third_party/py/tensorflow\",\n",
      "    ],\n",
      ")\n",
      "\n",
      "@@ -0,0 +1,41 @@\n",
      "+py_library(\n",
      "+    name = \"data_utils\",\n",
      "+    srcs = [\n",
      "+        \"data_utils.py\",\n",
      "+    ],\n",
      "+    deps = [\n",
      "+        \"//file/colossus/public:cns\",\n",
      "+        \"//third_party/py/numpy\",\n",
      "+        \"//third_party/py/tensorflow\",\n",
      "+    ],\n",
      "+)\n",
      "+\n",
      "+py_library(\n",
      "+    name = \"neural_gpu\",\n",
      "+    srcs = [\n",
      "+        \"neural_gpu.py\",\n",
      "+    ],\n",
      "+    deps = [\n",
      "+        \":data_utils\",\n",
      "+        \"//third_party/py/numpy\",\n",
      "+        \"//third_party/py/tensorflow\",\n",
      "+    ],\n",
      "+)\n",
      "+\n",
      "+py_binary(\n",
      "+    name = \"neural_gpu_trainer\",\n",
      "+    srcs = [\n",
      "+        \"neural_gpu_trainer.py\",\n",
      "+    ],\n",
      "+    launcher = \"//devtools/python/launcher\",\n",
      "+    malloc = \"//tcmalloc:tcmalloc_or_debug\",\n",
      "+    deps = [\n",
      "+        \":neural_gpu\",\n",
      "+        \"//file/colossus/public:cns\",\n",
      "+        \"//net/proto2/python/public:use_fast_cpp_protos\",\n",
      "+        \"//third_party/py/Tkinter\",\n",
      "+        \"//third_party/py/matplotlib\",\n",
      "+        \"//third_party/py/numpy\",\n",
      "+        \"//third_party/py/tensorflow\",\n",
      "+    ],\n",
      "+)\n",
      "\n",
      "\n",
      "__Before__: \n",
      "None\n",
      "__After__: \n",
      "# NeuralGPU\n",
      "Code for the Neural GPU model as described\n",
      "in [[http://arxiv.org/abs/1511.08228]].\n",
      "\n",
      "\n",
      "@@ -0,0 +1,4 @@\n",
      "+# NeuralGPU\n",
      "+Code for the Neural GPU model as described\n",
      "+in [[http://arxiv.org/abs/1511.08228]].\n",
      "+\n",
      "\n",
      "\n",
      "__Before__: \n",
      "None\n",
      "__After__: \n",
      "\"\"\"Convolutional Gated Recurrent Networks for Algorithm Learning.\"\"\"\n",
      "\n",
      "import math\n",
      "import random\n",
      "import sys\n",
      "import time\n",
      "\n",
      "import google3\n",
      "\n",
      "import numpy as np\n",
      "import tensorflow as tf\n",
      "\n",
      "from google3.third_party.tensorflow.python.platform import gfile\n",
      "\n",
      "FLAGS = tf.app.flags.FLAGS\n",
      "\n",
      "bins = [8, 16, 32, 64, 128]\n",
      "all_tasks = [\"sort\", \"id\", \"rev\", \"incr\", \"left\", \"right\", \"left-shift\", \"add\",\n",
      "             \"right-shift\", \"bmul\", \"dup\", \"badd\", \"qadd\"]\n",
      "forward_max = 128\n",
      "log_filename = \"\"\n",
      "\n",
      "\n",
      "def pad(l):\n",
      "  for b in bins:\n",
      "    if b >= l: return b\n",
      "  return forward_max\n",
      "\n",
      "\n",
      "train_set = {}\n",
      "test_set = {}\n",
      "for some_task in all_tasks:\n",
      "  train_set[some_task] = []\n",
      "  test_set[some_task] = []\n",
      "  for all_max_len in xrange(10000):\n",
      "    train_set[some_task].append([])\n",
      "    test_set[some_task].append([])\n",
      "\n",
      "\n",
      "def add(n1, n2, base=10):\n",
      "  \"\"\"Add two numbers represented as lower-endian digit lists.\"\"\"\n",
      "  k = max(len(n1), len(n2)) + 1\n",
      "  d1 = n1 + [0 for _ in xrange(k - len(n1))]\n",
      "  d2 = n2 + [0 for _ in xrange(k - len(n2))]\n",
      "  res = []\n",
      "  carry = 0\n",
      "  for i in xrange(k):\n",
      "    if d1[i] + d2[i] + carry < base:\n",
      "      res.append(d1[i] + d2[i] + carry)\n",
      "      carry = 0\n",
      "    else:\n",
      "      res.append(d1[i] + d2[i] + carry - base)\n",
      "      carry = 1\n",
      "  while res and res[-1] == 0:\n",
      "    res = res[:-1]\n",
      "  if res: return res\n",
      "  return [0]\n",
      "\n",
      "\n",
      "def init_data(task, length, nbr_cases, nclass):\n",
      "  \"\"\"Data initialization.\"\"\"\n",
      "  def rand_pair(l, task):\n",
      "    \"\"\"Random data pair for a task. Total length should be <= l.\"\"\"\n",
      "    k = (l-1)/2\n",
      "    base = 10\n",
      "    if task[0] == \"b\": base = 2\n",
      "    if task[0] == \"q\": base = 4\n",
      "    d1 = [np.random.randint(base) for _ in xrange(k)]\n",
      "    d2 = [np.random.randint(base) for _ in xrange(k)]\n",
      "    if task in [\"add\", \"badd\", \"qadd\"]:\n",
      "      res = add(d1, d2, base)\n",
      "    elif task in [\"bmul\"]:\n",
      "      d1n = sum([d * (base ** i) for i, d in enumerate(d1)])\n",
      "      d2n = sum([d * (base ** i) for i, d in enumerate(d2)])\n",
      "      res = [int(x) for x in list(reversed(str(bin(d1n * d2n))))[:-2]]\n",
      "    else:\n",
      "      sys.exit()\n",
      "    sep = [12]\n",
      "    if task in [\"add\", \"badd\", \"qadd\"]: sep = [11]\n",
      "    inp = [d + 1 for d in d1] + sep + [d + 1 for d in d2]\n",
      "    return inp, [r + 1 for r in res]\n",
      "\n",
      "  def rand_dup_pair(l):\n",
      "    \"\"\"Random data pair for duplication task. Total length should be <= l.\"\"\"\n",
      "    k = l/2\n",
      "    x = [np.random.randint(nclass - 1) + 1 for _ in xrange(k)]\n",
      "    inp = x + [0 for _ in xrange(l - k)]\n",
      "    res = x + x + [0 for _ in xrange(l - 2*k)]\n",
      "    return inp, res\n",
      "\n",
      "  def spec(inp):\n",
      "    \"\"\"Return the target given the input for some tasks.\"\"\"\n",
      "    if task == \"sort\":\n",
      "      return sorted(inp)\n",
      "    elif task == \"id\":\n",
      "      return inp\n",
      "    elif task == \"rev\":\n",
      "      return [i for i in reversed(inp)]\n",
      "    elif task == \"incr\":\n",
      "      carry = 1\n",
      "      res = []\n",
      "      for i in xrange(len(inp)):\n",
      "        if inp[i] + carry < nclass:\n",
      "          res.append(inp[i] + carry)\n",
      "          carry = 0\n",
      "        else:\n",
      "          res.append(1)\n",
      "          carry = 1\n",
      "      return res\n",
      "    elif task == \"left\":\n",
      "      return [inp[0]]\n",
      "    elif task == \"right\":\n",
      "      return [inp[-1]]\n",
      "    elif task == \"left-shift\":\n",
      "      return [inp[l-1] for l in xrange(len(inp))]\n",
      "    elif task == \"right-shift\":\n",
      "      return [inp[l+1] for l in xrange(len(inp))]\n",
      "    else:\n",
      "      print_out(\"Unknown spec for task \" + str(task))\n",
      "      sys.exit()\n",
      "\n",
      "  l = length\n",
      "  cur_time = time.time()\n",
      "  total_time = 0.0\n",
      "  for case in xrange(nbr_cases):\n",
      "    total_time += time.time() - cur_time\n",
      "    cur_time = time.time()\n",
      "    if l > 10000 and case % 100 == 1:\n",
      "      print_out(\"  avg gen time %.4f s\" % (total_time / float(case)))\n",
      "    if task in [\"add\", \"badd\", \"qadd\", \"bmul\"]:\n",
      "      i, t = rand_pair(l, task)\n",
      "      train_set[task][len(i)].append([i, t])\n",
      "      i, t = rand_pair(l, task)\n",
      "      test_set[task][len(i)].append([i, t])\n",
      "    elif task == \"dup\":\n",
      "      i, t = rand_dup_pair(l)\n",
      "      train_set[task][len(i)].append([i, t])\n",
      "      i, t = rand_dup_pair(l)\n",
      "      test_set[task][len(i)].append([i, t])\n",
      "    else:\n",
      "      inp = [np.random.randint(nclass - 1) + 1 for i in xrange(l)]\n",
      "      target = spec(inp)\n",
      "      train_set[task][l].append([inp, target])\n",
      "      inp = [np.random.randint(nclass - 1) + 1 for i in xrange(l)]\n",
      "      target = spec(inp)\n",
      "      test_set[task][l].append([inp, target])\n",
      "\n",
      "\n",
      "def get_batch(max_length, batch_size, do_train, task, offset=None, preset=None):\n",
      "  \"\"\"Get a batch of data, training or testing.\"\"\"\n",
      "  inputs = []\n",
      "  targets = []\n",
      "  length = max_length\n",
      "  if preset is None:\n",
      "    cur_set = test_set[task]\n",
      "    if do_train: cur_set = train_set[task]\n",
      "    while not cur_set[length]:\n",
      "      length -= 1\n",
      "  pad_length = pad(length)\n",
      "  for b in xrange(batch_size):\n",
      "    if preset is None:\n",
      "      elem = random.choice(cur_set[length])\n",
      "      if offset is not None and offset + b < len(cur_set[length]):\n",
      "        elem = cur_set[length][offset + b]\n",
      "    else:\n",
      "      elem = preset\n",
      "    inp, target = elem[0], elem[1]\n",
      "    assert len(inp) == length\n",
      "    inputs.append(inp + [0 for l in xrange(pad_length - len(inp))])\n",
      "    targets.append(target + [0 for l in xrange(pad_length - len(target))])\n",
      "  res_input = []\n",
      "  res_target = []\n",
      "  for l in xrange(pad_length):\n",
      "    new_input = np.array([inputs[b][l] for b in xrange(batch_size)],\n",
      "                         dtype=np.int32)\n",
      "    new_target = np.array([targets[b][l] for b in xrange(batch_size)],\n",
      "                          dtype=np.int32)\n",
      "    res_input.append(new_input)\n",
      "    res_target.append(new_target)\n",
      "  return res_input, res_target\n",
      "\n",
      "\n",
      "def print_out(s, newline=True):\n",
      "  \"\"\"Print a message out and log it to file.\"\"\"\n",
      "  if log_filename:\n",
      "    try:\n",
      "      with gfile.GFile(log_filename, mode=\"a\") as f:\n",
      "        f.write(s + (\"\\n\" if newline else \"\"))\n",
      "    # pylint: disable=bare-except\n",
      "    except:\n",
      "      sys.stdout.write(\"Error appending to %s\\n\" % log_filename)\n",
      "  sys.stdout.write(s + (\"\\n\" if newline else \"\"))\n",
      "  sys.stdout.flush()\n",
      "\n",
      "\n",
      "def decode(output):\n",
      "  return [np.argmax(o, axis=1) for o in output]\n",
      "\n",
      "\n",
      "def accuracy(inpt, output, target, batch_size, nprint):\n",
      "  \"\"\"Calculate output accuracy given target.\"\"\"\n",
      "  assert nprint < batch_size + 1\n",
      "  def task_print(inp, output, target):\n",
      "    stop_bound = 0\n",
      "    print_len = 0\n",
      "    while print_len < len(target) and target[print_len] > stop_bound:\n",
      "      print_len += 1\n",
      "    print_out(\"    i: \" + \" \".join([str(i - 1) for i in inp if i > 0]))\n",
      "    print_out(\"    o: \" +\n",
      "              \" \".join([str(output[l] - 1) for l in xrange(print_len)]))\n",
      "    print_out(\"    t: \" +\n",
      "              \" \".join([str(target[l] - 1) for l in xrange(print_len)]))\n",
      "  decoded_target = target\n",
      "  decoded_output = decode(output)\n",
      "  total = 0\n",
      "  errors = 0\n",
      "  seq = [0 for b in xrange(batch_size)]\n",
      "  for l in xrange(len(decoded_output)):\n",
      "    for b in xrange(batch_size):\n",
      "      if decoded_target[l][b] > 0:\n",
      "        total += 1\n",
      "        if decoded_output[l][b] != decoded_target[l][b]:\n",
      "          seq[b] = 1\n",
      "          errors += 1\n",
      "  e = 0  # Previous error index\n",
      "  for _ in xrange(min(nprint, sum(seq))):\n",
      "    while seq[e] == 0:\n",
      "      e += 1\n",
      "    task_print([inpt[l][e] for l in xrange(len(inpt))],\n",
      "               [decoded_output[l][e] for l in xrange(len(decoded_target))],\n",
      "               [decoded_target[l][e] for l in xrange(len(decoded_target))])\n",
      "    e += 1\n",
      "  for b in xrange(nprint - errors):\n",
      "    task_print([inpt[l][b] for l in xrange(len(inpt))],\n",
      "               [decoded_output[l][b] for l in xrange(len(decoded_target))],\n",
      "               [decoded_target[l][b] for l in xrange(len(decoded_target))])\n",
      "  return errors, total, sum(seq)\n",
      "\n",
      "\n",
      "def safe_exp(x):\n",
      "  perp = 10000\n",
      "  if x < 100: perp = math.exp(x)\n",
      "  if perp > 10000: return 10000\n",
      "  return perp\n",
      "\n",
      "@@ -0,0 +1,244 @@\n",
      "+\"\"\"Convolutional Gated Recurrent Networks for Algorithm Learning.\"\"\"\n",
      "+\n",
      "+import math\n",
      "+import random\n",
      "+import sys\n",
      "+import time\n",
      "+\n",
      "+import google3\n",
      "+\n",
      "+import numpy as np\n",
      "+import tensorflow as tf\n",
      "+\n",
      "+from google3.third_party.tensorflow.python.platform import gfile\n",
      "+\n",
      "+FLAGS = tf.app.flags.FLAGS\n",
      "+\n",
      "+bins = [8, 16, 32, 64, 128]\n",
      "+all_tasks = [\"sort\", \"id\", \"rev\", \"incr\", \"left\", \"right\", \"left-shift\", \"add\",\n",
      "+             \"right-shift\", \"bmul\", \"dup\", \"badd\", \"qadd\"]\n",
      "+forward_max = 128\n",
      "+log_filename = \"\"\n",
      "+\n",
      "+\n",
      "+def pad(l):\n",
      "+  for b in bins:\n",
      "+    if b >= l: return b\n",
      "+  return forward_max\n",
      "+\n",
      "+\n",
      "+train_set = {}\n",
      "+test_set = {}\n",
      "+for some_task in all_tasks:\n",
      "+  train_set[some_task] = []\n",
      "+  test_set[some_task] = []\n",
      "+  for all_max_len in xrange(10000):\n",
      "+    train_set[some_task].append([])\n",
      "+    test_set[some_task].append([])\n",
      "+\n",
      "+\n",
      "+def add(n1, n2, base=10):\n",
      "+  \"\"\"Add two numbers represented as lower-endian digit lists.\"\"\"\n",
      "+  k = max(len(n1), len(n2)) + 1\n",
      "+  d1 = n1 + [0 for _ in xrange(k - len(n1))]\n",
      "+  d2 = n2 + [0 for _ in xrange(k - len(n2))]\n",
      "+  res = []\n",
      "+  carry = 0\n",
      "+  for i in xrange(k):\n",
      "+    if d1[i] + d2[i] + carry < base:\n",
      "+      res.append(d1[i] + d2[i] + carry)\n",
      "+      carry = 0\n",
      "+    else:\n",
      "+      res.append(d1[i] + d2[i] + carry - base)\n",
      "+      carry = 1\n",
      "+  while res and res[-1] == 0:\n",
      "+    res = res[:-1]\n",
      "+  if res: return res\n",
      "+  return [0]\n",
      "+\n",
      "+\n",
      "+def init_data(task, length, nbr_cases, nclass):\n",
      "+  \"\"\"Data initialization.\"\"\"\n",
      "+  def rand_pair(l, task):\n",
      "+    \"\"\"Random data pair for a task. Total length should be <= l.\"\"\"\n",
      "+    k = (l-1)/2\n",
      "+    base = 10\n",
      "+    if task[0] == \"b\": base = 2\n",
      "+    if task[0] == \"q\": base = 4\n",
      "+    d1 = [np.random.randint(base) for _ in xrange(k)]\n",
      "+    d2 = [np.random.randint(base) for _ in xrange(k)]\n",
      "+    if task in [\"add\", \"badd\", \"qadd\"]:\n",
      "+      res = add(d1, d2, base)\n",
      "+    elif task in [\"bmul\"]:\n",
      "+      d1n = sum([d * (base ** i) for i, d in enumerate(d1)])\n",
      "+      d2n = sum([d * (base ** i) for i, d in enumerate(d2)])\n",
      "+      res = [int(x) for x in list(reversed(str(bin(d1n * d2n))))[:-2]]\n",
      "+    else:\n",
      "+      sys.exit()\n",
      "+    sep = [12]\n",
      "+    if task in [\"add\", \"badd\", \"qadd\"]: sep = [11]\n",
      "+    inp = [d + 1 for d in d1] + sep + [d + 1 for d in d2]\n",
      "+    return inp, [r + 1 for r in res]\n",
      "+\n",
      "+  def rand_dup_pair(l):\n",
      "+    \"\"\"Random data pair for duplication task. Total length should be <= l.\"\"\"\n",
      "+    k = l/2\n",
      "+    x = [np.random.randint(nclass - 1) + 1 for _ in xrange(k)]\n",
      "+    inp = x + [0 for _ in xrange(l - k)]\n",
      "+    res = x + x + [0 for _ in xrange(l - 2*k)]\n",
      "+    return inp, res\n",
      "+\n",
      "+  def spec(inp):\n",
      "+    \"\"\"Return the target given the input for some tasks.\"\"\"\n",
      "+    if task == \"sort\":\n",
      "+      return sorted(inp)\n",
      "+    elif task == \"id\":\n",
      "+      return inp\n",
      "+    elif task == \"rev\":\n",
      "+      return [i for i in reversed(inp)]\n",
      "+    elif task == \"incr\":\n",
      "+      carry = 1\n",
      "+      res = []\n",
      "+      for i in xrange(len(inp)):\n",
      "+        if inp[i] + carry < nclass:\n",
      "+          res.append(inp[i] + carry)\n",
      "+          carry = 0\n",
      "+        else:\n",
      "+          res.append(1)\n",
      "+          carry = 1\n",
      "+      return res\n",
      "+    elif task == \"left\":\n",
      "+      return [inp[0]]\n",
      "+    elif task == \"right\":\n",
      "+      return [inp[-1]]\n",
      "+    elif task == \"left-shift\":\n",
      "+      return [inp[l-1] for l in xrange(len(inp))]\n",
      "+    elif task == \"right-shift\":\n",
      "+      return [inp[l+1] for l in xrange(len(inp))]\n",
      "+    else:\n",
      "+      print_out(\"Unknown spec for task \" + str(task))\n",
      "+      sys.exit()\n",
      "+\n",
      "+  l = length\n",
      "+  cur_time = time.time()\n",
      "+  total_time = 0.0\n",
      "+  for case in xrange(nbr_cases):\n",
      "+    total_time += time.time() - cur_time\n",
      "+    cur_time = time.time()\n",
      "+    if l > 10000 and case % 100 == 1:\n",
      "+      print_out(\"  avg gen time %.4f s\" % (total_time / float(case)))\n",
      "+    if task in [\"add\", \"badd\", \"qadd\", \"bmul\"]:\n",
      "+      i, t = rand_pair(l, task)\n",
      "+      train_set[task][len(i)].append([i, t])\n",
      "+      i, t = rand_pair(l, task)\n",
      "+      test_set[task][len(i)].append([i, t])\n",
      "+    elif task == \"dup\":\n",
      "+      i, t = rand_dup_pair(l)\n",
      "+      train_set[task][len(i)].append([i, t])\n",
      "+      i, t = rand_dup_pair(l)\n",
      "+      test_set[task][len(i)].append([i, t])\n",
      "+    else:\n",
      "+      inp = [np.random.randint(nclass - 1) + 1 for i in xrange(l)]\n",
      "+      target = spec(inp)\n",
      "+      train_set[task][l].append([inp, target])\n",
      "+      inp = [np.random.randint(nclass - 1) + 1 for i in xrange(l)]\n",
      "+      target = spec(inp)\n",
      "+      test_set[task][l].append([inp, target])\n",
      "+\n",
      "+\n",
      "+def get_batch(max_length, batch_size, do_train, task, offset=None, preset=None):\n",
      "+  \"\"\"Get a batch of data, training or testing.\"\"\"\n",
      "+  inputs = []\n",
      "+  targets = []\n",
      "+  length = max_length\n",
      "+  if preset is None:\n",
      "+    cur_set = test_set[task]\n",
      "+    if do_train: cur_set = train_set[task]\n",
      "+    while not cur_set[length]:\n",
      "+      length -= 1\n",
      "+  pad_length = pad(length)\n",
      "+  for b in xrange(batch_size):\n",
      "+    if preset is None:\n",
      "+      elem = random.choice(cur_set[length])\n",
      "+      if offset is not None and offset + b < len(cur_set[length]):\n",
      "+        elem = cur_set[length][offset + b]\n",
      "+    else:\n",
      "+      elem = preset\n",
      "+    inp, target = elem[0], elem[1]\n",
      "+    assert len(inp) == length\n",
      "+    inputs.append(inp + [0 for l in xrange(pad_length - len(inp))])\n",
      "+    targets.append(target + [0 for l in xrange(pad_length - len(target))])\n",
      "+  res_input = []\n",
      "+  res_target = []\n",
      "+  for l in xrange(pad_length):\n",
      "+    new_input = np.array([inputs[b][l] for b in xrange(batch_size)],\n",
      "+                         dtype=np.int32)\n",
      "+    new_target = np.array([targets[b][l] for b in xrange(batch_size)],\n",
      "+                          dtype=np.int32)\n",
      "+    res_input.append(new_input)\n",
      "+    res_target.append(new_target)\n",
      "+  return res_input, res_target\n",
      "+\n",
      "+\n",
      "+def print_out(s, newline=True):\n",
      "+  \"\"\"Print a message out and log it to file.\"\"\"\n",
      "+  if log_filename:\n",
      "+    try:\n",
      "+      with gfile.GFile(log_filename, mode=\"a\") as f:\n",
      "+        f.write(s + (\"\\n\" if newline else \"\"))\n",
      "+    # pylint: disable=bare-except\n",
      "+    except:\n",
      "+      sys.stdout.write(\"Error appending to %s\\n\" % log_filename)\n",
      "+  sys.stdout.write(s + (\"\\n\" if newline else \"\"))\n",
      "+  sys.stdout.flush()\n",
      "+\n",
      "+\n",
      "+def decode(output):\n",
      "+  return [np.argmax(o, axis=1) for o in output]\n",
      "+\n",
      "+\n",
      "+def accuracy(inpt, output, target, batch_size, nprint):\n",
      "+  \"\"\"Calculate output accuracy given target.\"\"\"\n",
      "+  assert nprint < batch_size + 1\n",
      "+  def task_print(inp, output, target):\n",
      "+    stop_bound = 0\n",
      "+    print_len = 0\n",
      "+    while print_len < len(target) and target[print_len] > stop_bound:\n",
      "+      print_len += 1\n",
      "+    print_out(\"    i: \" + \" \".join([str(i - 1) for i in inp if i > 0]))\n",
      "+    print_out(\"    o: \" +\n",
      "+              \" \".join([str(output[l] - 1) for l in xrange(print_len)]))\n",
      "+    print_out(\"    t: \" +\n",
      "+              \" \".join([str(target[l] - 1) for l in xrange(print_len)]))\n",
      "+  decoded_target = target\n",
      "+  decoded_output = decode(output)\n",
      "+  total = 0\n",
      "+  errors = 0\n",
      "+  seq = [0 for b in xrange(batch_size)]\n",
      "+  for l in xrange(len(decoded_output)):\n",
      "+    for b in xrange(batch_size):\n",
      "+      if decoded_target[l][b] > 0:\n",
      "+        total += 1\n",
      "+        if decoded_output[l][b] != decoded_target[l][b]:\n",
      "+          seq[b] = 1\n",
      "+          errors += 1\n",
      "+  e = 0  # Previous error index\n",
      "+  for _ in xrange(min(nprint, sum(seq))):\n",
      "+    while seq[e] == 0:\n",
      "+      e += 1\n",
      "+    task_print([inpt[l][e] for l in xrange(len(inpt))],\n",
      "+               [decoded_output[l][e] for l in xrange(len(decoded_target))],\n",
      "+               [decoded_target[l][e] for l in xrange(len(decoded_target))])\n",
      "+    e += 1\n",
      "+  for b in xrange(nprint - errors):\n",
      "+    task_print([inpt[l][b] for l in xrange(len(inpt))],\n",
      "+               [decoded_output[l][b] for l in xrange(len(decoded_target))],\n",
      "+               [decoded_target[l][b] for l in xrange(len(decoded_target))])\n",
      "+  return errors, total, sum(seq)\n",
      "+\n",
      "+\n",
      "+def safe_exp(x):\n",
      "+  perp = 10000\n",
      "+  if x < 100: perp = math.exp(x)\n",
      "+  if perp > 10000: return 10000\n",
      "+  return perp\n",
      "\n",
      "\n",
      "__Before__: \n",
      "None\n",
      "__After__: \n",
      "\"\"\"The Neural GPU Model.\"\"\"\n",
      "\n",
      "import time\n",
      "\n",
      "import google3\n",
      "\n",
      "import tensorflow as tf\n",
      "\n",
      "from google3.experimental.users.lukaszkaiser.neural_gpu import data_utils\n",
      "\n",
      "\n",
      "def conv_linear(args, kw, kh, nin, nout, do_bias, bias_start, prefix):\n",
      "  \"\"\"Convolutional linear map.\"\"\"\n",
      "  assert args\n",
      "  if not isinstance(args, (list, tuple)):\n",
      "    args = [args]\n",
      "  with tf.variable_scope(prefix):\n",
      "    k = tf.get_variable(\"CvK\", [kw, kh, nin, nout])\n",
      "    if len(args) == 1:\n",
      "      res = tf.nn.conv2d(args[0], k, [1, 1, 1, 1], \"SAME\")\n",
      "    else:\n",
      "      res = tf.nn.conv2d(tf.concat(3, args), k, [1, 1, 1, 1], \"SAME\")\n",
      "    if not do_bias: return res\n",
      "    bias_term = tf.get_variable(\"CvB\", [nout],\n",
      "                                initializer=tf.constant_initializer(0.0))\n",
      "    return res + bias_term + bias_start\n",
      "\n",
      "\n",
      "def sigmoid_cutoff(x, cutoff):\n",
      "  \"\"\"Sigmoid with cutoff, e.g., 1.2sigmoid(x) - 0.1.\"\"\"\n",
      "  y = tf.sigmoid(x)\n",
      "  if cutoff < 1.01: return y\n",
      "  d = (cutoff - 1.0) / 2.0\n",
      "  return tf.minimum(1.0, tf.maximum(0.0, cutoff * y - d))\n",
      "\n",
      "\n",
      "def conv_gru(inpts, mem, kw, kh, nmaps, cutoff, prefix):\n",
      "  \"\"\"Convolutional GRU.\"\"\"\n",
      "  def conv_lin(args, suffix, bias_start):\n",
      "    return conv_linear(args, kw, kh, len(args) * nmaps, nmaps, True, bias_start,\n",
      "                       prefix + \"/\" + suffix)\n",
      "  reset = sigmoid_cutoff(conv_lin(inpts + [mem], \"r\", 1.0), cutoff)\n",
      "  candidate = tf.tanh(conv_lin(inpts + [reset * mem], \"c\", 0.0))\n",
      "  gate = sigmoid_cutoff(conv_lin(inpts + [mem], \"g\", 1.0), cutoff)\n",
      "  return gate * mem + (1 - gate) * candidate\n",
      "\n",
      "\n",
      "def relaxed_average(var_name_suffix, rx_step):\n",
      "  \"\"\"Calculate the average of relaxed variables having var_name_suffix.\"\"\"\n",
      "  relaxed_vars = []\n",
      "  for l in xrange(rx_step):\n",
      "    with tf.variable_scope(\"RX%d\" % l, reuse=True):\n",
      "      try:\n",
      "        relaxed_vars.append(tf.get_variable(var_name_suffix))\n",
      "      except ValueError:\n",
      "        pass\n",
      "  dsum = tf.add_n(relaxed_vars)\n",
      "  avg = dsum / len(relaxed_vars)\n",
      "  diff = [v - avg for v in relaxed_vars]\n",
      "  davg = tf.add_n([d*d for d in diff])\n",
      "  return avg, tf.reduce_sum(davg)\n",
      "\n",
      "\n",
      "def relaxed_distance(rx_step):\n",
      "  \"\"\"Distance between relaxed variables and their average.\"\"\"\n",
      "  res, ops, rx_done = [], [], {}\n",
      "  for v in tf.trainable_variables():\n",
      "    if v.name[0:2] == \"RX\":\n",
      "      rx_name = v.op.name[v.name.find(\"/\") + 1:]\n",
      "      if rx_name not in rx_done:\n",
      "        avg, dist_loss = relaxed_average(rx_name, rx_step)\n",
      "        res.append(dist_loss)\n",
      "        rx_done[rx_name] = avg\n",
      "      ops.append(v.assign(rx_done[rx_name]))\n",
      "  return tf.add_n(res), tf.group(*ops)\n",
      "\n",
      "\n",
      "def make_dense(targets, noclass):\n",
      "  \"\"\"Move a batch of targets to a dense 1-hot representation.\"\"\"\n",
      "  with tf.device(\"/cpu:0\"):\n",
      "    shape = tf.shape(targets)\n",
      "    batch_size = shape[0]\n",
      "    indices = targets + noclass * tf.range(0, batch_size)\n",
      "    length = batch_size * noclass\n",
      "    dense = tf.sparse_to_dense(indices, length, 1.0, 0.0)\n",
      "  return tf.reshape(dense, [-1, noclass])\n",
      "\n",
      "\n",
      "def check_for_zero(sparse):\n",
      "  \"\"\"In a sparse batch of ints, make 1.0 if it's 0 and 0.0 else.\"\"\"\n",
      "  with tf.device(\"/cpu:0\"):\n",
      "    shape = tf.shape(sparse)\n",
      "    batch_size = shape[0]\n",
      "    sparse = tf.minimum(sparse, 1)\n",
      "    indices = sparse + 2 * tf.range(0, batch_size)\n",
      "    dense = tf.sparse_to_dense(indices, 2 * batch_size, 1.0, 0.0)\n",
      "    reshaped = tf.reshape(dense, [-1, 2])\n",
      "  return tf.reshape(tf.slice(reshaped, [0, 0], [-1, 1]), [-1])\n",
      "\n",
      "\n",
      "class NeuralGPU(object):\n",
      "  \"\"\"Neural GPU Model.\"\"\"\n",
      "\n",
      "  def __init__(self, nmaps, vec_size, niclass, noclass, dropout, rx_step,\n",
      "               max_grad_norm, cutoff, nconvs, kw, kh, height, mode,\n",
      "               learning_rate, pull, pull_incr, min_length):\n",
      "    # Feeds for parameters and ops to update them.\n",
      "    self.global_step = tf.Variable(0, trainable=False)\n",
      "    self.cur_length = tf.Variable(min_length, trainable=False)\n",
      "    self.cur_length_incr_op = self.cur_length.assign_add(1)\n",
      "    self.lr = tf.Variable(float(learning_rate), trainable=False)\n",
      "    self.lr_decay_op = self.lr.assign(self.lr * 0.98)\n",
      "    self.pull = tf.Variable(float(pull), trainable=False)\n",
      "    self.pull_incr_op = self.pull.assign(self.pull * pull_incr)\n",
      "    self.do_training = tf.placeholder(tf.float32, name=\"do_training\")\n",
      "    self.noise_param = tf.placeholder(tf.float32, name=\"noise_param\")\n",
      "\n",
      "    # Feeds for inputs, targets, outputs, losses, etc.\n",
      "    self.input = []\n",
      "    self.target = []\n",
      "    for l in xrange(data_utils.forward_max + 1):\n",
      "      self.input.append(tf.placeholder(tf.int32, name=\"inp{0}\".format(l)))\n",
      "      self.target.append(tf.placeholder(tf.int32, name=\"tgt{0}\".format(l)))\n",
      "    self.outputs = []\n",
      "    self.losses = []\n",
      "    self.grad_norms = []\n",
      "    self.updates = []\n",
      "\n",
      "    # Computation.\n",
      "    inp0_shape = tf.shape(self.input[0])\n",
      "    batch_size = inp0_shape[0]\n",
      "    with tf.device(\"/cpu:0\"):\n",
      "      emb_weights = tf.get_variable(\n",
      "          \"embedding\", [niclass, vec_size],\n",
      "          initializer=tf.random_uniform_initializer(-1.7, 1.7))\n",
      "      e0 = tf.scatter_update(emb_weights,\n",
      "                             tf.constant(0, dtype=tf.int32, shape=[1]),\n",
      "                             tf.zeros([1, vec_size]))\n",
      "\n",
      "    adam = tf.train.AdamOptimizer(0.01*self.lr, epsilon=1e-5)\n",
      "\n",
      "    # Main graph creation loop, for every bin in data_utils.\n",
      "    self.steps = []\n",
      "    for length in sorted(list(set(data_utils.bins + [data_utils.forward_max]))):\n",
      "      data_utils.print_out(\"Creating model for bin of length %d.\" % length)\n",
      "      start_time = time.time()\n",
      "      if length > data_utils.bins[0]:\n",
      "        tf.get_variable_scope().reuse_variables()\n",
      "\n",
      "      # Embed inputs and calculate mask.\n",
      "      with tf.device(\"/cpu:0\"):\n",
      "        with tf.control_dependencies([e0]):\n",
      "          embedded = [tf.nn.embedding_lookup(emb_weights, self.input[l])\n",
      "                      for l in xrange(length)]\n",
      "        # Mask to 0-out padding space in each step.\n",
      "        imask = [check_for_zero(self.input[l]) for l in xrange(length)]\n",
      "        omask = [check_for_zero(self.target[l]) for l in xrange(length)]\n",
      "        mask = [1.0 - (imask[i] * omask[i]) for i in xrange(length)]\n",
      "        mask = [tf.reshape(m, [-1, 1]) for m in mask]\n",
      "        # Use a shifted mask for step scaling and concatenated for weights.\n",
      "        shifted_mask = mask + [tf.zeros_like(mask[0])]\n",
      "        scales = [shifted_mask[i] * (1.0 - shifted_mask[i+1])\n",
      "                  for i in xrange(length)]\n",
      "        scales = [tf.reshape(s, [-1, 1, 1, 1]) for s in scales]\n",
      "        mask = tf.concat(1, mask[0:length])  # batch x length\n",
      "        weights = mask\n",
      "        # Add a height dimension to mask to use later for masking.\n",
      "        mask = tf.reshape(mask, [-1, length, 1, 1])\n",
      "        mask = tf.concat(2, [mask for _ in xrange(height)]) + tf.zeros(\n",
      "            tf.pack([batch_size, length, height, nmaps]), dtype=tf.float32)\n",
      "\n",
      "      # Start is a length-list of batch-by-nmaps tensors, reshape and concat.\n",
      "      start = [tf.tanh(embedded[l]) for l in xrange(length)]\n",
      "      start = [tf.reshape(start[l], [-1, 1, nmaps]) for l in xrange(length)]\n",
      "      start = tf.reshape(tf.concat(1, start), [-1, length, 1, nmaps])\n",
      "\n",
      "      # First image comes from start by applying one convolution and adding 0s.\n",
      "      first = conv_linear(start, 1, 1, vec_size, nmaps, True, 0.0, \"input\")\n",
      "      first = [first] + [tf.zeros(tf.pack([batch_size, length, 1, nmaps]),\n",
      "                                  dtype=tf.float32) for _ in xrange(height - 1)]\n",
      "      first = tf.concat(2, first)\n",
      "\n",
      "      # Computation steps.\n",
      "      step = [tf.nn.dropout(first, 1.0 - self.do_training * dropout) * mask]\n",
      "      outputs = []\n",
      "      for it in xrange(length):\n",
      "        with tf.variable_scope(\"RX%d\" % (it % rx_step)) as vs:\n",
      "          if it >= rx_step:\n",
      "            vs.reuse_variables()\n",
      "          cur = step[it]\n",
      "          # Do nconvs-many CGRU steps.\n",
      "          for layer in xrange(nconvs):\n",
      "            cur = conv_gru([], cur, kw, kh, nmaps, cutoff, \"cgru_%d\" % layer)\n",
      "          cur = tf.nn.dropout(cur, 1.0 - self.do_training * dropout)\n",
      "          step.append(cur * mask)\n",
      "          outputs.append(tf.slice(step[-1], [0, 0, 0, 0], [-1, -1, 1, -1]))\n",
      "\n",
      "      self.steps.append([tf.reshape(s, [-1, length, height * nmaps])\n",
      "                         for s in step])\n",
      "      # Output is the n-th step output; n = current length, as in scales.\n",
      "      output = tf.add_n([outputs[i] * scales[i] for i in xrange(length)])\n",
      "      # Final convolution to get logits, list outputs.\n",
      "      output = conv_linear(output, 1, 1, nmaps, noclass, True, 0.0, \"output\")\n",
      "      output = tf.reshape(output, [-1, length, noclass])\n",
      "      self.outputs.append([tf.reshape(o, [-1, noclass])\n",
      "                           for o in list(tf.split(1, length, output))])\n",
      "\n",
      "      # Calculate cross-entropy loss and normalize it.\n",
      "      targets = tf.concat(1, [make_dense(self.target[l], noclass)\n",
      "                              for l in xrange(length)])\n",
      "      targets = tf.reshape(targets, [-1, noclass])\n",
      "      xent = tf.reshape(tf.nn.softmax_cross_entropy_with_logits(\n",
      "          tf.reshape(output, [-1, noclass]), targets), [-1, length])\n",
      "      perp_loss = tf.reduce_sum(xent * weights)\n",
      "      perp_loss /= tf.cast(batch_size, dtype=tf.float32)\n",
      "      perp_loss /= length\n",
      "\n",
      "      # Final loss: cross-entropy + shared parameter relaxation part.\n",
      "      relax_dist, self.avg_op = relaxed_distance(rx_step)\n",
      "      total_loss = perp_loss + relax_dist * self.pull\n",
      "      self.losses.append(perp_loss)\n",
      "\n",
      "      # Gradients and Adam update operation.\n",
      "      if length == data_utils.bins[0] or (mode == 0 and\n",
      "                                          length < data_utils.bins[-1] + 1):\n",
      "        data_utils.print_out(\"Creating backward for bin of length %d.\" % length)\n",
      "        params = tf.trainable_variables()\n",
      "        grads = tf.gradients(total_loss, params)\n",
      "        grads, norm = tf.clip_by_global_norm(grads, max_grad_norm)\n",
      "        self.grad_norms.append(norm)\n",
      "        for grad in grads:\n",
      "          if isinstance(grad, tf.Tensor):\n",
      "            grad += tf.truncated_normal(tf.shape(grad)) * self.noise_param\n",
      "        update = adam.apply_gradients(zip(grads, params),\n",
      "                                      global_step=self.global_step)\n",
      "        self.updates.append(update)\n",
      "      data_utils.print_out(\"Created model for bin of length %d in\"\n",
      "                           \" %.2f s.\" % (length, time.time() - start_time))\n",
      "    self.saver = tf.train.Saver(tf.all_variables())\n",
      "\n",
      "  def step(self, sess, inp, target, do_backward, noise_param=None):\n",
      "    \"\"\"Run a step of the network.\"\"\"\n",
      "    assert len(inp) == len(target)\n",
      "    length = len(target)\n",
      "    feed_in = {}\n",
      "    feed_in[self.noise_param.name] = noise_param if noise_param else 0.0\n",
      "    feed_in[self.do_training.name] = 1.0 if do_backward else 0.0\n",
      "    feed_out = []\n",
      "    index = len(data_utils.bins)\n",
      "    if length < data_utils.bins[-1] + 1:\n",
      "      index = data_utils.bins.index(length)\n",
      "    if do_backward:\n",
      "      feed_out.append(self.updates[index])\n",
      "      feed_out.append(self.grad_norms[index])\n",
      "    feed_out.append(self.losses[index])\n",
      "    for l in xrange(length):\n",
      "      feed_in[self.input[l].name] = inp[l]\n",
      "    for l in xrange(length):\n",
      "      feed_in[self.target[l].name] = target[l]\n",
      "      feed_out.append(self.outputs[index][l])\n",
      "    for l in xrange(length+1):\n",
      "      feed_out.append(self.steps[index][l])\n",
      "    res = sess.run(feed_out, feed_in)\n",
      "    offset = 0\n",
      "    norm = None\n",
      "    if do_backward:\n",
      "      offset = 2\n",
      "      norm = res[1]\n",
      "    outputs = res[offset + 1:offset + 1 + length]\n",
      "    steps = res[offset + 1 + length:]\n",
      "    return res[offset], outputs, norm, steps\n",
      "\n",
      "@@ -0,0 +1,271 @@\n",
      "+\"\"\"The Neural GPU Model.\"\"\"\n",
      "+\n",
      "+import time\n",
      "+\n",
      "+import google3\n",
      "+\n",
      "+import tensorflow as tf\n",
      "+\n",
      "+from google3.experimental.users.lukaszkaiser.neural_gpu import data_utils\n",
      "+\n",
      "+\n",
      "+def conv_linear(args, kw, kh, nin, nout, do_bias, bias_start, prefix):\n",
      "+  \"\"\"Convolutional linear map.\"\"\"\n",
      "+  assert args\n",
      "+  if not isinstance(args, (list, tuple)):\n",
      "+    args = [args]\n",
      "+  with tf.variable_scope(prefix):\n",
      "+    k = tf.get_variable(\"CvK\", [kw, kh, nin, nout])\n",
      "+    if len(args) == 1:\n",
      "+      res = tf.nn.conv2d(args[0], k, [1, 1, 1, 1], \"SAME\")\n",
      "+    else:\n",
      "+      res = tf.nn.conv2d(tf.concat(3, args), k, [1, 1, 1, 1], \"SAME\")\n",
      "+    if not do_bias: return res\n",
      "+    bias_term = tf.get_variable(\"CvB\", [nout],\n",
      "+                                initializer=tf.constant_initializer(0.0))\n",
      "+    return res + bias_term + bias_start\n",
      "+\n",
      "+\n",
      "+def sigmoid_cutoff(x, cutoff):\n",
      "+  \"\"\"Sigmoid with cutoff, e.g., 1.2sigmoid(x) - 0.1.\"\"\"\n",
      "+  y = tf.sigmoid(x)\n",
      "+  if cutoff < 1.01: return y\n",
      "+  d = (cutoff - 1.0) / 2.0\n",
      "+  return tf.minimum(1.0, tf.maximum(0.0, cutoff * y - d))\n",
      "+\n",
      "+\n",
      "+def conv_gru(inpts, mem, kw, kh, nmaps, cutoff, prefix):\n",
      "+  \"\"\"Convolutional GRU.\"\"\"\n",
      "+  def conv_lin(args, suffix, bias_start):\n",
      "+    return conv_linear(args, kw, kh, len(args) * nmaps, nmaps, True, bias_start,\n",
      "+                       prefix + \"/\" + suffix)\n",
      "+  reset = sigmoid_cutoff(conv_lin(inpts + [mem], \"r\", 1.0), cutoff)\n",
      "+  candidate = tf.tanh(conv_lin(inpts + [reset * mem], \"c\", 0.0))\n",
      "+  gate = sigmoid_cutoff(conv_lin(inpts + [mem], \"g\", 1.0), cutoff)\n",
      "+  return gate * mem + (1 - gate) * candidate\n",
      "+\n",
      "+\n",
      "+def relaxed_average(var_name_suffix, rx_step):\n",
      "+  \"\"\"Calculate the average of relaxed variables having var_name_suffix.\"\"\"\n",
      "+  relaxed_vars = []\n",
      "+  for l in xrange(rx_step):\n",
      "+    with tf.variable_scope(\"RX%d\" % l, reuse=True):\n",
      "+      try:\n",
      "+        relaxed_vars.append(tf.get_variable(var_name_suffix))\n",
      "+      except ValueError:\n",
      "+        pass\n",
      "+  dsum = tf.add_n(relaxed_vars)\n",
      "+  avg = dsum / len(relaxed_vars)\n",
      "+  diff = [v - avg for v in relaxed_vars]\n",
      "+  davg = tf.add_n([d*d for d in diff])\n",
      "+  return avg, tf.reduce_sum(davg)\n",
      "+\n",
      "+\n",
      "+def relaxed_distance(rx_step):\n",
      "+  \"\"\"Distance between relaxed variables and their average.\"\"\"\n",
      "+  res, ops, rx_done = [], [], {}\n",
      "+  for v in tf.trainable_variables():\n",
      "+    if v.name[0:2] == \"RX\":\n",
      "+      rx_name = v.op.name[v.name.find(\"/\") + 1:]\n",
      "+      if rx_name not in rx_done:\n",
      "+        avg, dist_loss = relaxed_average(rx_name, rx_step)\n",
      "+        res.append(dist_loss)\n",
      "+        rx_done[rx_name] = avg\n",
      "+      ops.append(v.assign(rx_done[rx_name]))\n",
      "+  return tf.add_n(res), tf.group(*ops)\n",
      "+\n",
      "+\n",
      "+def make_dense(targets, noclass):\n",
      "+  \"\"\"Move a batch of targets to a dense 1-hot representation.\"\"\"\n",
      "+  with tf.device(\"/cpu:0\"):\n",
      "+    shape = tf.shape(targets)\n",
      "+    batch_size = shape[0]\n",
      "+    indices = targets + noclass * tf.range(0, batch_size)\n",
      "+    length = batch_size * noclass\n",
      "+    dense = tf.sparse_to_dense(indices, length, 1.0, 0.0)\n",
      "+  return tf.reshape(dense, [-1, noclass])\n",
      "+\n",
      "+\n",
      "+def check_for_zero(sparse):\n",
      "+  \"\"\"In a sparse batch of ints, make 1.0 if it's 0 and 0.0 else.\"\"\"\n",
      "+  with tf.device(\"/cpu:0\"):\n",
      "+    shape = tf.shape(sparse)\n",
      "+    batch_size = shape[0]\n",
      "+    sparse = tf.minimum(sparse, 1)\n",
      "+    indices = sparse + 2 * tf.range(0, batch_size)\n",
      "+    dense = tf.sparse_to_dense(indices, 2 * batch_size, 1.0, 0.0)\n",
      "+    reshaped = tf.reshape(dense, [-1, 2])\n",
      "+  return tf.reshape(tf.slice(reshaped, [0, 0], [-1, 1]), [-1])\n",
      "+\n",
      "+\n",
      "+class NeuralGPU(object):\n",
      "+  \"\"\"Neural GPU Model.\"\"\"\n",
      "+\n",
      "+  def __init__(self, nmaps, vec_size, niclass, noclass, dropout, rx_step,\n",
      "+               max_grad_norm, cutoff, nconvs, kw, kh, height, mode,\n",
      "+               learning_rate, pull, pull_incr, min_length):\n",
      "+    # Feeds for parameters and ops to update them.\n",
      "+    self.global_step = tf.Variable(0, trainable=False)\n",
      "+    self.cur_length = tf.Variable(min_length, trainable=False)\n",
      "+    self.cur_length_incr_op = self.cur_length.assign_add(1)\n",
      "+    self.lr = tf.Variable(float(learning_rate), trainable=False)\n",
      "+    self.lr_decay_op = self.lr.assign(self.lr * 0.98)\n",
      "+    self.pull = tf.Variable(float(pull), trainable=False)\n",
      "+    self.pull_incr_op = self.pull.assign(self.pull * pull_incr)\n",
      "+    self.do_training = tf.placeholder(tf.float32, name=\"do_training\")\n",
      "+    self.noise_param = tf.placeholder(tf.float32, name=\"noise_param\")\n",
      "+\n",
      "+    # Feeds for inputs, targets, outputs, losses, etc.\n",
      "+    self.input = []\n",
      "+    self.target = []\n",
      "+    for l in xrange(data_utils.forward_max + 1):\n",
      "+      self.input.append(tf.placeholder(tf.int32, name=\"inp{0}\".format(l)))\n",
      "+      self.target.append(tf.placeholder(tf.int32, name=\"tgt{0}\".format(l)))\n",
      "+    self.outputs = []\n",
      "+    self.losses = []\n",
      "+    self.grad_norms = []\n",
      "+    self.updates = []\n",
      "+\n",
      "+    # Computation.\n",
      "+    inp0_shape = tf.shape(self.input[0])\n",
      "+    batch_size = inp0_shape[0]\n",
      "+    with tf.device(\"/cpu:0\"):\n",
      "+      emb_weights = tf.get_variable(\n",
      "+          \"embedding\", [niclass, vec_size],\n",
      "+          initializer=tf.random_uniform_initializer(-1.7, 1.7))\n",
      "+      e0 = tf.scatter_update(emb_weights,\n",
      "+                             tf.constant(0, dtype=tf.int32, shape=[1]),\n",
      "+                             tf.zeros([1, vec_size]))\n",
      "+\n",
      "+    adam = tf.train.AdamOptimizer(0.01*self.lr, epsilon=1e-5)\n",
      "+\n",
      "+    # Main graph creation loop, for every bin in data_utils.\n",
      "+    self.steps = []\n",
      "+    for length in sorted(list(set(data_utils.bins + [data_utils.forward_max]))):\n",
      "+      data_utils.print_out(\"Creating model for bin of length %d.\" % length)\n",
      "+      start_time = time.time()\n",
      "+      if length > data_utils.bins[0]:\n",
      "+        tf.get_variable_scope().reuse_variables()\n",
      "+\n",
      "+      # Embed inputs and calculate mask.\n",
      "+      with tf.device(\"/cpu:0\"):\n",
      "+        with tf.control_dependencies([e0]):\n",
      "+          embedded = [tf.nn.embedding_lookup(emb_weights, self.input[l])\n",
      "+                      for l in xrange(length)]\n",
      "+        # Mask to 0-out padding space in each step.\n",
      "+        imask = [check_for_zero(self.input[l]) for l in xrange(length)]\n",
      "+        omask = [check_for_zero(self.target[l]) for l in xrange(length)]\n",
      "+        mask = [1.0 - (imask[i] * omask[i]) for i in xrange(length)]\n",
      "+        mask = [tf.reshape(m, [-1, 1]) for m in mask]\n",
      "+        # Use a shifted mask for step scaling and concatenated for weights.\n",
      "+        shifted_mask = mask + [tf.zeros_like(mask[0])]\n",
      "+        scales = [shifted_mask[i] * (1.0 - shifted_mask[i+1])\n",
      "+                  for i in xrange(length)]\n",
      "+        scales = [tf.reshape(s, [-1, 1, 1, 1]) for s in scales]\n",
      "+        mask = tf.concat(1, mask[0:length])  # batch x length\n",
      "+        weights = mask\n",
      "+        # Add a height dimension to mask to use later for masking.\n",
      "+        mask = tf.reshape(mask, [-1, length, 1, 1])\n",
      "+        mask = tf.concat(2, [mask for _ in xrange(height)]) + tf.zeros(\n",
      "+            tf.pack([batch_size, length, height, nmaps]), dtype=tf.float32)\n",
      "+\n",
      "+      # Start is a length-list of batch-by-nmaps tensors, reshape and concat.\n",
      "+      start = [tf.tanh(embedded[l]) for l in xrange(length)]\n",
      "+      start = [tf.reshape(start[l], [-1, 1, nmaps]) for l in xrange(length)]\n",
      "+      start = tf.reshape(tf.concat(1, start), [-1, length, 1, nmaps])\n",
      "+\n",
      "+      # First image comes from start by applying one convolution and adding 0s.\n",
      "+      first = conv_linear(start, 1, 1, vec_size, nmaps, True, 0.0, \"input\")\n",
      "+      first = [first] + [tf.zeros(tf.pack([batch_size, length, 1, nmaps]),\n",
      "+                                  dtype=tf.float32) for _ in xrange(height - 1)]\n",
      "+      first = tf.concat(2, first)\n",
      "+\n",
      "+      # Computation steps.\n",
      "+      step = [tf.nn.dropout(first, 1.0 - self.do_training * dropout) * mask]\n",
      "+      outputs = []\n",
      "+      for it in xrange(length):\n",
      "+        with tf.variable_scope(\"RX%d\" % (it % rx_step)) as vs:\n",
      "+          if it >= rx_step:\n",
      "+            vs.reuse_variables()\n",
      "+          cur = step[it]\n",
      "+          # Do nconvs-many CGRU steps.\n",
      "+          for layer in xrange(nconvs):\n",
      "+            cur = conv_gru([], cur, kw, kh, nmaps, cutoff, \"cgru_%d\" % layer)\n",
      "+          cur = tf.nn.dropout(cur, 1.0 - self.do_training * dropout)\n",
      "+          step.append(cur * mask)\n",
      "+          outputs.append(tf.slice(step[-1], [0, 0, 0, 0], [-1, -1, 1, -1]))\n",
      "+\n",
      "+      self.steps.append([tf.reshape(s, [-1, length, height * nmaps])\n",
      "+                         for s in step])\n",
      "+      # Output is the n-th step output; n = current length, as in scales.\n",
      "+      output = tf.add_n([outputs[i] * scales[i] for i in xrange(length)])\n",
      "+      # Final convolution to get logits, list outputs.\n",
      "+      output = conv_linear(output, 1, 1, nmaps, noclass, True, 0.0, \"output\")\n",
      "+      output = tf.reshape(output, [-1, length, noclass])\n",
      "+      self.outputs.append([tf.reshape(o, [-1, noclass])\n",
      "+                           for o in list(tf.split(1, length, output))])\n",
      "+\n",
      "+      # Calculate cross-entropy loss and normalize it.\n",
      "+      targets = tf.concat(1, [make_dense(self.target[l], noclass)\n",
      "+                              for l in xrange(length)])\n",
      "+      targets = tf.reshape(targets, [-1, noclass])\n",
      "+      xent = tf.reshape(tf.nn.softmax_cross_entropy_with_logits(\n",
      "+          tf.reshape(output, [-1, noclass]), targets), [-1, length])\n",
      "+      perp_loss = tf.reduce_sum(xent * weights)\n",
      "+      perp_loss /= tf.cast(batch_size, dtype=tf.float32)\n",
      "+      perp_loss /= length\n",
      "+\n",
      "+      # Final loss: cross-entropy + shared parameter relaxation part.\n",
      "+      relax_dist, self.avg_op = relaxed_distance(rx_step)\n",
      "+      total_loss = perp_loss + relax_dist * self.pull\n",
      "+      self.losses.append(perp_loss)\n",
      "+\n",
      "+      # Gradients and Adam update operation.\n",
      "+      if length == data_utils.bins[0] or (mode == 0 and\n",
      "+                                          length < data_utils.bins[-1] + 1):\n",
      "+        data_utils.print_out(\"Creating backward for bin of length %d.\" % length)\n",
      "+        params = tf.trainable_variables()\n",
      "+        grads = tf.gradients(total_loss, params)\n",
      "+        grads, norm = tf.clip_by_global_norm(grads, max_grad_norm)\n",
      "+        self.grad_norms.append(norm)\n",
      "+        for grad in grads:\n",
      "+          if isinstance(grad, tf.Tensor):\n",
      "+            grad += tf.truncated_normal(tf.shape(grad)) * self.noise_param\n",
      "+        update = adam.apply_gradients(zip(grads, params),\n",
      "+                                      global_step=self.global_step)\n",
      "+        self.updates.append(update)\n",
      "+      data_utils.print_out(\"Created model for bin of length %d in\"\n",
      "+                           \" %.2f s.\" % (length, time.time() - start_time))\n",
      "+    self.saver = tf.train.Saver(tf.all_variables())\n",
      "+\n",
      "+  def step(self, sess, inp, target, do_backward, noise_param=None):\n",
      "+    \"\"\"Run a step of the network.\"\"\"\n",
      "+    assert len(inp) == len(target)\n",
      "+    length = len(target)\n",
      "+    feed_in = {}\n",
      "+    feed_in[self.noise_param.name] = noise_param if noise_param else 0.0\n",
      "+    feed_in[self.do_training.name] = 1.0 if do_backward else 0.0\n",
      "+    feed_out = []\n",
      "+    index = len(data_utils.bins)\n",
      "+    if length < data_utils.bins[-1] + 1:\n",
      "+      index = data_utils.bins.index(length)\n",
      "+    if do_backward:\n",
      "+      feed_out.append(self.updates[index])\n",
      "+      feed_out.append(self.grad_norms[index])\n",
      "+    feed_out.append(self.losses[index])\n",
      "+    for l in xrange(length):\n",
      "+      feed_in[self.input[l].name] = inp[l]\n",
      "+    for l in xrange(length):\n",
      "+      feed_in[self.target[l].name] = target[l]\n",
      "+      feed_out.append(self.outputs[index][l])\n",
      "+    for l in xrange(length+1):\n",
      "+      feed_out.append(self.steps[index][l])\n",
      "+    res = sess.run(feed_out, feed_in)\n",
      "+    offset = 0\n",
      "+    norm = None\n",
      "+    if do_backward:\n",
      "+      offset = 2\n",
      "+      norm = res[1]\n",
      "+    outputs = res[offset + 1:offset + 1 + length]\n",
      "+    steps = res[offset + 1 + length:]\n",
      "+    return res[offset], outputs, norm, steps\n",
      "\n",
      "\n",
      "__Before__: \n",
      "None\n",
      "__After__: \n",
      "\"\"\"Neural GPU for Learning Algorithms.\"\"\"\n",
      "\n",
      "import math\n",
      "import os\n",
      "import random\n",
      "import sys\n",
      "import time\n",
      "\n",
      "import google3\n",
      "\n",
      "import matplotlib.animation as anim\n",
      "import matplotlib.pyplot as plt\n",
      "import numpy as np\n",
      "import tensorflow as tf\n",
      "\n",
      "from google3.third_party.tensorflow.python.platform import gfile\n",
      "import google3.experimental.users.lukaszkaiser.neural_gpu.data_utils as data\n",
      "import google3.experimental.users.lukaszkaiser.neural_gpu.neural_gpu as ngpu\n",
      "\n",
      "tf.app.flags.DEFINE_float(\"lr\", 0.1, \"Learning rate.\")\n",
      "tf.app.flags.DEFINE_float(\"init_weight\", 1.0, \"Initial weights deviation.\")\n",
      "tf.app.flags.DEFINE_float(\"max_grad_norm\", 0.05, \"Clip gradients to this norm.\")\n",
      "tf.app.flags.DEFINE_float(\"cutoff\", 1.2, \"Cutoff at the gates.\")\n",
      "tf.app.flags.DEFINE_float(\"pull\", 0.0005, \"Starting pull of the relaxations.\")\n",
      "tf.app.flags.DEFINE_float(\"pull_incr\", 1.2, \"Increase pull by that much.\")\n",
      "tf.app.flags.DEFINE_float(\"dropout\", 0.2, \"Dropout that much.\")\n",
      "tf.app.flags.DEFINE_float(\"grad_noise_scale\", 1.0, \"Gradient noise scale.\")\n",
      "tf.app.flags.DEFINE_integer(\"batch_size\", 64, \"Batch size.\")\n",
      "tf.app.flags.DEFINE_integer(\"low_batch_size\", 16, \"Low batch size.\")\n",
      "tf.app.flags.DEFINE_integer(\"steps_per_checkpoint\", 100, \"Steps per epoch.\")\n",
      "tf.app.flags.DEFINE_integer(\"nmaps\", 24, \"Number of floats in each cell.\")\n",
      "tf.app.flags.DEFINE_integer(\"niclass\", 14, \"Number of classes (0 is padding).\")\n",
      "tf.app.flags.DEFINE_integer(\"noclass\", 14, \"Number of classes (0 is padding).\")\n",
      "tf.app.flags.DEFINE_integer(\"train_data_size\", 5000, \"Training examples/len.\")\n",
      "tf.app.flags.DEFINE_integer(\"max_length\", 41, \"Maximum length.\")\n",
      "tf.app.flags.DEFINE_integer(\"rx_step\", 6, \"Relax that many recursive steps.\")\n",
      "tf.app.flags.DEFINE_integer(\"random_seed\", 125459, \"Random seed.\")\n",
      "tf.app.flags.DEFINE_integer(\"nconvs\", 2, \"How many convolutions / 1 step.\")\n",
      "tf.app.flags.DEFINE_integer(\"kw\", 3, \"Kernel width.\")\n",
      "tf.app.flags.DEFINE_integer(\"kh\", 3, \"Kernel height.\")\n",
      "tf.app.flags.DEFINE_integer(\"height\", 4, \"Height.\")\n",
      "tf.app.flags.DEFINE_integer(\"forward_max\", 401, \"Maximum forward length.\")\n",
      "tf.app.flags.DEFINE_integer(\"jobid\", -1, \"Task id when running on borg.\")\n",
      "tf.app.flags.DEFINE_integer(\"nprint\", 0, \"How many test examples to print out.\")\n",
      "tf.app.flags.DEFINE_integer(\"mode\", 0, \"Mode: 0-train other-decode.\")\n",
      "tf.app.flags.DEFINE_string(\"task\", \"rev\", \"Which task are we learning?\")\n",
      "tf.app.flags.DEFINE_string(\"train_dir\", \"/tmp/\", \"Directory to store models.\")\n",
      "\n",
      "FLAGS = tf.app.flags.FLAGS\n",
      "\n",
      "\n",
      "def initialize(sess):\n",
      "  \"\"\"Initialize data and model.\"\"\"\n",
      "  if FLAGS.jobid >= 0:\n",
      "    data.log_filename = os.path.join(FLAGS.train_dir, \"log%d\" % FLAGS.jobid)\n",
      "  data.print_out(\"NN \", newline=False)\n",
      "\n",
      "  # Set random seed.\n",
      "  seed = FLAGS.random_seed + max(0, FLAGS.jobid)\n",
      "  tf.set_random_seed(seed)\n",
      "  random.seed(seed)\n",
      "  np.random.seed(seed)\n",
      "\n",
      "  # Check data sizes.\n",
      "  data.forward_max = max(FLAGS.forward_max, data.bins[-1])\n",
      "  assert data.bins\n",
      "  min_length = 3\n",
      "  max_length = min(FLAGS.max_length, data.bins[-1])\n",
      "  assert max_length + 1 > min_length\n",
      "  while len(data.bins) > 1 and data.bins[-2] > max_length + 12:\n",
      "    data.bins = data.bins[:-1]\n",
      "  assert data.bins[0] > FLAGS.rx_step\n",
      "  nclass = min(FLAGS.niclass, FLAGS.noclass)\n",
      "  data_size = FLAGS.train_data_size if FLAGS.mode == 0 else 1000\n",
      "\n",
      "  # Initialize data for each task.\n",
      "  tasks = FLAGS.task.split(\"-\")\n",
      "  for t in tasks:\n",
      "    for l in xrange(max_length + 11):\n",
      "      data.init_data(t, l, data_size, nclass)\n",
      "    data.init_data(t, data.bins[-2], data_size, nclass)\n",
      "    data.init_data(t, data.bins[-1], data_size, nclass)\n",
      "    end_size = 4 * 1024 if FLAGS.mode > 0 else 1024\n",
      "    data.init_data(t, data.forward_max, end_size, nclass)\n",
      "\n",
      "  # Print out parameters.\n",
      "  curriculum = 0.12\n",
      "  fin = (\"cv %d kw %d h %d kh %d rxr %d bs %d ns %.2f t %s\"\n",
      "         % (FLAGS.nconvs, FLAGS.kw, FLAGS.height, FLAGS.kh, FLAGS.rx_step,\n",
      "            FLAGS.batch_size, FLAGS.grad_noise_scale, FLAGS.task))\n",
      "  fin = \"data %d %s\" % (FLAGS.train_data_size, fin)\n",
      "  tag = (\"df %.2f p %.3f lr %.2f iw %.2f cr %.2f nm %d d%.4f gn %.2f %s\" %\n",
      "         (FLAGS.cutoff, FLAGS.pull_incr, FLAGS.lr, FLAGS.init_weight,\n",
      "          curriculum, FLAGS.nmaps, FLAGS.dropout, FLAGS.max_grad_norm, fin))\n",
      "  data.print_out(tag)\n",
      "\n",
      "  # Create checkpoint directory if it does not exist.\n",
      "  checkpoint_dir = os.path.join(FLAGS.train_dir, \"neural_gpu%s\"\n",
      "                                % (\"\" if FLAGS.jobid < 0 else str(FLAGS.jobid)))\n",
      "  if not gfile.IsDirectory(checkpoint_dir):\n",
      "    data.print_out(\"Creating checkpoint directory %s.\" % checkpoint_dir)\n",
      "    gfile.MkDir(checkpoint_dir)\n",
      "\n",
      "  # Create model and initialize it.\n",
      "  tf.get_variable_scope().set_initializer(\n",
      "      tf.uniform_unit_scaling_initializer(factor=1.8 * FLAGS.init_weight))\n",
      "  model = ngpu.NeuralGPU(\n",
      "      FLAGS.nmaps, FLAGS.nmaps, FLAGS.niclass, FLAGS.noclass, FLAGS.dropout,\n",
      "      FLAGS.rx_step, FLAGS.max_grad_norm, FLAGS.cutoff, FLAGS.nconvs,\n",
      "      FLAGS.kw, FLAGS.kh, FLAGS.height, FLAGS.mode, FLAGS.lr,\n",
      "      FLAGS.pull, FLAGS.pull_incr, min_length + 3)\n",
      "  data.print_out(\"Created model.\")\n",
      "  sess.run(tf.initialize_all_variables())\n",
      "  data.print_out(\"Initialized variables.\")\n",
      "\n",
      "  # Load model from parameters if a checkpoint exists.\n",
      "  ckpt = tf.train.get_checkpoint_state(checkpoint_dir)\n",
      "  if ckpt and gfile.Exists(ckpt.model_checkpoint_path):\n",
      "    data.print_out(\"Reading model parameters from %s\"\n",
      "                   % ckpt.model_checkpoint_path)\n",
      "    model.saver.restore(sess, ckpt.model_checkpoint_path)\n",
      "\n",
      "  # Return the model and needed variables.\n",
      "  return (model, min_length, max_length, checkpoint_dir, curriculum)\n",
      "\n",
      "\n",
      "def single_test(l, model, sess, task, nprint, batch_size, print_out=True,\n",
      "                offset=None):\n",
      "  \"\"\"Test model on test data of length l using the given session.\"\"\"\n",
      "  inpt, target = data.get_batch(l, batch_size, False, task, offset)\n",
      "  _, res, _, steps = model.step(sess, inpt, target, False)\n",
      "  errors, total, seq = data.accuracy(inpt, res, target, batch_size, nprint)\n",
      "  seq = float(seq) / batch_size\n",
      "  if total > 0:\n",
      "    errors = float(errors) / total\n",
      "  if print_out:\n",
      "    data.print_out(\"  %s len %d errors %.2f sequence-errors %.2f\"\n",
      "                   % (task, l, 100*errors, 100*seq))\n",
      "  return errors, seq, (steps, inpt, [np.argmax(o, axis=1) for o in res])\n",
      "\n",
      "\n",
      "def multi_test(l, model, sess, task, nprint, batch_size, offset=None):\n",
      "  \"\"\"Run multiple tests at lower batch size to save memory.\"\"\"\n",
      "  errors = 0.0\n",
      "  seq = 0.0\n",
      "  to_print = nprint\n",
      "  low_batch = FLAGS.low_batch_size\n",
      "  low_batch = min(low_batch, batch_size)\n",
      "  for mstep in xrange(batch_size / low_batch):\n",
      "    cur_offset = None if offset is None else offset + mstep * low_batch\n",
      "    err, sq, _ = single_test(l, model, sess, task, to_print, low_batch, False,\n",
      "                             cur_offset)\n",
      "    to_print = max(0, to_print - low_batch)\n",
      "    errors += err\n",
      "    seq += sq\n",
      "    if FLAGS.mode > 0:\n",
      "      cur_errors = float(low_batch * errors) / ((mstep+1) * low_batch)\n",
      "      cur_seq = float(low_batch * seq) / ((mstep+1) * low_batch)\n",
      "      data.print_out(\"    %s multitest current errors %.2f sequence-errors %.2f\"\n",
      "                     % (task, 100*cur_errors, 100*cur_seq))\n",
      "  errors = float(low_batch) * float(errors) / batch_size\n",
      "  seq = float(low_batch) * float(seq) / batch_size\n",
      "  data.print_out(\"  %s len %d errors %.2f sequence-errors %.2f\"\n",
      "                 % (task, l, 100*errors, 100*seq))\n",
      "  return errors, seq\n",
      "\n",
      "\n",
      "def train():\n",
      "  \"\"\"Main training function.\"\"\"\n",
      "  batch_size = FLAGS.batch_size\n",
      "  tasks = FLAGS.task.split(\"-\")\n",
      "  with tf.Session() as sess:\n",
      "    model, min_length, max_length, checkpoint_dir, curriculum = initialize(sess)\n",
      "    max_cur_length = min(min_length + 3, max_length)\n",
      "    prev_acc_perp = [1000000 for _ in xrange(3)]\n",
      "    prev_sq = 1.0\n",
      "\n",
      "    while True:\n",
      "      global_step, pull, max_cur_length, learning_rate = sess.run(\n",
      "          [model.global_step, model.pull, model.cur_length, model.lr])\n",
      "      ep = global_step / FLAGS.steps_per_checkpoint\n",
      "      acc_loss, acc_total, acc_errors, acc_seq = 0.0, 0, 0, 0\n",
      "      acc_grad_norm, step_count, step_time = 0.0, 0, 0.0\n",
      "      for _ in xrange(FLAGS.steps_per_checkpoint):\n",
      "        global_step += 1\n",
      "        task = random.choice(tasks)\n",
      "        l1 = np.random.randint(max_cur_length - min_length + 1) + min_length\n",
      "        l = l1\n",
      "        if np.random.randint(10) > 3:  # Prefer longer stuff 60% of time.\n",
      "          l = np.random.randint(max_cur_length - min_length+1) + min_length\n",
      "          l = max(l, l1)\n",
      "        if np.random.randint(4) < 1:  # Mixed learning: once in a while big.\n",
      "          l = np.random.randint(max_length - min_length + 1) + min_length\n",
      "          l = max(l, l1)\n",
      "        start_time = time.time()\n",
      "        inp, target = data.get_batch(l, batch_size, True, task)\n",
      "        stepp = math.pow(global_step, -0.55)\n",
      "        noise_param = math.sqrt(stepp * 20 * prev_sq) * FLAGS.grad_noise_scale\n",
      "        loss, res, gnorm, _ = model.step(sess, inp, target, True, noise_param)\n",
      "        step_time += time.time() - start_time\n",
      "        acc_grad_norm += float(gnorm)\n",
      "        if l < max_cur_length + 1:\n",
      "          step_count += 1\n",
      "          acc_loss += loss\n",
      "          errors, total, seq = data.accuracy(inp, res, target,\n",
      "                                             batch_size, 0)\n",
      "          acc_total += total\n",
      "          acc_errors += errors\n",
      "          acc_seq += seq\n",
      "      acc_loss /= step_count\n",
      "      step_time /= FLAGS.steps_per_checkpoint\n",
      "      acc_seq = float(acc_seq) / (step_count * batch_size)\n",
      "      prev_sq = acc_seq\n",
      "      acc_errors = float(acc_errors) / acc_total if acc_total > 0 else 1.0\n",
      "      msg1 = \"ep %d st %.2f lr %.8f\" % (ep, step_time, learning_rate)\n",
      "      msg2 = \"pl %.3f cme %.3f\" % (pull, curriculum)\n",
      "      msg = (\"%s %s gn %.8f\"\n",
      "             % (msg1, msg2, acc_grad_norm / FLAGS.steps_per_checkpoint))\n",
      "      data.print_out(\"%s len %d ppx %.8f errs %.2f sq %.2f\" %\n",
      "                     (msg, max_cur_length, data.safe_exp(acc_loss),\n",
      "                      100*acc_errors, 100*acc_seq))\n",
      "      if curriculum > acc_seq:\n",
      "        prev_acc_perp.append(1000000)\n",
      "        do_incr = True\n",
      "        while do_incr and max_cur_length < max_length:\n",
      "          sess.run(model.cur_length_incr_op)\n",
      "          for t in tasks:\n",
      "            if data.train_set[t]: do_incr = False\n",
      "        if pull < 1:\n",
      "          sess.run(model.pull_incr_op)\n",
      "        else:\n",
      "          data.print_out(\"  Averaging parameters.\")\n",
      "          sess.run([model.avg_op, model.lr_decay_op])\n",
      "      else:\n",
      "        acc_perp = data.safe_exp(acc_loss)\n",
      "        if acc_perp > max(prev_acc_perp[-3:]):\n",
      "          sess.run(model.lr_decay_op)\n",
      "        prev_acc_perp.append(acc_perp)\n",
      "      checkpoint_path = os.path.join(checkpoint_dir, \"neural_gpu.ckpt\")\n",
      "      model.saver.save(sess, checkpoint_path,\n",
      "                       global_step=model.global_step)\n",
      "      # Run evaluation.\n",
      "      should_exit = True\n",
      "      bound = data.bins[-1] + 1\n",
      "      for t in tasks:\n",
      "        l = min_length\n",
      "        while l < max_length + 12 and l < bound:\n",
      "          _, sq, _ = single_test(l, model, sess, t, FLAGS.nprint, batch_size)\n",
      "          l += 1\n",
      "          while l < bound + 1 and not data.test_set[t][l]:\n",
      "            l += 1\n",
      "        if sq < 0.5:\n",
      "          _, sq = multi_test(data.forward_max, model, sess, t, FLAGS.nprint,\n",
      "                             batch_size * 4)\n",
      "        if sq > 0.001: should_exit = False\n",
      "      if should_exit:\n",
      "        if data.forward_max > 4000 and len(tasks) == 1:\n",
      "          multi_test(data.forward_max, model, sess, tasks[0], FLAGS.nprint,\n",
      "                     batch_size * 16, 0)\n",
      "\n",
      "\n",
      "def animate(l, test_data, anim_size):\n",
      "  \"\"\"Create animation for the given data (hacky matplotlib use).\"\"\"\n",
      "  xf = 12\n",
      "  fps = 2\n",
      "  fig = plt.figure(figsize=(16, 9), facecolor=\"white\")\n",
      "  ax = fig.add_axes([0, 0, 1, 1], frameon=False, zorder=2)\n",
      "  ax.set_xticks([i * 24-0.5 for i in xrange(4)])\n",
      "  ax.set_xticklabels([])\n",
      "  ax.set_yticks([i - 0.5 for i in xrange(l+1)])\n",
      "  ax.grid(which=\"major\", axis=\"both\", linestyle=\"-\", color=\"black\")\n",
      "  text_fields = []\n",
      "  text_size = 24*32/l\n",
      "  for y in xrange(l):\n",
      "    text_fields.append(ax.text(\n",
      "        11.25, y + 0.15, \"\", color=\"g\", ha=\"center\", va=\"center\",\n",
      "        bbox={\"facecolor\": \"b\", \"alpha\": 0.01, \"pad\": 24 * text_size},\n",
      "        size=text_size - (4 * 32 / l), animated=True))\n",
      "  im = ax.imshow(np.zeros_like(test_data[0][0][0]), vmin=-1.0,\n",
      "                 vmax=1.0, cmap=\"gray\", aspect=\"auto\", origin=\"upper\",\n",
      "                 interpolation=\"none\", animated=True)\n",
      "  im.set_zorder(1)\n",
      "  def to_symbol(i):\n",
      "    if i == 0: return \"\"\n",
      "    if i == 11: return \"+\"\n",
      "    if i == 12: return \"*\"\n",
      "    return str(i-1)\n",
      "  def animation_update(frame_no, test_data, xf, im, text_fields):\n",
      "    \"\"\"Update an animation frame.\"\"\"\n",
      "    steps, inpt, out_raw = test_data\n",
      "    length = len(steps)\n",
      "    batch = frame_no / (fps * (l+4*xf))\n",
      "    index = int((frame_no % (fps * (l+4*xf))) / fps)\n",
      "    # Cut output after first padding.\n",
      "    out = [out_raw[i][batch] for i in xrange(len(text_fields))]\n",
      "    if 0 in out:\n",
      "      i = out.index(0)\n",
      "      out = out[0:i] + [0 for _ in xrange(len(out) - i)]\n",
      "    # Show the state after the first frames.\n",
      "    if index >= 2*xf:\n",
      "      im.set_array(steps[min(length - 1, index - 2*xf)][batch])\n",
      "      for i, t in enumerate(text_fields):\n",
      "        if index - 2*xf < length:\n",
      "          t.set_text(\"\")\n",
      "        else:\n",
      "          t.set_text(to_symbol(out[i]))\n",
      "    else:\n",
      "      for i, t in enumerate(text_fields):\n",
      "        t.set_text(to_symbol(inpt[i][batch]) if index < xf else \"\")\n",
      "      if index < xf:\n",
      "        im.set_array(np.zeros_like(steps[0][0]))\n",
      "      else:\n",
      "        im.set_array(steps[0][batch])\n",
      "    return im,\n",
      "  animation = anim.FuncAnimation(\n",
      "      fig, animation_update, blit=True, frames=(l+4*xf)*anim_size*fps,\n",
      "      interval=500/fps, fargs=(test_data, xf, im, text_fields))\n",
      "  animation.save(\"/tmp/neural_gpu.mp4\", writer=\"mencoder\", fps=4*fps, dpi=3*80)\n",
      "\n",
      "\n",
      "def evaluate():\n",
      "  \"\"\"Evaluate an existing model.\"\"\"\n",
      "  batch_size = FLAGS.batch_size\n",
      "  tasks = FLAGS.task.split(\"-\")\n",
      "  with tf.Session() as sess:\n",
      "    model, min_length, max_length, _, _ = initialize(sess)\n",
      "    bound = data.bins[-1] + 1\n",
      "    for t in tasks:\n",
      "      l = min_length\n",
      "      while l < max_length + 12 and l < bound:\n",
      "        _, sq, _ = single_test(l, model, sess, t, FLAGS.nprint, batch_size)\n",
      "        l += 1\n",
      "        while l < bound + 1 and not data.test_set[t][l]:\n",
      "          l += 1\n",
      "      # Animate.\n",
      "      anim_size = 2\n",
      "      _, _, test_data = single_test(l, model, sess, t, 0, anim_size)\n",
      "      animate(l, test_data, anim_size)\n",
      "      # More tests.\n",
      "      _, sq = multi_test(data.forward_max, model, sess, t, FLAGS.nprint,\n",
      "                         batch_size * 4)\n",
      "    if sq < 0.01:  # More tests.\n",
      "      if data.forward_max > 4000 and len(tasks) == 1:\n",
      "        multi_test(data.forward_max, model, sess, tasks[0], FLAGS.nprint,\n",
      "                   batch_size * 64, 0)\n",
      "\n",
      "\n",
      "def interactive():\n",
      "  \"\"\"Interactively probe an existing model.\"\"\"\n",
      "  with tf.Session() as sess:\n",
      "    model, _, _, _, _ = initialize(sess)\n",
      "    sys.stdout.write(\"> \")\n",
      "    sys.stdout.flush()\n",
      "    inpt = sys.stdin.readline()\n",
      "    while inpt:\n",
      "      ids = [int(c) for c in inpt.strip()]\n",
      "      inpt, target = data.get_batch(len(ids), 1, False, \"\",\n",
      "                                    preset=(ids, [0 for _ in ids]))\n",
      "      _, res, _, _ = model.step(sess, inpt, target, False)\n",
      "      res = [np.argmax(o, axis=1) for o in res]\n",
      "      print \" \".join([str(output[0]) for output in res])\n",
      "      sys.stdout.write(\"> \")\n",
      "      sys.stdout.flush()\n",
      "      inpt = sys.stdin.readline()\n",
      "\n",
      "\n",
      "def main(_):\n",
      "  if FLAGS.mode == 0:\n",
      "    train()\n",
      "  elif FLAGS.mode == 1:\n",
      "    evaluate()\n",
      "  else:\n",
      "    interactive()\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "  tf.app.run()\n",
      "\n",
      "@@ -0,0 +1,376 @@\n",
      "+\"\"\"Neural GPU for Learning Algorithms.\"\"\"\n",
      "+\n",
      "+import math\n",
      "+import os\n",
      "+import random\n",
      "+import sys\n",
      "+import time\n",
      "+\n",
      "+import google3\n",
      "+\n",
      "+import matplotlib.animation as anim\n",
      "+import matplotlib.pyplot as plt\n",
      "+import numpy as np\n",
      "+import tensorflow as tf\n",
      "+\n",
      "+from google3.third_party.tensorflow.python.platform import gfile\n",
      "+import google3.experimental.users.lukaszkaiser.neural_gpu.data_utils as data\n",
      "+import google3.experimental.users.lukaszkaiser.neural_gpu.neural_gpu as ngpu\n",
      "+\n",
      "+tf.app.flags.DEFINE_float(\"lr\", 0.1, \"Learning rate.\")\n",
      "+tf.app.flags.DEFINE_float(\"init_weight\", 1.0, \"Initial weights deviation.\")\n",
      "+tf.app.flags.DEFINE_float(\"max_grad_norm\", 0.05, \"Clip gradients to this norm.\")\n",
      "+tf.app.flags.DEFINE_float(\"cutoff\", 1.2, \"Cutoff at the gates.\")\n",
      "+tf.app.flags.DEFINE_float(\"pull\", 0.0005, \"Starting pull of the relaxations.\")\n",
      "+tf.app.flags.DEFINE_float(\"pull_incr\", 1.2, \"Increase pull by that much.\")\n",
      "+tf.app.flags.DEFINE_float(\"dropout\", 0.2, \"Dropout that much.\")\n",
      "+tf.app.flags.DEFINE_float(\"grad_noise_scale\", 1.0, \"Gradient noise scale.\")\n",
      "+tf.app.flags.DEFINE_integer(\"batch_size\", 64, \"Batch size.\")\n",
      "+tf.app.flags.DEFINE_integer(\"low_batch_size\", 16, \"Low batch size.\")\n",
      "+tf.app.flags.DEFINE_integer(\"steps_per_checkpoint\", 100, \"Steps per epoch.\")\n",
      "+tf.app.flags.DEFINE_integer(\"nmaps\", 24, \"Number of floats in each cell.\")\n",
      "+tf.app.flags.DEFINE_integer(\"niclass\", 14, \"Number of classes (0 is padding).\")\n",
      "+tf.app.flags.DEFINE_integer(\"noclass\", 14, \"Number of classes (0 is padding).\")\n",
      "+tf.app.flags.DEFINE_integer(\"train_data_size\", 5000, \"Training examples/len.\")\n",
      "+tf.app.flags.DEFINE_integer(\"max_length\", 41, \"Maximum length.\")\n",
      "+tf.app.flags.DEFINE_integer(\"rx_step\", 6, \"Relax that many recursive steps.\")\n",
      "+tf.app.flags.DEFINE_integer(\"random_seed\", 125459, \"Random seed.\")\n",
      "+tf.app.flags.DEFINE_integer(\"nconvs\", 2, \"How many convolutions / 1 step.\")\n",
      "+tf.app.flags.DEFINE_integer(\"kw\", 3, \"Kernel width.\")\n",
      "+tf.app.flags.DEFINE_integer(\"kh\", 3, \"Kernel height.\")\n",
      "+tf.app.flags.DEFINE_integer(\"height\", 4, \"Height.\")\n",
      "+tf.app.flags.DEFINE_integer(\"forward_max\", 401, \"Maximum forward length.\")\n",
      "+tf.app.flags.DEFINE_integer(\"jobid\", -1, \"Task id when running on borg.\")\n",
      "+tf.app.flags.DEFINE_integer(\"nprint\", 0, \"How many test examples to print out.\")\n",
      "+tf.app.flags.DEFINE_integer(\"mode\", 0, \"Mode: 0-train other-decode.\")\n",
      "+tf.app.flags.DEFINE_string(\"task\", \"rev\", \"Which task are we learning?\")\n",
      "+tf.app.flags.DEFINE_string(\"train_dir\", \"/tmp/\", \"Directory to store models.\")\n",
      "+\n",
      "+FLAGS = tf.app.flags.FLAGS\n",
      "+\n",
      "+\n",
      "+def initialize(sess):\n",
      "+  \"\"\"Initialize data and model.\"\"\"\n",
      "+  if FLAGS.jobid >= 0:\n",
      "+    data.log_filename = os.path.join(FLAGS.train_dir, \"log%d\" % FLAGS.jobid)\n",
      "+  data.print_out(\"NN \", newline=False)\n",
      "+\n",
      "+  # Set random seed.\n",
      "+  seed = FLAGS.random_seed + max(0, FLAGS.jobid)\n",
      "+  tf.set_random_seed(seed)\n",
      "+  random.seed(seed)\n",
      "+  np.random.seed(seed)\n",
      "+\n",
      "+  # Check data sizes.\n",
      "+  data.forward_max = max(FLAGS.forward_max, data.bins[-1])\n",
      "+  assert data.bins\n",
      "+  min_length = 3\n",
      "+  max_length = min(FLAGS.max_length, data.bins[-1])\n",
      "+  assert max_length + 1 > min_length\n",
      "+  while len(data.bins) > 1 and data.bins[-2] > max_length + 12:\n",
      "+    data.bins = data.bins[:-1]\n",
      "+  assert data.bins[0] > FLAGS.rx_step\n",
      "+  nclass = min(FLAGS.niclass, FLAGS.noclass)\n",
      "+  data_size = FLAGS.train_data_size if FLAGS.mode == 0 else 1000\n",
      "+\n",
      "+  # Initialize data for each task.\n",
      "+  tasks = FLAGS.task.split(\"-\")\n",
      "+  for t in tasks:\n",
      "+    for l in xrange(max_length + 11):\n",
      "+      data.init_data(t, l, data_size, nclass)\n",
      "+    data.init_data(t, data.bins[-2], data_size, nclass)\n",
      "+    data.init_data(t, data.bins[-1], data_size, nclass)\n",
      "+    end_size = 4 * 1024 if FLAGS.mode > 0 else 1024\n",
      "+    data.init_data(t, data.forward_max, end_size, nclass)\n",
      "+\n",
      "+  # Print out parameters.\n",
      "+  curriculum = 0.12\n",
      "+  fin = (\"cv %d kw %d h %d kh %d rxr %d bs %d ns %.2f t %s\"\n",
      "+         % (FLAGS.nconvs, FLAGS.kw, FLAGS.height, FLAGS.kh, FLAGS.rx_step,\n",
      "+            FLAGS.batch_size, FLAGS.grad_noise_scale, FLAGS.task))\n",
      "+  fin = \"data %d %s\" % (FLAGS.train_data_size, fin)\n",
      "+  tag = (\"df %.2f p %.3f lr %.2f iw %.2f cr %.2f nm %d d%.4f gn %.2f %s\" %\n",
      "+         (FLAGS.cutoff, FLAGS.pull_incr, FLAGS.lr, FLAGS.init_weight,\n",
      "+          curriculum, FLAGS.nmaps, FLAGS.dropout, FLAGS.max_grad_norm, fin))\n",
      "+  data.print_out(tag)\n",
      "+\n",
      "+  # Create checkpoint directory if it does not exist.\n",
      "+  checkpoint_dir = os.path.join(FLAGS.train_dir, \"neural_gpu%s\"\n",
      "+                                % (\"\" if FLAGS.jobid < 0 else str(FLAGS.jobid)))\n",
      "+  if not gfile.IsDirectory(checkpoint_dir):\n",
      "+    data.print_out(\"Creating checkpoint directory %s.\" % checkpoint_dir)\n",
      "+    gfile.MkDir(checkpoint_dir)\n",
      "+\n",
      "+  # Create model and initialize it.\n",
      "+  tf.get_variable_scope().set_initializer(\n",
      "+      tf.uniform_unit_scaling_initializer(factor=1.8 * FLAGS.init_weight))\n",
      "+  model = ngpu.NeuralGPU(\n",
      "+      FLAGS.nmaps, FLAGS.nmaps, FLAGS.niclass, FLAGS.noclass, FLAGS.dropout,\n",
      "+      FLAGS.rx_step, FLAGS.max_grad_norm, FLAGS.cutoff, FLAGS.nconvs,\n",
      "+      FLAGS.kw, FLAGS.kh, FLAGS.height, FLAGS.mode, FLAGS.lr,\n",
      "+      FLAGS.pull, FLAGS.pull_incr, min_length + 3)\n",
      "+  data.print_out(\"Created model.\")\n",
      "+  sess.run(tf.initialize_all_variables())\n",
      "+  data.print_out(\"Initialized variables.\")\n",
      "+\n",
      "+  # Load model from parameters if a checkpoint exists.\n",
      "+  ckpt = tf.train.get_checkpoint_state(checkpoint_dir)\n",
      "+  if ckpt and gfile.Exists(ckpt.model_checkpoint_path):\n",
      "+    data.print_out(\"Reading model parameters from %s\"\n",
      "+                   % ckpt.model_checkpoint_path)\n",
      "+    model.saver.restore(sess, ckpt.model_checkpoint_path)\n",
      "+\n",
      "+  # Return the model and needed variables.\n",
      "+  return (model, min_length, max_length, checkpoint_dir, curriculum)\n",
      "+\n",
      "+\n",
      "+def single_test(l, model, sess, task, nprint, batch_size, print_out=True,\n",
      "+                offset=None):\n",
      "+  \"\"\"Test model on test data of length l using the given session.\"\"\"\n",
      "+  inpt, target = data.get_batch(l, batch_size, False, task, offset)\n",
      "+  _, res, _, steps = model.step(sess, inpt, target, False)\n",
      "+  errors, total, seq = data.accuracy(inpt, res, target, batch_size, nprint)\n",
      "+  seq = float(seq) / batch_size\n",
      "+  if total > 0:\n",
      "+    errors = float(errors) / total\n",
      "+  if print_out:\n",
      "+    data.print_out(\"  %s len %d errors %.2f sequence-errors %.2f\"\n",
      "+                   % (task, l, 100*errors, 100*seq))\n",
      "+  return errors, seq, (steps, inpt, [np.argmax(o, axis=1) for o in res])\n",
      "+\n",
      "+\n",
      "+def multi_test(l, model, sess, task, nprint, batch_size, offset=None):\n",
      "+  \"\"\"Run multiple tests at lower batch size to save memory.\"\"\"\n",
      "+  errors = 0.0\n",
      "+  seq = 0.0\n",
      "+  to_print = nprint\n",
      "+  low_batch = FLAGS.low_batch_size\n",
      "+  low_batch = min(low_batch, batch_size)\n",
      "+  for mstep in xrange(batch_size / low_batch):\n",
      "+    cur_offset = None if offset is None else offset + mstep * low_batch\n",
      "+    err, sq, _ = single_test(l, model, sess, task, to_print, low_batch, False,\n",
      "+                             cur_offset)\n",
      "+    to_print = max(0, to_print - low_batch)\n",
      "+    errors += err\n",
      "+    seq += sq\n",
      "+    if FLAGS.mode > 0:\n",
      "+      cur_errors = float(low_batch * errors) / ((mstep+1) * low_batch)\n",
      "+      cur_seq = float(low_batch * seq) / ((mstep+1) * low_batch)\n",
      "+      data.print_out(\"    %s multitest current errors %.2f sequence-errors %.2f\"\n",
      "+                     % (task, 100*cur_errors, 100*cur_seq))\n",
      "+  errors = float(low_batch) * float(errors) / batch_size\n",
      "+  seq = float(low_batch) * float(seq) / batch_size\n",
      "+  data.print_out(\"  %s len %d errors %.2f sequence-errors %.2f\"\n",
      "+                 % (task, l, 100*errors, 100*seq))\n",
      "+  return errors, seq\n",
      "+\n",
      "+\n",
      "+def train():\n",
      "+  \"\"\"Main training function.\"\"\"\n",
      "+  batch_size = FLAGS.batch_size\n",
      "+  tasks = FLAGS.task.split(\"-\")\n",
      "+  with tf.Session() as sess:\n",
      "+    model, min_length, max_length, checkpoint_dir, curriculum = initialize(sess)\n",
      "+    max_cur_length = min(min_length + 3, max_length)\n",
      "+    prev_acc_perp = [1000000 for _ in xrange(3)]\n",
      "+    prev_sq = 1.0\n",
      "+\n",
      "+    while True:\n",
      "+      global_step, pull, max_cur_length, learning_rate = sess.run(\n",
      "+          [model.global_step, model.pull, model.cur_length, model.lr])\n",
      "+      ep = global_step / FLAGS.steps_per_checkpoint\n",
      "+      acc_loss, acc_total, acc_errors, acc_seq = 0.0, 0, 0, 0\n",
      "+      acc_grad_norm, step_count, step_time = 0.0, 0, 0.0\n",
      "+      for _ in xrange(FLAGS.steps_per_checkpoint):\n",
      "+        global_step += 1\n",
      "+        task = random.choice(tasks)\n",
      "+        l1 = np.random.randint(max_cur_length - min_length + 1) + min_length\n",
      "+        l = l1\n",
      "+        if np.random.randint(10) > 3:  # Prefer longer stuff 60% of time.\n",
      "+          l = np.random.randint(max_cur_length - min_length+1) + min_length\n",
      "+          l = max(l, l1)\n",
      "+        if np.random.randint(4) < 1:  # Mixed learning: once in a while big.\n",
      "+          l = np.random.randint(max_length - min_length + 1) + min_length\n",
      "+          l = max(l, l1)\n",
      "+        start_time = time.time()\n",
      "+        inp, target = data.get_batch(l, batch_size, True, task)\n",
      "+        stepp = math.pow(global_step, -0.55)\n",
      "+        noise_param = math.sqrt(stepp * 20 * prev_sq) * FLAGS.grad_noise_scale\n",
      "+        loss, res, gnorm, _ = model.step(sess, inp, target, True, noise_param)\n",
      "+        step_time += time.time() - start_time\n",
      "+        acc_grad_norm += float(gnorm)\n",
      "+        if l < max_cur_length + 1:\n",
      "+          step_count += 1\n",
      "+          acc_loss += loss\n",
      "+          errors, total, seq = data.accuracy(inp, res, target,\n",
      "+                                             batch_size, 0)\n",
      "+          acc_total += total\n",
      "+          acc_errors += errors\n",
      "+          acc_seq += seq\n",
      "+      acc_loss /= step_count\n",
      "+      step_time /= FLAGS.steps_per_checkpoint\n",
      "+      acc_seq = float(acc_seq) / (step_count * batch_size)\n",
      "+      prev_sq = acc_seq\n",
      "+      acc_errors = float(acc_errors) / acc_total if acc_total > 0 else 1.0\n",
      "+      msg1 = \"ep %d st %.2f lr %.8f\" % (ep, step_time, learning_rate)\n",
      "+      msg2 = \"pl %.3f cme %.3f\" % (pull, curriculum)\n",
      "+      msg = (\"%s %s gn %.8f\"\n",
      "+             % (msg1, msg2, acc_grad_norm / FLAGS.steps_per_checkpoint))\n",
      "+      data.print_out(\"%s len %d ppx %.8f errs %.2f sq %.2f\" %\n",
      "+                     (msg, max_cur_length, data.safe_exp(acc_loss),\n",
      "+                      100*acc_errors, 100*acc_seq))\n",
      "+      if curriculum > acc_seq:\n",
      "+        prev_acc_perp.append(1000000)\n",
      "+        do_incr = True\n",
      "+        while do_incr and max_cur_length < max_length:\n",
      "+          sess.run(model.cur_length_incr_op)\n",
      "+          for t in tasks:\n",
      "+            if data.train_set[t]: do_incr = False\n",
      "+        if pull < 1:\n",
      "+          sess.run(model.pull_incr_op)\n",
      "+        else:\n",
      "+          data.print_out(\"  Averaging parameters.\")\n",
      "+          sess.run([model.avg_op, model.lr_decay_op])\n",
      "+      else:\n",
      "+        acc_perp = data.safe_exp(acc_loss)\n",
      "+        if acc_perp > max(prev_acc_perp[-3:]):\n",
      "+          sess.run(model.lr_decay_op)\n",
      "+        prev_acc_perp.append(acc_perp)\n",
      "+      checkpoint_path = os.path.join(checkpoint_dir, \"neural_gpu.ckpt\")\n",
      "+      model.saver.save(sess, checkpoint_path,\n",
      "+                       global_step=model.global_step)\n",
      "+      # Run evaluation.\n",
      "+      should_exit = True\n",
      "+      bound = data.bins[-1] + 1\n",
      "+      for t in tasks:\n",
      "+        l = min_length\n",
      "+        while l < max_length + 12 and l < bound:\n",
      "+          _, sq, _ = single_test(l, model, sess, t, FLAGS.nprint, batch_size)\n",
      "+          l += 1\n",
      "+          while l < bound + 1 and not data.test_set[t][l]:\n",
      "+            l += 1\n",
      "+        if sq < 0.5:\n",
      "+          _, sq = multi_test(data.forward_max, model, sess, t, FLAGS.nprint,\n",
      "+                             batch_size * 4)\n",
      "+        if sq > 0.001: should_exit = False\n",
      "+      if should_exit:\n",
      "+        if data.forward_max > 4000 and len(tasks) == 1:\n",
      "+          multi_test(data.forward_max, model, sess, tasks[0], FLAGS.nprint,\n",
      "+                     batch_size * 16, 0)\n",
      "+\n",
      "+\n",
      "+def animate(l, test_data, anim_size):\n",
      "+  \"\"\"Create animation for the given data (hacky matplotlib use).\"\"\"\n",
      "+  xf = 12\n",
      "+  fps = 2\n",
      "+  fig = plt.figure(figsize=(16, 9), facecolor=\"white\")\n",
      "+  ax = fig.add_axes([0, 0, 1, 1], frameon=False, zorder=2)\n",
      "+  ax.set_xticks([i * 24-0.5 for i in xrange(4)])\n",
      "+  ax.set_xticklabels([])\n",
      "+  ax.set_yticks([i - 0.5 for i in xrange(l+1)])\n",
      "+  ax.grid(which=\"major\", axis=\"both\", linestyle=\"-\", color=\"black\")\n",
      "+  text_fields = []\n",
      "+  text_size = 24*32/l\n",
      "+  for y in xrange(l):\n",
      "+    text_fields.append(ax.text(\n",
      "+        11.25, y + 0.15, \"\", color=\"g\", ha=\"center\", va=\"center\",\n",
      "+        bbox={\"facecolor\": \"b\", \"alpha\": 0.01, \"pad\": 24 * text_size},\n",
      "+        size=text_size - (4 * 32 / l), animated=True))\n",
      "+  im = ax.imshow(np.zeros_like(test_data[0][0][0]), vmin=-1.0,\n",
      "+                 vmax=1.0, cmap=\"gray\", aspect=\"auto\", origin=\"upper\",\n",
      "+                 interpolation=\"none\", animated=True)\n",
      "+  im.set_zorder(1)\n",
      "+  def to_symbol(i):\n",
      "+    if i == 0: return \"\"\n",
      "+    if i == 11: return \"+\"\n",
      "+    if i == 12: return \"*\"\n",
      "+    return str(i-1)\n",
      "+  def animation_update(frame_no, test_data, xf, im, text_fields):\n",
      "+    \"\"\"Update an animation frame.\"\"\"\n",
      "+    steps, inpt, out_raw = test_data\n",
      "+    length = len(steps)\n",
      "+    batch = frame_no / (fps * (l+4*xf))\n",
      "+    index = int((frame_no % (fps * (l+4*xf))) / fps)\n",
      "+    # Cut output after first padding.\n",
      "+    out = [out_raw[i][batch] for i in xrange(len(text_fields))]\n",
      "+    if 0 in out:\n",
      "+      i = out.index(0)\n",
      "+      out = out[0:i] + [0 for _ in xrange(len(out) - i)]\n",
      "+    # Show the state after the first frames.\n",
      "+    if index >= 2*xf:\n",
      "+      im.set_array(steps[min(length - 1, index - 2*xf)][batch])\n",
      "+      for i, t in enumerate(text_fields):\n",
      "+        if index - 2*xf < length:\n",
      "+          t.set_text(\"\")\n",
      "+        else:\n",
      "+          t.set_text(to_symbol(out[i]))\n",
      "+    else:\n",
      "+      for i, t in enumerate(text_fields):\n",
      "+        t.set_text(to_symbol(inpt[i][batch]) if index < xf else \"\")\n",
      "+      if index < xf:\n",
      "+        im.set_array(np.zeros_like(steps[0][0]))\n",
      "+      else:\n",
      "+        im.set_array(steps[0][batch])\n",
      "+    return im,\n",
      "+  animation = anim.FuncAnimation(\n",
      "+      fig, animation_update, blit=True, frames=(l+4*xf)*anim_size*fps,\n",
      "+      interval=500/fps, fargs=(test_data, xf, im, text_fields))\n",
      "+  animation.save(\"/tmp/neural_gpu.mp4\", writer=\"mencoder\", fps=4*fps, dpi=3*80)\n",
      "+\n",
      "+\n",
      "+def evaluate():\n",
      "+  \"\"\"Evaluate an existing model.\"\"\"\n",
      "+  batch_size = FLAGS.batch_size\n",
      "+  tasks = FLAGS.task.split(\"-\")\n",
      "+  with tf.Session() as sess:\n",
      "+    model, min_length, max_length, _, _ = initialize(sess)\n",
      "+    bound = data.bins[-1] + 1\n",
      "+    for t in tasks:\n",
      "+      l = min_length\n",
      "+      while l < max_length + 12 and l < bound:\n",
      "+        _, sq, _ = single_test(l, model, sess, t, FLAGS.nprint, batch_size)\n",
      "+        l += 1\n",
      "+        while l < bound + 1 and not data.test_set[t][l]:\n",
      "+          l += 1\n",
      "+      # Animate.\n",
      "+      anim_size = 2\n",
      "+      _, _, test_data = single_test(l, model, sess, t, 0, anim_size)\n",
      "+      animate(l, test_data, anim_size)\n",
      "+      # More tests.\n",
      "+      _, sq = multi_test(data.forward_max, model, sess, t, FLAGS.nprint,\n",
      "+                         batch_size * 4)\n",
      "+    if sq < 0.01:  # More tests.\n",
      "+      if data.forward_max > 4000 and len(tasks) == 1:\n",
      "+        multi_test(data.forward_max, model, sess, tasks[0], FLAGS.nprint,\n",
      "+                   batch_size * 64, 0)\n",
      "+\n",
      "+\n",
      "+def interactive():\n",
      "+  \"\"\"Interactively probe an existing model.\"\"\"\n",
      "+  with tf.Session() as sess:\n",
      "+    model, _, _, _, _ = initialize(sess)\n",
      "+    sys.stdout.write(\"> \")\n",
      "+    sys.stdout.flush()\n",
      "+    inpt = sys.stdin.readline()\n",
      "+    while inpt:\n",
      "+      ids = [int(c) for c in inpt.strip()]\n",
      "+      inpt, target = data.get_batch(len(ids), 1, False, \"\",\n",
      "+                                    preset=(ids, [0 for _ in ids]))\n",
      "+      _, res, _, _ = model.step(sess, inpt, target, False)\n",
      "+      res = [np.argmax(o, axis=1) for o in res]\n",
      "+      print \" \".join([str(output[0]) for output in res])\n",
      "+      sys.stdout.write(\"> \")\n",
      "+      sys.stdout.flush()\n",
      "+      inpt = sys.stdin.readline()\n",
      "+\n",
      "+\n",
      "+def main(_):\n",
      "+  if FLAGS.mode == 0:\n",
      "+    train()\n",
      "+  elif FLAGS.mode == 1:\n",
      "+    evaluate()\n",
      "+  else:\n",
      "+    interactive()\n",
      "+\n",
      "+if __name__ == \"__main__\":\n",
      "+  tf.app.run()\n",
      "\n",
      "\n",
      "commit #3, \n",
      "message: added license, minor adjustments to README.md's.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__Before__: \n",
      "None\n",
      "__After__: \n",
      "# Contributing guidelines\n",
      "\n",
      "If you have created a model and would like to publish it here, please send us a\n",
      "pull request. The code for any model in this repository is licensed under the\n",
      "Apache License 2.0.\n",
      "\n",
      "In order to accept our code, we have to make sure that we can publish your code:\n",
      "You have to sign a Contributor License Agreement (CLA).\n",
      "\n",
      "### Contributor License Agreements\n",
      "\n",
      "We'd love to accept your patches! Before we can take them, we have to jump a couple of legal hurdles.\n",
      "\n",
      "Please fill out either the individual or corporate Contributor License Agreement (CLA).\n",
      "\n",
      "  * If you are an individual writing original source code and you're sure you own the intellectual property, then you'll need to sign an [individual CLA](http://code.google.com/legal/individual-cla-v1.0.html).\n",
      "  * If you work for a company that wants to allow you to contribute your work, then you'll need to sign a [corporate CLA](http://code.google.com/legal/corporate-cla-v1.0.html).\n",
      "\n",
      "Follow either of the two links above to access the appropriate CLA and instructions for how to sign and return it. Once we receive it, we'll be able to accept your pull requests.\n",
      "\n",
      "***NOTE***: Only original source code from you and other people that have signed the CLA can be accepted into the repository.\n",
      "\n",
      "### Contributing code\n",
      "\n",
      "If you have improvements to TensorFlow, send us your pull requests! For those\n",
      "just getting started, Github has a [howto](https://help.github.com/articles/using-pull-requests/).\n",
      "\n",
      "If you want to contribute but you're not sure where to start, take a look at the\n",
      "[issues with the \"contributions welcome\" label](https://github.com/tensorflow/tensorflow/labels/contributions%20welcome).\n",
      "These are issues that we believe are particularly well suited for outside\n",
      "contributions, often because we probably won't get to them right now. If you\n",
      "decide to start on an issue, leave a comment so that other people know that\n",
      "you're working on it. If you want to help out, but not alone, use the issue\n",
      "comment thread to coordinate.\n",
      "\n",
      "@@ -0,0 +1,34 @@\n",
      "+# Contributing guidelines\n",
      "+\n",
      "+If you have created a model and would like to publish it here, please send us a\n",
      "+pull request. The code for any model in this repository is licensed under the\n",
      "+Apache License 2.0.\n",
      "+\n",
      "+In order to accept our code, we have to make sure that we can publish your code:\n",
      "+You have to sign a Contributor License Agreement (CLA).\n",
      "+\n",
      "+### Contributor License Agreements\n",
      "+\n",
      "+We'd love to accept your patches! Before we can take them, we have to jump a couple of legal hurdles.\n",
      "+\n",
      "+Please fill out either the individual or corporate Contributor License Agreement (CLA).\n",
      "+\n",
      "+  * If you are an individual writing original source code and you're sure you own the intellectual property, then you'll need to sign an [individual CLA](http://code.google.com/legal/individual-cla-v1.0.html).\n",
      "+  * If you work for a company that wants to allow you to contribute your work, then you'll need to sign a [corporate CLA](http://code.google.com/legal/corporate-cla-v1.0.html).\n",
      "+\n",
      "+Follow either of the two links above to access the appropriate CLA and instructions for how to sign and return it. Once we receive it, we'll be able to accept your pull requests.\n",
      "+\n",
      "+***NOTE***: Only original source code from you and other people that have signed the CLA can be accepted into the repository.\n",
      "+\n",
      "+### Contributing code\n",
      "+\n",
      "+If you have improvements to TensorFlow, send us your pull requests! For those\n",
      "+just getting started, Github has a [howto](https://help.github.com/articles/using-pull-requests/).\n",
      "+\n",
      "+If you want to contribute but you're not sure where to start, take a look at the\n",
      "+[issues with the \"contributions welcome\" label](https://github.com/tensorflow/tensorflow/labels/contributions%20welcome).\n",
      "+These are issues that we believe are particularly well suited for outside\n",
      "+contributions, often because we probably won't get to them right now. If you\n",
      "+decide to start on an issue, leave a comment so that other people know that\n",
      "+you're working on it. If you want to help out, but not alone, use the issue\n",
      "+comment thread to coordinate.\n",
      "\n",
      "\n",
      "__Before__: \n",
      "None\n",
      "__After__: \n",
      "Copyright 2015 The Authors.  All rights reserved.\n",
      "\n",
      "                                 Apache License\n",
      "                           Version 2.0, January 2004\n",
      "                        http://www.apache.org/licenses/\n",
      "\n",
      "   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\n",
      "\n",
      "   1. Definitions.\n",
      "\n",
      "      \"License\" shall mean the terms and conditions for use, reproduction,\n",
      "      and distribution as defined by Sections 1 through 9 of this document.\n",
      "\n",
      "      \"Licensor\" shall mean the copyright owner or entity authorized by\n",
      "      the copyright owner that is granting the License.\n",
      "\n",
      "      \"Legal Entity\" shall mean the union of the acting entity and all\n",
      "      other entities that control, are controlled by, or are under common\n",
      "      control with that entity. For the purposes of this definition,\n",
      "      \"control\" means (i) the power, direct or indirect, to cause the\n",
      "      direction or management of such entity, whether by contract or\n",
      "      otherwise, or (ii) ownership of fifty percent (50%) or more of the\n",
      "      outstanding shares, or (iii) beneficial ownership of such entity.\n",
      "\n",
      "      \"You\" (or \"Your\") shall mean an individual or Legal Entity\n",
      "      exercising permissions granted by this License.\n",
      "\n",
      "      \"Source\" form shall mean the preferred form for making modifications,\n",
      "      including but not limited to software source code, documentation\n",
      "      source, and configuration files.\n",
      "\n",
      "      \"Object\" form shall mean any form resulting from mechanical\n",
      "      transformation or translation of a Source form, including but\n",
      "      not limited to compiled object code, generated documentation,\n",
      "      and conversions to other media types.\n",
      "\n",
      "      \"Work\" shall mean the work of authorship, whether in Source or\n",
      "      Object form, made available under the License, as indicated by a\n",
      "      copyright notice that is included in or attached to the work\n",
      "      (an example is provided in the Appendix below).\n",
      "\n",
      "      \"Derivative Works\" shall mean any work, whether in Source or Object\n",
      "      form, that is based on (or derived from) the Work and for which the\n",
      "      editorial revisions, annotations, elaborations, or other modifications\n",
      "      represent, as a whole, an original work of authorship. For the purposes\n",
      "      of this License, Derivative Works shall not include works that remain\n",
      "      separable from, or merely link (or bind by name) to the interfaces of,\n",
      "      the Work and Derivative Works thereof.\n",
      "\n",
      "      \"Contribution\" shall mean any work of authorship, including\n",
      "      the original version of the Work and any modifications or additions\n",
      "      to that Work or Derivative Works thereof, that is intentionally\n",
      "      submitted to Licensor for inclusion in the Work by the copyright owner\n",
      "      or by an individual or Legal Entity authorized to submit on behalf of\n",
      "      the copyright owner. For the purposes of this definition, \"submitted\"\n",
      "      means any form of electronic, verbal, or written communication sent\n",
      "      to the Licensor or its representatives, including but not limited to\n",
      "      communication on electronic mailing lists, source code control systems,\n",
      "      and issue tracking systems that are managed by, or on behalf of, the\n",
      "      Licensor for the purpose of discussing and improving the Work, but\n",
      "      excluding communication that is conspicuously marked or otherwise\n",
      "      designated in writing by the copyright owner as \"Not a Contribution.\"\n",
      "\n",
      "      \"Contributor\" shall mean Licensor and any individual or Legal Entity\n",
      "      on behalf of whom a Contribution has been received by Licensor and\n",
      "      subsequently incorporated within the Work.\n",
      "\n",
      "   2. Grant of Copyright License. Subject to the terms and conditions of\n",
      "      this License, each Contributor hereby grants to You a perpetual,\n",
      "      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n",
      "      copyright license to reproduce, prepare Derivative Works of,\n",
      "      publicly display, publicly perform, sublicense, and distribute the\n",
      "      Work and such Derivative Works in Source or Object form.\n",
      "\n",
      "   3. Grant of Patent License. Subject to the terms and conditions of\n",
      "      this License, each Contributor hereby grants to You a perpetual,\n",
      "      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n",
      "      (except as stated in this section) patent license to make, have made,\n",
      "      use, offer to sell, sell, import, and otherwise transfer the Work,\n",
      "      where such license applies only to those patent claims licensable\n",
      "      by such Contributor that are necessarily infringed by their\n",
      "      Contribution(s) alone or by combination of their Contribution(s)\n",
      "      with the Work to which such Contribution(s) was submitted. If You\n",
      "      institute patent litigation against any entity (including a\n",
      "      cross-claim or counterclaim in a lawsuit) alleging that the Work\n",
      "      or a Contribution incorporated within the Work constitutes direct\n",
      "      or contributory patent infringement, then any patent licenses\n",
      "      granted to You under this License for that Work shall terminate\n",
      "      as of the date such litigation is filed.\n",
      "\n",
      "   4. Redistribution. You may reproduce and distribute copies of the\n",
      "      Work or Derivative Works thereof in any medium, with or without\n",
      "      modifications, and in Source or Object form, provided that You\n",
      "      meet the following conditions:\n",
      "\n",
      "      (a) You must give any other recipients of the Work or\n",
      "          Derivative Works a copy of this License; and\n",
      "\n",
      "      (b) You must cause any modified files to carry prominent notices\n",
      "          stating that You changed the files; and\n",
      "\n",
      "      (c) You must retain, in the Source form of any Derivative Works\n",
      "          that You distribute, all copyright, patent, trademark, and\n",
      "          attribution notices from the Source form of the Work,\n",
      "          excluding those notices that do not pertain to any part of\n",
      "          the Derivative Works; and\n",
      "\n",
      "      (d) If the Work includes a \"NOTICE\" text file as part of its\n",
      "          distribution, then any Derivative Works that You distribute must\n",
      "          include a readable copy of the attribution notices contained\n",
      "          within such NOTICE file, excluding those notices that do not\n",
      "          pertain to any part of the Derivative Works, in at least one\n",
      "          of the following places: within a NOTICE text file distributed\n",
      "          as part of the Derivative Works; within the Source form or\n",
      "          documentation, if provided along with the Derivative Works; or,\n",
      "          within a display generated by the Derivative Works, if and\n",
      "          wherever such third-party notices normally appear. The contents\n",
      "          of the NOTICE file are for informational purposes only and\n",
      "          do not modify the License. You may add Your own attribution\n",
      "          notices within Derivative Works that You distribute, alongside\n",
      "          or as an addendum to the NOTICE text from the Work, provided\n",
      "          that such additional attribution notices cannot be construed\n",
      "          as modifying the License.\n",
      "\n",
      "      You may add Your own copyright statement to Your modifications and\n",
      "      may provide additional or different license terms and conditions\n",
      "      for use, reproduction, or distribution of Your modifications, or\n",
      "      for any such Derivative Works as a whole, provided Your use,\n",
      "      reproduction, and distribution of the Work otherwise complies with\n",
      "      the conditions stated in this License.\n",
      "\n",
      "   5. Submission of Contributions. Unless You explicitly state otherwise,\n",
      "      any Contribution intentionally submitted for inclusion in the Work\n",
      "      by You to the Licensor shall be under the terms and conditions of\n",
      "      this License, without any additional terms or conditions.\n",
      "      Notwithstanding the above, nothing herein shall supersede or modify\n",
      "      the terms of any separate license agreement you may have executed\n",
      "      with Licensor regarding such Contributions.\n",
      "\n",
      "   6. Trademarks. This License does not grant permission to use the trade\n",
      "      names, trademarks, service marks, or product names of the Licensor,\n",
      "      except as required for reasonable and customary use in describing the\n",
      "      origin of the Work and reproducing the content of the NOTICE file.\n",
      "\n",
      "   7. Disclaimer of Warranty. Unless required by applicable law or\n",
      "      agreed to in writing, Licensor provides the Work (and each\n",
      "      Contributor provides its Contributions) on an \"AS IS\" BASIS,\n",
      "      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n",
      "      implied, including, without limitation, any warranties or conditions\n",
      "      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\n",
      "      PARTICULAR PURPOSE. You are solely responsible for determining the\n",
      "      appropriateness of using or redistributing the Work and assume any\n",
      "      risks associated with Your exercise of permissions under this License.\n",
      "\n",
      "   8. Limitation of Liability. In no event and under no legal theory,\n",
      "      whether in tort (including negligence), contract, or otherwise,\n",
      "      unless required by applicable law (such as deliberate and grossly\n",
      "      negligent acts) or agreed to in writing, shall any Contributor be\n",
      "      liable to You for damages, including any direct, indirect, special,\n",
      "      incidental, or consequential damages of any character arising as a\n",
      "      result of this License or out of the use or inability to use the\n",
      "      Work (including but not limited to damages for loss of goodwill,\n",
      "      work stoppage, computer failure or malfunction, or any and all\n",
      "      other commercial damages or losses), even if such Contributor\n",
      "      has been advised of the possibility of such damages.\n",
      "\n",
      "   9. Accepting Warranty or Additional Liability. While redistributing\n",
      "      the Work or Derivative Works thereof, You may choose to offer,\n",
      "      and charge a fee for, acceptance of support, warranty, indemnity,\n",
      "      or other liability obligations and/or rights consistent with this\n",
      "      License. However, in accepting such obligations, You may act only\n",
      "      on Your own behalf and on Your sole responsibility, not on behalf\n",
      "      of any other Contributor, and only if You agree to indemnify,\n",
      "      defend, and hold each Contributor harmless for any liability\n",
      "      incurred by, or claims asserted against, such Contributor by reason\n",
      "      of your accepting any such warranty or additional liability.\n",
      "\n",
      "   END OF TERMS AND CONDITIONS\n",
      "\n",
      "   APPENDIX: How to apply the Apache License to your work.\n",
      "\n",
      "      To apply the Apache License to your work, attach the following\n",
      "      boilerplate notice, with the fields enclosed by brackets \"[]\"\n",
      "      replaced with your own identifying information. (Don't include\n",
      "      the brackets!)  The text should be enclosed in the appropriate\n",
      "      comment syntax for the file format. We also recommend that a\n",
      "      file or class name and description of purpose be included on the\n",
      "      same \"printed page\" as the copyright notice for easier\n",
      "      identification within third-party archives.\n",
      "\n",
      "   Copyright 2015, The TensorFlow Authors.\n",
      "\n",
      "   Licensed under the Apache License, Version 2.0 (the \"License\");\n",
      "   you may not use this file except in compliance with the License.\n",
      "   You may obtain a copy of the License at\n",
      "\n",
      "       http://www.apache.org/licenses/LICENSE-2.0\n",
      "\n",
      "   Unless required by applicable law or agreed to in writing, software\n",
      "   distributed under the License is distributed on an \"AS IS\" BASIS,\n",
      "   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
      "   See the License for the specific language governing permissions and\n",
      "   limitations under the License.\n",
      "\n",
      "@@ -0,0 +1,203 @@\n",
      "+Copyright 2015 The Authors.  All rights reserved.\n",
      "+\n",
      "+                                 Apache License\n",
      "+                           Version 2.0, January 2004\n",
      "+                        http://www.apache.org/licenses/\n",
      "+\n",
      "+   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\n",
      "+\n",
      "+   1. Definitions.\n",
      "+\n",
      "+      \"License\" shall mean the terms and conditions for use, reproduction,\n",
      "+      and distribution as defined by Sections 1 through 9 of this document.\n",
      "+\n",
      "+      \"Licensor\" shall mean the copyright owner or entity authorized by\n",
      "+      the copyright owner that is granting the License.\n",
      "+\n",
      "+      \"Legal Entity\" shall mean the union of the acting entity and all\n",
      "+      other entities that control, are controlled by, or are under common\n",
      "+      control with that entity. For the purposes of this definition,\n",
      "+      \"control\" means (i) the power, direct or indirect, to cause the\n",
      "+      direction or management of such entity, whether by contract or\n",
      "+      otherwise, or (ii) ownership of fifty percent (50%) or more of the\n",
      "+      outstanding shares, or (iii) beneficial ownership of such entity.\n",
      "+\n",
      "+      \"You\" (or \"Your\") shall mean an individual or Legal Entity\n",
      "+      exercising permissions granted by this License.\n",
      "+\n",
      "+      \"Source\" form shall mean the preferred form for making modifications,\n",
      "+      including but not limited to software source code, documentation\n",
      "+      source, and configuration files.\n",
      "+\n",
      "+      \"Object\" form shall mean any form resulting from mechanical\n",
      "+      transformation or translation of a Source form, including but\n",
      "+      not limited to compiled object code, generated documentation,\n",
      "+      and conversions to other media types.\n",
      "+\n",
      "+      \"Work\" shall mean the work of authorship, whether in Source or\n",
      "+      Object form, made available under the License, as indicated by a\n",
      "+      copyright notice that is included in or attached to the work\n",
      "+      (an example is provided in the Appendix below).\n",
      "+\n",
      "+      \"Derivative Works\" shall mean any work, whether in Source or Object\n",
      "+      form, that is based on (or derived from) the Work and for which the\n",
      "+      editorial revisions, annotations, elaborations, or other modifications\n",
      "+      represent, as a whole, an original work of authorship. For the purposes\n",
      "+      of this License, Derivative Works shall not include works that remain\n",
      "+      separable from, or merely link (or bind by name) to the interfaces of,\n",
      "+      the Work and Derivative Works thereof.\n",
      "+\n",
      "+      \"Contribution\" shall mean any work of authorship, including\n",
      "+      the original version of the Work and any modifications or additions\n",
      "+      to that Work or Derivative Works thereof, that is intentionally\n",
      "+      submitted to Licensor for inclusion in the Work by the copyright owner\n",
      "+      or by an individual or Legal Entity authorized to submit on behalf of\n",
      "+      the copyright owner. For the purposes of this definition, \"submitted\"\n",
      "+      means any form of electronic, verbal, or written communication sent\n",
      "+      to the Licensor or its representatives, including but not limited to\n",
      "+      communication on electronic mailing lists, source code control systems,\n",
      "+      and issue tracking systems that are managed by, or on behalf of, the\n",
      "+      Licensor for the purpose of discussing and improving the Work, but\n",
      "+      excluding communication that is conspicuously marked or otherwise\n",
      "+      designated in writing by the copyright owner as \"Not a Contribution.\"\n",
      "+\n",
      "+      \"Contributor\" shall mean Licensor and any individual or Legal Entity\n",
      "+      on behalf of whom a Contribution has been received by Licensor and\n",
      "+      subsequently incorporated within the Work.\n",
      "+\n",
      "+   2. Grant of Copyright License. Subject to the terms and conditions of\n",
      "+      this License, each Contributor hereby grants to You a perpetual,\n",
      "+      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n",
      "+      copyright license to reproduce, prepare Derivative Works of,\n",
      "+      publicly display, publicly perform, sublicense, and distribute the\n",
      "+      Work and such Derivative Works in Source or Object form.\n",
      "+\n",
      "+   3. Grant of Patent License. Subject to the terms and conditions of\n",
      "+      this License, each Contributor hereby grants to You a perpetual,\n",
      "+      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n",
      "+      (except as stated in this section) patent license to make, have made,\n",
      "+      use, offer to sell, sell, import, and otherwise transfer the Work,\n",
      "+      where such license applies only to those patent claims licensable\n",
      "+      by such Contributor that are necessarily infringed by their\n",
      "+      Contribution(s) alone or by combination of their Contribution(s)\n",
      "+      with the Work to which such Contribution(s) was submitted. If You\n",
      "+      institute patent litigation against any entity (including a\n",
      "+      cross-claim or counterclaim in a lawsuit) alleging that the Work\n",
      "+      or a Contribution incorporated within the Work constitutes direct\n",
      "+      or contributory patent infringement, then any patent licenses\n",
      "+      granted to You under this License for that Work shall terminate\n",
      "+      as of the date such litigation is filed.\n",
      "+\n",
      "+   4. Redistribution. You may reproduce and distribute copies of the\n",
      "+      Work or Derivative Works thereof in any medium, with or without\n",
      "+      modifications, and in Source or Object form, provided that You\n",
      "+      meet the following conditions:\n",
      "+\n",
      "+      (a) You must give any other recipients of the Work or\n",
      "+          Derivative Works a copy of this License; and\n",
      "+\n",
      "+      (b) You must cause any modified files to carry prominent notices\n",
      "+          stating that You changed the files; and\n",
      "+\n",
      "+      (c) You must retain, in the Source form of any Derivative Works\n",
      "+          that You distribute, all copyright, patent, trademark, and\n",
      "+          attribution notices from the Source form of the Work,\n",
      "+          excluding those notices that do not pertain to any part of\n",
      "+          the Derivative Works; and\n",
      "+\n",
      "+      (d) If the Work includes a \"NOTICE\" text file as part of its\n",
      "+          distribution, then any Derivative Works that You distribute must\n",
      "+          include a readable copy of the attribution notices contained\n",
      "+          within such NOTICE file, excluding those notices that do not\n",
      "+          pertain to any part of the Derivative Works, in at least one\n",
      "+          of the following places: within a NOTICE text file distributed\n",
      "+          as part of the Derivative Works; within the Source form or\n",
      "+          documentation, if provided along with the Derivative Works; or,\n",
      "+          within a display generated by the Derivative Works, if and\n",
      "+          wherever such third-party notices normally appear. The contents\n",
      "+          of the NOTICE file are for informational purposes only and\n",
      "+          do not modify the License. You may add Your own attribution\n",
      "+          notices within Derivative Works that You distribute, alongside\n",
      "+          or as an addendum to the NOTICE text from the Work, provided\n",
      "+          that such additional attribution notices cannot be construed\n",
      "+          as modifying the License.\n",
      "+\n",
      "+      You may add Your own copyright statement to Your modifications and\n",
      "+      may provide additional or different license terms and conditions\n",
      "+      for use, reproduction, or distribution of Your modifications, or\n",
      "+      for any such Derivative Works as a whole, provided Your use,\n",
      "+      reproduction, and distribution of the Work otherwise complies with\n",
      "+      the conditions stated in this License.\n",
      "+\n",
      "+   5. Submission of Contributions. Unless You explicitly state otherwise,\n",
      "+      any Contribution intentionally submitted for inclusion in the Work\n",
      "+      by You to the Licensor shall be under the terms and conditions of\n",
      "+      this License, without any additional terms or conditions.\n",
      "+      Notwithstanding the above, nothing herein shall supersede or modify\n",
      "+      the terms of any separate license agreement you may have executed\n",
      "+      with Licensor regarding such Contributions.\n",
      "+\n",
      "+   6. Trademarks. This License does not grant permission to use the trade\n",
      "+      names, trademarks, service marks, or product names of the Licensor,\n",
      "+      except as required for reasonable and customary use in describing the\n",
      "+      origin of the Work and reproducing the content of the NOTICE file.\n",
      "+\n",
      "+   7. Disclaimer of Warranty. Unless required by applicable law or\n",
      "+      agreed to in writing, Licensor provides the Work (and each\n",
      "+      Contributor provides its Contributions) on an \"AS IS\" BASIS,\n",
      "+      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n",
      "+      implied, including, without limitation, any warranties or conditions\n",
      "+      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\n",
      "+      PARTICULAR PURPOSE. You are solely responsible for determining the\n",
      "+      appropriateness of using or redistributing the Work and assume any\n",
      "+      risks associated with Your exercise of permissions under this License.\n",
      "+\n",
      "+   8. Limitation of Liability. In no event and under no legal theory,\n",
      "+      whether in tort (including negligence), contract, or otherwise,\n",
      "+      unless required by applicable law (such as deliberate and grossly\n",
      "+      negligent acts) or agreed to in writing, shall any Contributor be\n",
      "+      liable to You for damages, including any direct, indirect, special,\n",
      "+      incidental, or consequential damages of any character arising as a\n",
      "+      result of this License or out of the use or inability to use the\n",
      "+      Work (including but not limited to damages for loss of goodwill,\n",
      "+      work stoppage, computer failure or malfunction, or any and all\n",
      "+      other commercial damages or losses), even if such Contributor\n",
      "+      has been advised of the possibility of such damages.\n",
      "+\n",
      "+   9. Accepting Warranty or Additional Liability. While redistributing\n",
      "+      the Work or Derivative Works thereof, You may choose to offer,\n",
      "+      and charge a fee for, acceptance of support, warranty, indemnity,\n",
      "+      or other liability obligations and/or rights consistent with this\n",
      "+      License. However, in accepting such obligations, You may act only\n",
      "+      on Your own behalf and on Your sole responsibility, not on behalf\n",
      "+      of any other Contributor, and only if You agree to indemnify,\n",
      "+      defend, and hold each Contributor harmless for any liability\n",
      "+      incurred by, or claims asserted against, such Contributor by reason\n",
      "+      of your accepting any such warranty or additional liability.\n",
      "+\n",
      "+   END OF TERMS AND CONDITIONS\n",
      "+\n",
      "+   APPENDIX: How to apply the Apache License to your work.\n",
      "+\n",
      "+      To apply the Apache License to your work, attach the following\n",
      "+      boilerplate notice, with the fields enclosed by brackets \"[]\"\n",
      "+      replaced with your own identifying information. (Don't include\n",
      "+      the brackets!)  The text should be enclosed in the appropriate\n",
      "+      comment syntax for the file format. We also recommend that a\n",
      "+      file or class name and description of purpose be included on the\n",
      "+      same \"printed page\" as the copyright notice for easier\n",
      "+      identification within third-party archives.\n",
      "+\n",
      "+   Copyright 2015, The TensorFlow Authors.\n",
      "+\n",
      "+   Licensed under the Apache License, Version 2.0 (the \"License\");\n",
      "+   you may not use this file except in compliance with the License.\n",
      "+   You may obtain a copy of the License at\n",
      "+\n",
      "+       http://www.apache.org/licenses/LICENSE-2.0\n",
      "+\n",
      "+   Unless required by applicable law or agreed to in writing, software\n",
      "+   distributed under the License is distributed on an \"AS IS\" BASIS,\n",
      "+   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
      "+   See the License for the specific language governing permissions and\n",
      "+   limitations under the License.\n",
      "\n",
      "\n",
      "__Before__: \n",
      "# TensorFlow Models\n",
      "\n",
      "__After__: \n",
      "# TensorFlow Models\n",
      "\n",
      "This repository contains machine learning models implemented in\n",
      "[TensorFlow](https://tensorflow.org). The models are maintained by their\n",
      "respective authors.\n",
      "\n",
      "To propose a model for inclusion please submit a pull request.\n",
      "\n",
      "@@ -1 +1,7 @@\n",
      " # TensorFlow Models\n",
      "+\n",
      "+This repository contains machine learning models implemented in\n",
      "+[TensorFlow](https://tensorflow.org). The models are maintained by their\n",
      "+respective authors.\n",
      "+\n",
      "+To propose a model for inclusion please submit a pull request.\n",
      "\n",
      "\n",
      "__Before__: \n",
      "# NeuralGPU\n",
      "Code for the Neural GPU model as described\n",
      "in [[http://arxiv.org/abs/1511.08228]].\n",
      "\n",
      "\n",
      "__After__: \n",
      "# NeuralGPU\n",
      "Code for the Neural GPU model as described\n",
      "in [[http://arxiv.org/abs/1511.08228]].\n",
      "\n",
      "Maintained by Lukasz Kaiser (lukaszkaiser)\n",
      "\n",
      "@@ -2,3 +2,4 @@\n",
      " Code for the Neural GPU model as described\n",
      " in [[http://arxiv.org/abs/1511.08228]].\n",
      " \n",
      "+Maintained by Lukasz Kaiser (lukaszkaiser)\n",
      "\n",
      "\n",
      "__Before__: \n",
      "\"\"\"Convolutional Gated Recurrent Networks for Algorithm Learning.\"\"\"\n",
      "\n",
      "import math\n",
      "import random\n",
      "import sys\n",
      "import time\n",
      "\n",
      "import google3\n",
      "\n",
      "import numpy as np\n",
      "import tensorflow as tf\n",
      "\n",
      "from google3.third_party.tensorflow.python.platform import gfile\n",
      "\n",
      "FLAGS = tf.app.flags.FLAGS\n",
      "\n",
      "bins = [8, 16, 32, 64, 128]\n",
      "all_tasks = [\"sort\", \"id\", \"rev\", \"incr\", \"left\", \"right\", \"left-shift\", \"add\",\n",
      "             \"right-shift\", \"bmul\", \"dup\", \"badd\", \"qadd\"]\n",
      "forward_max = 128\n",
      "log_filename = \"\"\n",
      "\n",
      "\n",
      "def pad(l):\n",
      "  for b in bins:\n",
      "    if b >= l: return b\n",
      "  return forward_max\n",
      "\n",
      "\n",
      "train_set = {}\n",
      "test_set = {}\n",
      "for some_task in all_tasks:\n",
      "  train_set[some_task] = []\n",
      "  test_set[some_task] = []\n",
      "  for all_max_len in xrange(10000):\n",
      "    train_set[some_task].append([])\n",
      "    test_set[some_task].append([])\n",
      "\n",
      "\n",
      "def add(n1, n2, base=10):\n",
      "  \"\"\"Add two numbers represented as lower-endian digit lists.\"\"\"\n",
      "  k = max(len(n1), len(n2)) + 1\n",
      "  d1 = n1 + [0 for _ in xrange(k - len(n1))]\n",
      "  d2 = n2 + [0 for _ in xrange(k - len(n2))]\n",
      "  res = []\n",
      "  carry = 0\n",
      "  for i in xrange(k):\n",
      "    if d1[i] + d2[i] + carry < base:\n",
      "      res.append(d1[i] + d2[i] + carry)\n",
      "      carry = 0\n",
      "    else:\n",
      "      res.append(d1[i] + d2[i] + carry - base)\n",
      "      carry = 1\n",
      "  while res and res[-1] == 0:\n",
      "    res = res[:-1]\n",
      "  if res: return res\n",
      "  return [0]\n",
      "\n",
      "\n",
      "def init_data(task, length, nbr_cases, nclass):\n",
      "  \"\"\"Data initialization.\"\"\"\n",
      "  def rand_pair(l, task):\n",
      "    \"\"\"Random data pair for a task. Total length should be <= l.\"\"\"\n",
      "    k = (l-1)/2\n",
      "    base = 10\n",
      "    if task[0] == \"b\": base = 2\n",
      "    if task[0] == \"q\": base = 4\n",
      "    d1 = [np.random.randint(base) for _ in xrange(k)]\n",
      "    d2 = [np.random.randint(base) for _ in xrange(k)]\n",
      "    if task in [\"add\", \"badd\", \"qadd\"]:\n",
      "      res = add(d1, d2, base)\n",
      "    elif task in [\"bmul\"]:\n",
      "      d1n = sum([d * (base ** i) for i, d in enumerate(d1)])\n",
      "      d2n = sum([d * (base ** i) for i, d in enumerate(d2)])\n",
      "      res = [int(x) for x in list(reversed(str(bin(d1n * d2n))))[:-2]]\n",
      "    else:\n",
      "      sys.exit()\n",
      "    sep = [12]\n",
      "    if task in [\"add\", \"badd\", \"qadd\"]: sep = [11]\n",
      "    inp = [d + 1 for d in d1] + sep + [d + 1 for d in d2]\n",
      "    return inp, [r + 1 for r in res]\n",
      "\n",
      "  def rand_dup_pair(l):\n",
      "    \"\"\"Random data pair for duplication task. Total length should be <= l.\"\"\"\n",
      "    k = l/2\n",
      "    x = [np.random.randint(nclass - 1) + 1 for _ in xrange(k)]\n",
      "    inp = x + [0 for _ in xrange(l - k)]\n",
      "    res = x + x + [0 for _ in xrange(l - 2*k)]\n",
      "    return inp, res\n",
      "\n",
      "  def spec(inp):\n",
      "    \"\"\"Return the target given the input for some tasks.\"\"\"\n",
      "    if task == \"sort\":\n",
      "      return sorted(inp)\n",
      "    elif task == \"id\":\n",
      "      return inp\n",
      "    elif task == \"rev\":\n",
      "      return [i for i in reversed(inp)]\n",
      "    elif task == \"incr\":\n",
      "      carry = 1\n",
      "      res = []\n",
      "      for i in xrange(len(inp)):\n",
      "        if inp[i] + carry < nclass:\n",
      "          res.append(inp[i] + carry)\n",
      "          carry = 0\n",
      "        else:\n",
      "          res.append(1)\n",
      "          carry = 1\n",
      "      return res\n",
      "    elif task == \"left\":\n",
      "      return [inp[0]]\n",
      "    elif task == \"right\":\n",
      "      return [inp[-1]]\n",
      "    elif task == \"left-shift\":\n",
      "      return [inp[l-1] for l in xrange(len(inp))]\n",
      "    elif task == \"right-shift\":\n",
      "      return [inp[l+1] for l in xrange(len(inp))]\n",
      "    else:\n",
      "      print_out(\"Unknown spec for task \" + str(task))\n",
      "      sys.exit()\n",
      "\n",
      "  l = length\n",
      "  cur_time = time.time()\n",
      "  total_time = 0.0\n",
      "  for case in xrange(nbr_cases):\n",
      "    total_time += time.time() - cur_time\n",
      "    cur_time = time.time()\n",
      "    if l > 10000 and case % 100 == 1:\n",
      "      print_out(\"  avg gen time %.4f s\" % (total_time / float(case)))\n",
      "    if task in [\"add\", \"badd\", \"qadd\", \"bmul\"]:\n",
      "      i, t = rand_pair(l, task)\n",
      "      train_set[task][len(i)].append([i, t])\n",
      "      i, t = rand_pair(l, task)\n",
      "      test_set[task][len(i)].append([i, t])\n",
      "    elif task == \"dup\":\n",
      "      i, t = rand_dup_pair(l)\n",
      "      train_set[task][len(i)].append([i, t])\n",
      "      i, t = rand_dup_pair(l)\n",
      "      test_set[task][len(i)].append([i, t])\n",
      "    else:\n",
      "      inp = [np.random.randint(nclass - 1) + 1 for i in xrange(l)]\n",
      "      target = spec(inp)\n",
      "      train_set[task][l].append([inp, target])\n",
      "      inp = [np.random.randint(nclass - 1) + 1 for i in xrange(l)]\n",
      "      target = spec(inp)\n",
      "      test_set[task][l].append([inp, target])\n",
      "\n",
      "\n",
      "def get_batch(max_length, batch_size, do_train, task, offset=None, preset=None):\n",
      "  \"\"\"Get a batch of data, training or testing.\"\"\"\n",
      "  inputs = []\n",
      "  targets = []\n",
      "  length = max_length\n",
      "  if preset is None:\n",
      "    cur_set = test_set[task]\n",
      "    if do_train: cur_set = train_set[task]\n",
      "    while not cur_set[length]:\n",
      "      length -= 1\n",
      "  pad_length = pad(length)\n",
      "  for b in xrange(batch_size):\n",
      "    if preset is None:\n",
      "      elem = random.choice(cur_set[length])\n",
      "      if offset is not None and offset + b < len(cur_set[length]):\n",
      "        elem = cur_set[length][offset + b]\n",
      "    else:\n",
      "      elem = preset\n",
      "    inp, target = elem[0], elem[1]\n",
      "    assert len(inp) == length\n",
      "    inputs.append(inp + [0 for l in xrange(pad_length - len(inp))])\n",
      "    targets.append(target + [0 for l in xrange(pad_length - len(target))])\n",
      "  res_input = []\n",
      "  res_target = []\n",
      "  for l in xrange(pad_length):\n",
      "    new_input = np.array([inputs[b][l] for b in xrange(batch_size)],\n",
      "                         dtype=np.int32)\n",
      "    new_target = np.array([targets[b][l] for b in xrange(batch_size)],\n",
      "                          dtype=np.int32)\n",
      "    res_input.append(new_input)\n",
      "    res_target.append(new_target)\n",
      "  return res_input, res_target\n",
      "\n",
      "\n",
      "def print_out(s, newline=True):\n",
      "  \"\"\"Print a message out and log it to file.\"\"\"\n",
      "  if log_filename:\n",
      "    try:\n",
      "      with gfile.GFile(log_filename, mode=\"a\") as f:\n",
      "        f.write(s + (\"\\n\" if newline else \"\"))\n",
      "    # pylint: disable=bare-except\n",
      "    except:\n",
      "      sys.stdout.write(\"Error appending to %s\\n\" % log_filename)\n",
      "  sys.stdout.write(s + (\"\\n\" if newline else \"\"))\n",
      "  sys.stdout.flush()\n",
      "\n",
      "\n",
      "def decode(output):\n",
      "  return [np.argmax(o, axis=1) for o in output]\n",
      "\n",
      "\n",
      "def accuracy(inpt, output, target, batch_size, nprint):\n",
      "  \"\"\"Calculate output accuracy given target.\"\"\"\n",
      "  assert nprint < batch_size + 1\n",
      "  def task_print(inp, output, target):\n",
      "    stop_bound = 0\n",
      "    print_len = 0\n",
      "    while print_len < len(target) and target[print_len] > stop_bound:\n",
      "      print_len += 1\n",
      "    print_out(\"    i: \" + \" \".join([str(i - 1) for i in inp if i > 0]))\n",
      "    print_out(\"    o: \" +\n",
      "              \" \".join([str(output[l] - 1) for l in xrange(print_len)]))\n",
      "    print_out(\"    t: \" +\n",
      "              \" \".join([str(target[l] - 1) for l in xrange(print_len)]))\n",
      "  decoded_target = target\n",
      "  decoded_output = decode(output)\n",
      "  total = 0\n",
      "  errors = 0\n",
      "  seq = [0 for b in xrange(batch_size)]\n",
      "  for l in xrange(len(decoded_output)):\n",
      "    for b in xrange(batch_size):\n",
      "      if decoded_target[l][b] > 0:\n",
      "        total += 1\n",
      "        if decoded_output[l][b] != decoded_target[l][b]:\n",
      "          seq[b] = 1\n",
      "          errors += 1\n",
      "  e = 0  # Previous error index\n",
      "  for _ in xrange(min(nprint, sum(seq))):\n",
      "    while seq[e] == 0:\n",
      "      e += 1\n",
      "    task_print([inpt[l][e] for l in xrange(len(inpt))],\n",
      "               [decoded_output[l][e] for l in xrange(len(decoded_target))],\n",
      "               [decoded_target[l][e] for l in xrange(len(decoded_target))])\n",
      "    e += 1\n",
      "  for b in xrange(nprint - errors):\n",
      "    task_print([inpt[l][b] for l in xrange(len(inpt))],\n",
      "               [decoded_output[l][b] for l in xrange(len(decoded_target))],\n",
      "               [decoded_target[l][b] for l in xrange(len(decoded_target))])\n",
      "  return errors, total, sum(seq)\n",
      "\n",
      "\n",
      "def safe_exp(x):\n",
      "  perp = 10000\n",
      "  if x < 100: perp = math.exp(x)\n",
      "  if perp > 10000: return 10000\n",
      "  return perp\n",
      "\n",
      "__After__: \n",
      "# Copyright 2015 Google Inc. All Rights Reserved.\n",
      "#\n",
      "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
      "# you may not use this file except in compliance with the License.\n",
      "# You may obtain a copy of the License at\n",
      "#\n",
      "#     http://www.apache.org/licenses/LICENSE-2.0\n",
      "#\n",
      "# Unless required by applicable law or agreed to in writing, software\n",
      "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
      "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
      "# See the License for the specific language governing permissions and\n",
      "# limitations under the License.\n",
      "#\n",
      "#==============================================================================\n",
      "\n",
      "\"\"\"Convolutional Gated Recurrent Networks for Algorithm Learning.\"\"\"\n",
      "\n",
      "import math\n",
      "import random\n",
      "import sys\n",
      "import time\n",
      "\n",
      "import google3\n",
      "\n",
      "import numpy as np\n",
      "import tensorflow as tf\n",
      "\n",
      "from google3.third_party.tensorflow.python.platform import gfile\n",
      "\n",
      "FLAGS = tf.app.flags.FLAGS\n",
      "\n",
      "bins = [8, 16, 32, 64, 128]\n",
      "all_tasks = [\"sort\", \"id\", \"rev\", \"incr\", \"left\", \"right\", \"left-shift\", \"add\",\n",
      "             \"right-shift\", \"bmul\", \"dup\", \"badd\", \"qadd\"]\n",
      "forward_max = 128\n",
      "log_filename = \"\"\n",
      "\n",
      "\n",
      "def pad(l):\n",
      "  for b in bins:\n",
      "    if b >= l: return b\n",
      "  return forward_max\n",
      "\n",
      "\n",
      "train_set = {}\n",
      "test_set = {}\n",
      "for some_task in all_tasks:\n",
      "  train_set[some_task] = []\n",
      "  test_set[some_task] = []\n",
      "  for all_max_len in xrange(10000):\n",
      "    train_set[some_task].append([])\n",
      "    test_set[some_task].append([])\n",
      "\n",
      "\n",
      "def add(n1, n2, base=10):\n",
      "  \"\"\"Add two numbers represented as lower-endian digit lists.\"\"\"\n",
      "  k = max(len(n1), len(n2)) + 1\n",
      "  d1 = n1 + [0 for _ in xrange(k - len(n1))]\n",
      "  d2 = n2 + [0 for _ in xrange(k - len(n2))]\n",
      "  res = []\n",
      "  carry = 0\n",
      "  for i in xrange(k):\n",
      "    if d1[i] + d2[i] + carry < base:\n",
      "      res.append(d1[i] + d2[i] + carry)\n",
      "      carry = 0\n",
      "    else:\n",
      "      res.append(d1[i] + d2[i] + carry - base)\n",
      "      carry = 1\n",
      "  while res and res[-1] == 0:\n",
      "    res = res[:-1]\n",
      "  if res: return res\n",
      "  return [0]\n",
      "\n",
      "\n",
      "def init_data(task, length, nbr_cases, nclass):\n",
      "  \"\"\"Data initialization.\"\"\"\n",
      "  def rand_pair(l, task):\n",
      "    \"\"\"Random data pair for a task. Total length should be <= l.\"\"\"\n",
      "    k = (l-1)/2\n",
      "    base = 10\n",
      "    if task[0] == \"b\": base = 2\n",
      "    if task[0] == \"q\": base = 4\n",
      "    d1 = [np.random.randint(base) for _ in xrange(k)]\n",
      "    d2 = [np.random.randint(base) for _ in xrange(k)]\n",
      "    if task in [\"add\", \"badd\", \"qadd\"]:\n",
      "      res = add(d1, d2, base)\n",
      "    elif task in [\"bmul\"]:\n",
      "      d1n = sum([d * (base ** i) for i, d in enumerate(d1)])\n",
      "      d2n = sum([d * (base ** i) for i, d in enumerate(d2)])\n",
      "      res = [int(x) for x in list(reversed(str(bin(d1n * d2n))))[:-2]]\n",
      "    else:\n",
      "      sys.exit()\n",
      "    sep = [12]\n",
      "    if task in [\"add\", \"badd\", \"qadd\"]: sep = [11]\n",
      "    inp = [d + 1 for d in d1] + sep + [d + 1 for d in d2]\n",
      "    return inp, [r + 1 for r in res]\n",
      "\n",
      "  def rand_dup_pair(l):\n",
      "    \"\"\"Random data pair for duplication task. Total length should be <= l.\"\"\"\n",
      "    k = l/2\n",
      "    x = [np.random.randint(nclass - 1) + 1 for _ in xrange(k)]\n",
      "    inp = x + [0 for _ in xrange(l - k)]\n",
      "    res = x + x + [0 for _ in xrange(l - 2*k)]\n",
      "    return inp, res\n",
      "\n",
      "  def spec(inp):\n",
      "    \"\"\"Return the target given the input for some tasks.\"\"\"\n",
      "    if task == \"sort\":\n",
      "      return sorted(inp)\n",
      "    elif task == \"id\":\n",
      "      return inp\n",
      "    elif task == \"rev\":\n",
      "      return [i for i in reversed(inp)]\n",
      "    elif task == \"incr\":\n",
      "      carry = 1\n",
      "      res = []\n",
      "      for i in xrange(len(inp)):\n",
      "        if inp[i] + carry < nclass:\n",
      "          res.append(inp[i] + carry)\n",
      "          carry = 0\n",
      "        else:\n",
      "          res.append(1)\n",
      "          carry = 1\n",
      "      return res\n",
      "    elif task == \"left\":\n",
      "      return [inp[0]]\n",
      "    elif task == \"right\":\n",
      "      return [inp[-1]]\n",
      "    elif task == \"left-shift\":\n",
      "      return [inp[l-1] for l in xrange(len(inp))]\n",
      "    elif task == \"right-shift\":\n",
      "      return [inp[l+1] for l in xrange(len(inp))]\n",
      "    else:\n",
      "      print_out(\"Unknown spec for task \" + str(task))\n",
      "      sys.exit()\n",
      "\n",
      "  l = length\n",
      "  cur_time = time.time()\n",
      "  total_time = 0.0\n",
      "  for case in xrange(nbr_cases):\n",
      "    total_time += time.time() - cur_time\n",
      "    cur_time = time.time()\n",
      "    if l > 10000 and case % 100 == 1:\n",
      "      print_out(\"  avg gen time %.4f s\" % (total_time / float(case)))\n",
      "    if task in [\"add\", \"badd\", \"qadd\", \"bmul\"]:\n",
      "      i, t = rand_pair(l, task)\n",
      "      train_set[task][len(i)].append([i, t])\n",
      "      i, t = rand_pair(l, task)\n",
      "      test_set[task][len(i)].append([i, t])\n",
      "    elif task == \"dup\":\n",
      "      i, t = rand_dup_pair(l)\n",
      "      train_set[task][len(i)].append([i, t])\n",
      "      i, t = rand_dup_pair(l)\n",
      "      test_set[task][len(i)].append([i, t])\n",
      "    else:\n",
      "      inp = [np.random.randint(nclass - 1) + 1 for i in xrange(l)]\n",
      "      target = spec(inp)\n",
      "      train_set[task][l].append([inp, target])\n",
      "      inp = [np.random.randint(nclass - 1) + 1 for i in xrange(l)]\n",
      "      target = spec(inp)\n",
      "      test_set[task][l].append([inp, target])\n",
      "\n",
      "\n",
      "def get_batch(max_length, batch_size, do_train, task, offset=None, preset=None):\n",
      "  \"\"\"Get a batch of data, training or testing.\"\"\"\n",
      "  inputs = []\n",
      "  targets = []\n",
      "  length = max_length\n",
      "  if preset is None:\n",
      "    cur_set = test_set[task]\n",
      "    if do_train: cur_set = train_set[task]\n",
      "    while not cur_set[length]:\n",
      "      length -= 1\n",
      "  pad_length = pad(length)\n",
      "  for b in xrange(batch_size):\n",
      "    if preset is None:\n",
      "      elem = random.choice(cur_set[length])\n",
      "      if offset is not None and offset + b < len(cur_set[length]):\n",
      "        elem = cur_set[length][offset + b]\n",
      "    else:\n",
      "      elem = preset\n",
      "    inp, target = elem[0], elem[1]\n",
      "    assert len(inp) == length\n",
      "    inputs.append(inp + [0 for l in xrange(pad_length - len(inp))])\n",
      "    targets.append(target + [0 for l in xrange(pad_length - len(target))])\n",
      "  res_input = []\n",
      "  res_target = []\n",
      "  for l in xrange(pad_length):\n",
      "    new_input = np.array([inputs[b][l] for b in xrange(batch_size)],\n",
      "                         dtype=np.int32)\n",
      "    new_target = np.array([targets[b][l] for b in xrange(batch_size)],\n",
      "                          dtype=np.int32)\n",
      "    res_input.append(new_input)\n",
      "    res_target.append(new_target)\n",
      "  return res_input, res_target\n",
      "\n",
      "\n",
      "def print_out(s, newline=True):\n",
      "  \"\"\"Print a message out and log it to file.\"\"\"\n",
      "  if log_filename:\n",
      "    try:\n",
      "      with gfile.GFile(log_filename, mode=\"a\") as f:\n",
      "        f.write(s + (\"\\n\" if newline else \"\"))\n",
      "    # pylint: disable=bare-except\n",
      "    except:\n",
      "      sys.stdout.write(\"Error appending to %s\\n\" % log_filename)\n",
      "  sys.stdout.write(s + (\"\\n\" if newline else \"\"))\n",
      "  sys.stdout.flush()\n",
      "\n",
      "\n",
      "def decode(output):\n",
      "  return [np.argmax(o, axis=1) for o in output]\n",
      "\n",
      "\n",
      "def accuracy(inpt, output, target, batch_size, nprint):\n",
      "  \"\"\"Calculate output accuracy given target.\"\"\"\n",
      "  assert nprint < batch_size + 1\n",
      "  def task_print(inp, output, target):\n",
      "    stop_bound = 0\n",
      "    print_len = 0\n",
      "    while print_len < len(target) and target[print_len] > stop_bound:\n",
      "      print_len += 1\n",
      "    print_out(\"    i: \" + \" \".join([str(i - 1) for i in inp if i > 0]))\n",
      "    print_out(\"    o: \" +\n",
      "              \" \".join([str(output[l] - 1) for l in xrange(print_len)]))\n",
      "    print_out(\"    t: \" +\n",
      "              \" \".join([str(target[l] - 1) for l in xrange(print_len)]))\n",
      "  decoded_target = target\n",
      "  decoded_output = decode(output)\n",
      "  total = 0\n",
      "  errors = 0\n",
      "  seq = [0 for b in xrange(batch_size)]\n",
      "  for l in xrange(len(decoded_output)):\n",
      "    for b in xrange(batch_size):\n",
      "      if decoded_target[l][b] > 0:\n",
      "        total += 1\n",
      "        if decoded_output[l][b] != decoded_target[l][b]:\n",
      "          seq[b] = 1\n",
      "          errors += 1\n",
      "  e = 0  # Previous error index\n",
      "  for _ in xrange(min(nprint, sum(seq))):\n",
      "    while seq[e] == 0:\n",
      "      e += 1\n",
      "    task_print([inpt[l][e] for l in xrange(len(inpt))],\n",
      "               [decoded_output[l][e] for l in xrange(len(decoded_target))],\n",
      "               [decoded_target[l][e] for l in xrange(len(decoded_target))])\n",
      "    e += 1\n",
      "  for b in xrange(nprint - errors):\n",
      "    task_print([inpt[l][b] for l in xrange(len(inpt))],\n",
      "               [decoded_output[l][b] for l in xrange(len(decoded_target))],\n",
      "               [decoded_target[l][b] for l in xrange(len(decoded_target))])\n",
      "  return errors, total, sum(seq)\n",
      "\n",
      "\n",
      "def safe_exp(x):\n",
      "  perp = 10000\n",
      "  if x < 100: perp = math.exp(x)\n",
      "  if perp > 10000: return 10000\n",
      "  return perp\n",
      "\n",
      "@@ -1,3 +1,19 @@\n",
      "+# Copyright 2015 Google Inc. All Rights Reserved.\n",
      "+#\n",
      "+# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
      "+# you may not use this file except in compliance with the License.\n",
      "+# You may obtain a copy of the License at\n",
      "+#\n",
      "+#     http://www.apache.org/licenses/LICENSE-2.0\n",
      "+#\n",
      "+# Unless required by applicable law or agreed to in writing, software\n",
      "+# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
      "+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
      "+# See the License for the specific language governing permissions and\n",
      "+# limitations under the License.\n",
      "+#\n",
      "+#==============================================================================\n",
      "+\n",
      " \"\"\"Convolutional Gated Recurrent Networks for Algorithm Learning.\"\"\"\n",
      " \n",
      " import math\n",
      "\n",
      "\n",
      "__Before__: \n",
      "\"\"\"The Neural GPU Model.\"\"\"\n",
      "\n",
      "import time\n",
      "\n",
      "import google3\n",
      "\n",
      "import tensorflow as tf\n",
      "\n",
      "from google3.experimental.users.lukaszkaiser.neural_gpu import data_utils\n",
      "\n",
      "\n",
      "def conv_linear(args, kw, kh, nin, nout, do_bias, bias_start, prefix):\n",
      "  \"\"\"Convolutional linear map.\"\"\"\n",
      "  assert args\n",
      "  if not isinstance(args, (list, tuple)):\n",
      "    args = [args]\n",
      "  with tf.variable_scope(prefix):\n",
      "    k = tf.get_variable(\"CvK\", [kw, kh, nin, nout])\n",
      "    if len(args) == 1:\n",
      "      res = tf.nn.conv2d(args[0], k, [1, 1, 1, 1], \"SAME\")\n",
      "    else:\n",
      "      res = tf.nn.conv2d(tf.concat(3, args), k, [1, 1, 1, 1], \"SAME\")\n",
      "    if not do_bias: return res\n",
      "    bias_term = tf.get_variable(\"CvB\", [nout],\n",
      "                                initializer=tf.constant_initializer(0.0))\n",
      "    return res + bias_term + bias_start\n",
      "\n",
      "\n",
      "def sigmoid_cutoff(x, cutoff):\n",
      "  \"\"\"Sigmoid with cutoff, e.g., 1.2sigmoid(x) - 0.1.\"\"\"\n",
      "  y = tf.sigmoid(x)\n",
      "  if cutoff < 1.01: return y\n",
      "  d = (cutoff - 1.0) / 2.0\n",
      "  return tf.minimum(1.0, tf.maximum(0.0, cutoff * y - d))\n",
      "\n",
      "\n",
      "def conv_gru(inpts, mem, kw, kh, nmaps, cutoff, prefix):\n",
      "  \"\"\"Convolutional GRU.\"\"\"\n",
      "  def conv_lin(args, suffix, bias_start):\n",
      "    return conv_linear(args, kw, kh, len(args) * nmaps, nmaps, True, bias_start,\n",
      "                       prefix + \"/\" + suffix)\n",
      "  reset = sigmoid_cutoff(conv_lin(inpts + [mem], \"r\", 1.0), cutoff)\n",
      "  candidate = tf.tanh(conv_lin(inpts + [reset * mem], \"c\", 0.0))\n",
      "  gate = sigmoid_cutoff(conv_lin(inpts + [mem], \"g\", 1.0), cutoff)\n",
      "  return gate * mem + (1 - gate) * candidate\n",
      "\n",
      "\n",
      "def relaxed_average(var_name_suffix, rx_step):\n",
      "  \"\"\"Calculate the average of relaxed variables having var_name_suffix.\"\"\"\n",
      "  relaxed_vars = []\n",
      "  for l in xrange(rx_step):\n",
      "    with tf.variable_scope(\"RX%d\" % l, reuse=True):\n",
      "      try:\n",
      "        relaxed_vars.append(tf.get_variable(var_name_suffix))\n",
      "      except ValueError:\n",
      "        pass\n",
      "  dsum = tf.add_n(relaxed_vars)\n",
      "  avg = dsum / len(relaxed_vars)\n",
      "  diff = [v - avg for v in relaxed_vars]\n",
      "  davg = tf.add_n([d*d for d in diff])\n",
      "  return avg, tf.reduce_sum(davg)\n",
      "\n",
      "\n",
      "def relaxed_distance(rx_step):\n",
      "  \"\"\"Distance between relaxed variables and their average.\"\"\"\n",
      "  res, ops, rx_done = [], [], {}\n",
      "  for v in tf.trainable_variables():\n",
      "    if v.name[0:2] == \"RX\":\n",
      "      rx_name = v.op.name[v.name.find(\"/\") + 1:]\n",
      "      if rx_name not in rx_done:\n",
      "        avg, dist_loss = relaxed_average(rx_name, rx_step)\n",
      "        res.append(dist_loss)\n",
      "        rx_done[rx_name] = avg\n",
      "      ops.append(v.assign(rx_done[rx_name]))\n",
      "  return tf.add_n(res), tf.group(*ops)\n",
      "\n",
      "\n",
      "def make_dense(targets, noclass):\n",
      "  \"\"\"Move a batch of targets to a dense 1-hot representation.\"\"\"\n",
      "  with tf.device(\"/cpu:0\"):\n",
      "    shape = tf.shape(targets)\n",
      "    batch_size = shape[0]\n",
      "    indices = targets + noclass * tf.range(0, batch_size)\n",
      "    length = batch_size * noclass\n",
      "    dense = tf.sparse_to_dense(indices, length, 1.0, 0.0)\n",
      "  return tf.reshape(dense, [-1, noclass])\n",
      "\n",
      "\n",
      "def check_for_zero(sparse):\n",
      "  \"\"\"In a sparse batch of ints, make 1.0 if it's 0 and 0.0 else.\"\"\"\n",
      "  with tf.device(\"/cpu:0\"):\n",
      "    shape = tf.shape(sparse)\n",
      "    batch_size = shape[0]\n",
      "    sparse = tf.minimum(sparse, 1)\n",
      "    indices = sparse + 2 * tf.range(0, batch_size)\n",
      "    dense = tf.sparse_to_dense(indices, 2 * batch_size, 1.0, 0.0)\n",
      "    reshaped = tf.reshape(dense, [-1, 2])\n",
      "  return tf.reshape(tf.slice(reshaped, [0, 0], [-1, 1]), [-1])\n",
      "\n",
      "\n",
      "class NeuralGPU(object):\n",
      "  \"\"\"Neural GPU Model.\"\"\"\n",
      "\n",
      "  def __init__(self, nmaps, vec_size, niclass, noclass, dropout, rx_step,\n",
      "               max_grad_norm, cutoff, nconvs, kw, kh, height, mode,\n",
      "               learning_rate, pull, pull_incr, min_length):\n",
      "    # Feeds for parameters and ops to update them.\n",
      "    self.global_step = tf.Variable(0, trainable=False)\n",
      "    self.cur_length = tf.Variable(min_length, trainable=False)\n",
      "    self.cur_length_incr_op = self.cur_length.assign_add(1)\n",
      "    self.lr = tf.Variable(float(learning_rate), trainable=False)\n",
      "    self.lr_decay_op = self.lr.assign(self.lr * 0.98)\n",
      "    self.pull = tf.Variable(float(pull), trainable=False)\n",
      "    self.pull_incr_op = self.pull.assign(self.pull * pull_incr)\n",
      "    self.do_training = tf.placeholder(tf.float32, name=\"do_training\")\n",
      "    self.noise_param = tf.placeholder(tf.float32, name=\"noise_param\")\n",
      "\n",
      "    # Feeds for inputs, targets, outputs, losses, etc.\n",
      "    self.input = []\n",
      "    self.target = []\n",
      "    for l in xrange(data_utils.forward_max + 1):\n",
      "      self.input.append(tf.placeholder(tf.int32, name=\"inp{0}\".format(l)))\n",
      "      self.target.append(tf.placeholder(tf.int32, name=\"tgt{0}\".format(l)))\n",
      "    self.outputs = []\n",
      "    self.losses = []\n",
      "    self.grad_norms = []\n",
      "    self.updates = []\n",
      "\n",
      "    # Computation.\n",
      "    inp0_shape = tf.shape(self.input[0])\n",
      "    batch_size = inp0_shape[0]\n",
      "    with tf.device(\"/cpu:0\"):\n",
      "      emb_weights = tf.get_variable(\n",
      "          \"embedding\", [niclass, vec_size],\n",
      "          initializer=tf.random_uniform_initializer(-1.7, 1.7))\n",
      "      e0 = tf.scatter_update(emb_weights,\n",
      "                             tf.constant(0, dtype=tf.int32, shape=[1]),\n",
      "                             tf.zeros([1, vec_size]))\n",
      "\n",
      "    adam = tf.train.AdamOptimizer(0.01*self.lr, epsilon=1e-5)\n",
      "\n",
      "    # Main graph creation loop, for every bin in data_utils.\n",
      "    self.steps = []\n",
      "    for length in sorted(list(set(data_utils.bins + [data_utils.forward_max]))):\n",
      "      data_utils.print_out(\"Creating model for bin of length %d.\" % length)\n",
      "      start_time = time.time()\n",
      "      if length > data_utils.bins[0]:\n",
      "        tf.get_variable_scope().reuse_variables()\n",
      "\n",
      "      # Embed inputs and calculate mask.\n",
      "      with tf.device(\"/cpu:0\"):\n",
      "        with tf.control_dependencies([e0]):\n",
      "          embedded = [tf.nn.embedding_lookup(emb_weights, self.input[l])\n",
      "                      for l in xrange(length)]\n",
      "        # Mask to 0-out padding space in each step.\n",
      "        imask = [check_for_zero(self.input[l]) for l in xrange(length)]\n",
      "        omask = [check_for_zero(self.target[l]) for l in xrange(length)]\n",
      "        mask = [1.0 - (imask[i] * omask[i]) for i in xrange(length)]\n",
      "        mask = [tf.reshape(m, [-1, 1]) for m in mask]\n",
      "        # Use a shifted mask for step scaling and concatenated for weights.\n",
      "        shifted_mask = mask + [tf.zeros_like(mask[0])]\n",
      "        scales = [shifted_mask[i] * (1.0 - shifted_mask[i+1])\n",
      "                  for i in xrange(length)]\n",
      "        scales = [tf.reshape(s, [-1, 1, 1, 1]) for s in scales]\n",
      "        mask = tf.concat(1, mask[0:length])  # batch x length\n",
      "        weights = mask\n",
      "        # Add a height dimension to mask to use later for masking.\n",
      "        mask = tf.reshape(mask, [-1, length, 1, 1])\n",
      "        mask = tf.concat(2, [mask for _ in xrange(height)]) + tf.zeros(\n",
      "            tf.pack([batch_size, length, height, nmaps]), dtype=tf.float32)\n",
      "\n",
      "      # Start is a length-list of batch-by-nmaps tensors, reshape and concat.\n",
      "      start = [tf.tanh(embedded[l]) for l in xrange(length)]\n",
      "      start = [tf.reshape(start[l], [-1, 1, nmaps]) for l in xrange(length)]\n",
      "      start = tf.reshape(tf.concat(1, start), [-1, length, 1, nmaps])\n",
      "\n",
      "      # First image comes from start by applying one convolution and adding 0s.\n",
      "      first = conv_linear(start, 1, 1, vec_size, nmaps, True, 0.0, \"input\")\n",
      "      first = [first] + [tf.zeros(tf.pack([batch_size, length, 1, nmaps]),\n",
      "                                  dtype=tf.float32) for _ in xrange(height - 1)]\n",
      "      first = tf.concat(2, first)\n",
      "\n",
      "      # Computation steps.\n",
      "      step = [tf.nn.dropout(first, 1.0 - self.do_training * dropout) * mask]\n",
      "      outputs = []\n",
      "      for it in xrange(length):\n",
      "        with tf.variable_scope(\"RX%d\" % (it % rx_step)) as vs:\n",
      "          if it >= rx_step:\n",
      "            vs.reuse_variables()\n",
      "          cur = step[it]\n",
      "          # Do nconvs-many CGRU steps.\n",
      "          for layer in xrange(nconvs):\n",
      "            cur = conv_gru([], cur, kw, kh, nmaps, cutoff, \"cgru_%d\" % layer)\n",
      "          cur = tf.nn.dropout(cur, 1.0 - self.do_training * dropout)\n",
      "          step.append(cur * mask)\n",
      "          outputs.append(tf.slice(step[-1], [0, 0, 0, 0], [-1, -1, 1, -1]))\n",
      "\n",
      "      self.steps.append([tf.reshape(s, [-1, length, height * nmaps])\n",
      "                         for s in step])\n",
      "      # Output is the n-th step output; n = current length, as in scales.\n",
      "      output = tf.add_n([outputs[i] * scales[i] for i in xrange(length)])\n",
      "      # Final convolution to get logits, list outputs.\n",
      "      output = conv_linear(output, 1, 1, nmaps, noclass, True, 0.0, \"output\")\n",
      "      output = tf.reshape(output, [-1, length, noclass])\n",
      "      self.outputs.append([tf.reshape(o, [-1, noclass])\n",
      "                           for o in list(tf.split(1, length, output))])\n",
      "\n",
      "      # Calculate cross-entropy loss and normalize it.\n",
      "      targets = tf.concat(1, [make_dense(self.target[l], noclass)\n",
      "                              for l in xrange(length)])\n",
      "      targets = tf.reshape(targets, [-1, noclass])\n",
      "      xent = tf.reshape(tf.nn.softmax_cross_entropy_with_logits(\n",
      "          tf.reshape(output, [-1, noclass]), targets), [-1, length])\n",
      "      perp_loss = tf.reduce_sum(xent * weights)\n",
      "      perp_loss /= tf.cast(batch_size, dtype=tf.float32)\n",
      "      perp_loss /= length\n",
      "\n",
      "      # Final loss: cross-entropy + shared parameter relaxation part.\n",
      "      relax_dist, self.avg_op = relaxed_distance(rx_step)\n",
      "      total_loss = perp_loss + relax_dist * self.pull\n",
      "      self.losses.append(perp_loss)\n",
      "\n",
      "      # Gradients and Adam update operation.\n",
      "      if length == data_utils.bins[0] or (mode == 0 and\n",
      "                                          length < data_utils.bins[-1] + 1):\n",
      "        data_utils.print_out(\"Creating backward for bin of length %d.\" % length)\n",
      "        params = tf.trainable_variables()\n",
      "        grads = tf.gradients(total_loss, params)\n",
      "        grads, norm = tf.clip_by_global_norm(grads, max_grad_norm)\n",
      "        self.grad_norms.append(norm)\n",
      "        for grad in grads:\n",
      "          if isinstance(grad, tf.Tensor):\n",
      "            grad += tf.truncated_normal(tf.shape(grad)) * self.noise_param\n",
      "        update = adam.apply_gradients(zip(grads, params),\n",
      "                                      global_step=self.global_step)\n",
      "        self.updates.append(update)\n",
      "      data_utils.print_out(\"Created model for bin of length %d in\"\n",
      "                           \" %.2f s.\" % (length, time.time() - start_time))\n",
      "    self.saver = tf.train.Saver(tf.all_variables())\n",
      "\n",
      "  def step(self, sess, inp, target, do_backward, noise_param=None):\n",
      "    \"\"\"Run a step of the network.\"\"\"\n",
      "    assert len(inp) == len(target)\n",
      "    length = len(target)\n",
      "    feed_in = {}\n",
      "    feed_in[self.noise_param.name] = noise_param if noise_param else 0.0\n",
      "    feed_in[self.do_training.name] = 1.0 if do_backward else 0.0\n",
      "    feed_out = []\n",
      "    index = len(data_utils.bins)\n",
      "    if length < data_utils.bins[-1] + 1:\n",
      "      index = data_utils.bins.index(length)\n",
      "    if do_backward:\n",
      "      feed_out.append(self.updates[index])\n",
      "      feed_out.append(self.grad_norms[index])\n",
      "    feed_out.append(self.losses[index])\n",
      "    for l in xrange(length):\n",
      "      feed_in[self.input[l].name] = inp[l]\n",
      "    for l in xrange(length):\n",
      "      feed_in[self.target[l].name] = target[l]\n",
      "      feed_out.append(self.outputs[index][l])\n",
      "    for l in xrange(length+1):\n",
      "      feed_out.append(self.steps[index][l])\n",
      "    res = sess.run(feed_out, feed_in)\n",
      "    offset = 0\n",
      "    norm = None\n",
      "    if do_backward:\n",
      "      offset = 2\n",
      "      norm = res[1]\n",
      "    outputs = res[offset + 1:offset + 1 + length]\n",
      "    steps = res[offset + 1 + length:]\n",
      "    return res[offset], outputs, norm, steps\n",
      "\n",
      "__After__: \n",
      "# Copyright 2015 Google Inc. All Rights Reserved.\n",
      "#\n",
      "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
      "# you may not use this file except in compliance with the License.\n",
      "# You may obtain a copy of the License at\n",
      "#\n",
      "#     http://www.apache.org/licenses/LICENSE-2.0\n",
      "#\n",
      "# Unless required by applicable law or agreed to in writing, software\n",
      "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
      "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
      "# See the License for the specific language governing permissions and\n",
      "# limitations under the License.\n",
      "#\n",
      "#==============================================================================\n",
      "\n",
      "\"\"\"The Neural GPU Model.\"\"\"\n",
      "\n",
      "import time\n",
      "\n",
      "import google3\n",
      "\n",
      "import tensorflow as tf\n",
      "\n",
      "from google3.experimental.users.lukaszkaiser.neural_gpu import data_utils\n",
      "\n",
      "\n",
      "def conv_linear(args, kw, kh, nin, nout, do_bias, bias_start, prefix):\n",
      "  \"\"\"Convolutional linear map.\"\"\"\n",
      "  assert args\n",
      "  if not isinstance(args, (list, tuple)):\n",
      "    args = [args]\n",
      "  with tf.variable_scope(prefix):\n",
      "    k = tf.get_variable(\"CvK\", [kw, kh, nin, nout])\n",
      "    if len(args) == 1:\n",
      "      res = tf.nn.conv2d(args[0], k, [1, 1, 1, 1], \"SAME\")\n",
      "    else:\n",
      "      res = tf.nn.conv2d(tf.concat(3, args), k, [1, 1, 1, 1], \"SAME\")\n",
      "    if not do_bias: return res\n",
      "    bias_term = tf.get_variable(\"CvB\", [nout],\n",
      "                                initializer=tf.constant_initializer(0.0))\n",
      "    return res + bias_term + bias_start\n",
      "\n",
      "\n",
      "def sigmoid_cutoff(x, cutoff):\n",
      "  \"\"\"Sigmoid with cutoff, e.g., 1.2sigmoid(x) - 0.1.\"\"\"\n",
      "  y = tf.sigmoid(x)\n",
      "  if cutoff < 1.01: return y\n",
      "  d = (cutoff - 1.0) / 2.0\n",
      "  return tf.minimum(1.0, tf.maximum(0.0, cutoff * y - d))\n",
      "\n",
      "\n",
      "def conv_gru(inpts, mem, kw, kh, nmaps, cutoff, prefix):\n",
      "  \"\"\"Convolutional GRU.\"\"\"\n",
      "  def conv_lin(args, suffix, bias_start):\n",
      "    return conv_linear(args, kw, kh, len(args) * nmaps, nmaps, True, bias_start,\n",
      "                       prefix + \"/\" + suffix)\n",
      "  reset = sigmoid_cutoff(conv_lin(inpts + [mem], \"r\", 1.0), cutoff)\n",
      "  candidate = tf.tanh(conv_lin(inpts + [reset * mem], \"c\", 0.0))\n",
      "  gate = sigmoid_cutoff(conv_lin(inpts + [mem], \"g\", 1.0), cutoff)\n",
      "  return gate * mem + (1 - gate) * candidate\n",
      "\n",
      "\n",
      "def relaxed_average(var_name_suffix, rx_step):\n",
      "  \"\"\"Calculate the average of relaxed variables having var_name_suffix.\"\"\"\n",
      "  relaxed_vars = []\n",
      "  for l in xrange(rx_step):\n",
      "    with tf.variable_scope(\"RX%d\" % l, reuse=True):\n",
      "      try:\n",
      "        relaxed_vars.append(tf.get_variable(var_name_suffix))\n",
      "      except ValueError:\n",
      "        pass\n",
      "  dsum = tf.add_n(relaxed_vars)\n",
      "  avg = dsum / len(relaxed_vars)\n",
      "  diff = [v - avg for v in relaxed_vars]\n",
      "  davg = tf.add_n([d*d for d in diff])\n",
      "  return avg, tf.reduce_sum(davg)\n",
      "\n",
      "\n",
      "def relaxed_distance(rx_step):\n",
      "  \"\"\"Distance between relaxed variables and their average.\"\"\"\n",
      "  res, ops, rx_done = [], [], {}\n",
      "  for v in tf.trainable_variables():\n",
      "    if v.name[0:2] == \"RX\":\n",
      "      rx_name = v.op.name[v.name.find(\"/\") + 1:]\n",
      "      if rx_name not in rx_done:\n",
      "        avg, dist_loss = relaxed_average(rx_name, rx_step)\n",
      "        res.append(dist_loss)\n",
      "        rx_done[rx_name] = avg\n",
      "      ops.append(v.assign(rx_done[rx_name]))\n",
      "  return tf.add_n(res), tf.group(*ops)\n",
      "\n",
      "\n",
      "def make_dense(targets, noclass):\n",
      "  \"\"\"Move a batch of targets to a dense 1-hot representation.\"\"\"\n",
      "  with tf.device(\"/cpu:0\"):\n",
      "    shape = tf.shape(targets)\n",
      "    batch_size = shape[0]\n",
      "    indices = targets + noclass * tf.range(0, batch_size)\n",
      "    length = batch_size * noclass\n",
      "    dense = tf.sparse_to_dense(indices, length, 1.0, 0.0)\n",
      "  return tf.reshape(dense, [-1, noclass])\n",
      "\n",
      "\n",
      "def check_for_zero(sparse):\n",
      "  \"\"\"In a sparse batch of ints, make 1.0 if it's 0 and 0.0 else.\"\"\"\n",
      "  with tf.device(\"/cpu:0\"):\n",
      "    shape = tf.shape(sparse)\n",
      "    batch_size = shape[0]\n",
      "    sparse = tf.minimum(sparse, 1)\n",
      "    indices = sparse + 2 * tf.range(0, batch_size)\n",
      "    dense = tf.sparse_to_dense(indices, 2 * batch_size, 1.0, 0.0)\n",
      "    reshaped = tf.reshape(dense, [-1, 2])\n",
      "  return tf.reshape(tf.slice(reshaped, [0, 0], [-1, 1]), [-1])\n",
      "\n",
      "\n",
      "class NeuralGPU(object):\n",
      "  \"\"\"Neural GPU Model.\"\"\"\n",
      "\n",
      "  def __init__(self, nmaps, vec_size, niclass, noclass, dropout, rx_step,\n",
      "               max_grad_norm, cutoff, nconvs, kw, kh, height, mode,\n",
      "               learning_rate, pull, pull_incr, min_length):\n",
      "    # Feeds for parameters and ops to update them.\n",
      "    self.global_step = tf.Variable(0, trainable=False)\n",
      "    self.cur_length = tf.Variable(min_length, trainable=False)\n",
      "    self.cur_length_incr_op = self.cur_length.assign_add(1)\n",
      "    self.lr = tf.Variable(float(learning_rate), trainable=False)\n",
      "    self.lr_decay_op = self.lr.assign(self.lr * 0.98)\n",
      "    self.pull = tf.Variable(float(pull), trainable=False)\n",
      "    self.pull_incr_op = self.pull.assign(self.pull * pull_incr)\n",
      "    self.do_training = tf.placeholder(tf.float32, name=\"do_training\")\n",
      "    self.noise_param = tf.placeholder(tf.float32, name=\"noise_param\")\n",
      "\n",
      "    # Feeds for inputs, targets, outputs, losses, etc.\n",
      "    self.input = []\n",
      "    self.target = []\n",
      "    for l in xrange(data_utils.forward_max + 1):\n",
      "      self.input.append(tf.placeholder(tf.int32, name=\"inp{0}\".format(l)))\n",
      "      self.target.append(tf.placeholder(tf.int32, name=\"tgt{0}\".format(l)))\n",
      "    self.outputs = []\n",
      "    self.losses = []\n",
      "    self.grad_norms = []\n",
      "    self.updates = []\n",
      "\n",
      "    # Computation.\n",
      "    inp0_shape = tf.shape(self.input[0])\n",
      "    batch_size = inp0_shape[0]\n",
      "    with tf.device(\"/cpu:0\"):\n",
      "      emb_weights = tf.get_variable(\n",
      "          \"embedding\", [niclass, vec_size],\n",
      "          initializer=tf.random_uniform_initializer(-1.7, 1.7))\n",
      "      e0 = tf.scatter_update(emb_weights,\n",
      "                             tf.constant(0, dtype=tf.int32, shape=[1]),\n",
      "                             tf.zeros([1, vec_size]))\n",
      "\n",
      "    adam = tf.train.AdamOptimizer(0.01*self.lr, epsilon=1e-5)\n",
      "\n",
      "    # Main graph creation loop, for every bin in data_utils.\n",
      "    self.steps = []\n",
      "    for length in sorted(list(set(data_utils.bins + [data_utils.forward_max]))):\n",
      "      data_utils.print_out(\"Creating model for bin of length %d.\" % length)\n",
      "      start_time = time.time()\n",
      "      if length > data_utils.bins[0]:\n",
      "        tf.get_variable_scope().reuse_variables()\n",
      "\n",
      "      # Embed inputs and calculate mask.\n",
      "      with tf.device(\"/cpu:0\"):\n",
      "        with tf.control_dependencies([e0]):\n",
      "          embedded = [tf.nn.embedding_lookup(emb_weights, self.input[l])\n",
      "                      for l in xrange(length)]\n",
      "        # Mask to 0-out padding space in each step.\n",
      "        imask = [check_for_zero(self.input[l]) for l in xrange(length)]\n",
      "        omask = [check_for_zero(self.target[l]) for l in xrange(length)]\n",
      "        mask = [1.0 - (imask[i] * omask[i]) for i in xrange(length)]\n",
      "        mask = [tf.reshape(m, [-1, 1]) for m in mask]\n",
      "        # Use a shifted mask for step scaling and concatenated for weights.\n",
      "        shifted_mask = mask + [tf.zeros_like(mask[0])]\n",
      "        scales = [shifted_mask[i] * (1.0 - shifted_mask[i+1])\n",
      "                  for i in xrange(length)]\n",
      "        scales = [tf.reshape(s, [-1, 1, 1, 1]) for s in scales]\n",
      "        mask = tf.concat(1, mask[0:length])  # batch x length\n",
      "        weights = mask\n",
      "        # Add a height dimension to mask to use later for masking.\n",
      "        mask = tf.reshape(mask, [-1, length, 1, 1])\n",
      "        mask = tf.concat(2, [mask for _ in xrange(height)]) + tf.zeros(\n",
      "            tf.pack([batch_size, length, height, nmaps]), dtype=tf.float32)\n",
      "\n",
      "      # Start is a length-list of batch-by-nmaps tensors, reshape and concat.\n",
      "      start = [tf.tanh(embedded[l]) for l in xrange(length)]\n",
      "      start = [tf.reshape(start[l], [-1, 1, nmaps]) for l in xrange(length)]\n",
      "      start = tf.reshape(tf.concat(1, start), [-1, length, 1, nmaps])\n",
      "\n",
      "      # First image comes from start by applying one convolution and adding 0s.\n",
      "      first = conv_linear(start, 1, 1, vec_size, nmaps, True, 0.0, \"input\")\n",
      "      first = [first] + [tf.zeros(tf.pack([batch_size, length, 1, nmaps]),\n",
      "                                  dtype=tf.float32) for _ in xrange(height - 1)]\n",
      "      first = tf.concat(2, first)\n",
      "\n",
      "      # Computation steps.\n",
      "      step = [tf.nn.dropout(first, 1.0 - self.do_training * dropout) * mask]\n",
      "      outputs = []\n",
      "      for it in xrange(length):\n",
      "        with tf.variable_scope(\"RX%d\" % (it % rx_step)) as vs:\n",
      "          if it >= rx_step:\n",
      "            vs.reuse_variables()\n",
      "          cur = step[it]\n",
      "          # Do nconvs-many CGRU steps.\n",
      "          for layer in xrange(nconvs):\n",
      "            cur = conv_gru([], cur, kw, kh, nmaps, cutoff, \"cgru_%d\" % layer)\n",
      "          cur = tf.nn.dropout(cur, 1.0 - self.do_training * dropout)\n",
      "          step.append(cur * mask)\n",
      "          outputs.append(tf.slice(step[-1], [0, 0, 0, 0], [-1, -1, 1, -1]))\n",
      "\n",
      "      self.steps.append([tf.reshape(s, [-1, length, height * nmaps])\n",
      "                         for s in step])\n",
      "      # Output is the n-th step output; n = current length, as in scales.\n",
      "      output = tf.add_n([outputs[i] * scales[i] for i in xrange(length)])\n",
      "      # Final convolution to get logits, list outputs.\n",
      "      output = conv_linear(output, 1, 1, nmaps, noclass, True, 0.0, \"output\")\n",
      "      output = tf.reshape(output, [-1, length, noclass])\n",
      "      self.outputs.append([tf.reshape(o, [-1, noclass])\n",
      "                           for o in list(tf.split(1, length, output))])\n",
      "\n",
      "      # Calculate cross-entropy loss and normalize it.\n",
      "      targets = tf.concat(1, [make_dense(self.target[l], noclass)\n",
      "                              for l in xrange(length)])\n",
      "      targets = tf.reshape(targets, [-1, noclass])\n",
      "      xent = tf.reshape(tf.nn.softmax_cross_entropy_with_logits(\n",
      "          tf.reshape(output, [-1, noclass]), targets), [-1, length])\n",
      "      perp_loss = tf.reduce_sum(xent * weights)\n",
      "      perp_loss /= tf.cast(batch_size, dtype=tf.float32)\n",
      "      perp_loss /= length\n",
      "\n",
      "      # Final loss: cross-entropy + shared parameter relaxation part.\n",
      "      relax_dist, self.avg_op = relaxed_distance(rx_step)\n",
      "      total_loss = perp_loss + relax_dist * self.pull\n",
      "      self.losses.append(perp_loss)\n",
      "\n",
      "      # Gradients and Adam update operation.\n",
      "      if length == data_utils.bins[0] or (mode == 0 and\n",
      "                                          length < data_utils.bins[-1] + 1):\n",
      "        data_utils.print_out(\"Creating backward for bin of length %d.\" % length)\n",
      "        params = tf.trainable_variables()\n",
      "        grads = tf.gradients(total_loss, params)\n",
      "        grads, norm = tf.clip_by_global_norm(grads, max_grad_norm)\n",
      "        self.grad_norms.append(norm)\n",
      "        for grad in grads:\n",
      "          if isinstance(grad, tf.Tensor):\n",
      "            grad += tf.truncated_normal(tf.shape(grad)) * self.noise_param\n",
      "        update = adam.apply_gradients(zip(grads, params),\n",
      "                                      global_step=self.global_step)\n",
      "        self.updates.append(update)\n",
      "      data_utils.print_out(\"Created model for bin of length %d in\"\n",
      "                           \" %.2f s.\" % (length, time.time() - start_time))\n",
      "    self.saver = tf.train.Saver(tf.all_variables())\n",
      "\n",
      "  def step(self, sess, inp, target, do_backward, noise_param=None):\n",
      "    \"\"\"Run a step of the network.\"\"\"\n",
      "    assert len(inp) == len(target)\n",
      "    length = len(target)\n",
      "    feed_in = {}\n",
      "    feed_in[self.noise_param.name] = noise_param if noise_param else 0.0\n",
      "    feed_in[self.do_training.name] = 1.0 if do_backward else 0.0\n",
      "    feed_out = []\n",
      "    index = len(data_utils.bins)\n",
      "    if length < data_utils.bins[-1] + 1:\n",
      "      index = data_utils.bins.index(length)\n",
      "    if do_backward:\n",
      "      feed_out.append(self.updates[index])\n",
      "      feed_out.append(self.grad_norms[index])\n",
      "    feed_out.append(self.losses[index])\n",
      "    for l in xrange(length):\n",
      "      feed_in[self.input[l].name] = inp[l]\n",
      "    for l in xrange(length):\n",
      "      feed_in[self.target[l].name] = target[l]\n",
      "      feed_out.append(self.outputs[index][l])\n",
      "    for l in xrange(length+1):\n",
      "      feed_out.append(self.steps[index][l])\n",
      "    res = sess.run(feed_out, feed_in)\n",
      "    offset = 0\n",
      "    norm = None\n",
      "    if do_backward:\n",
      "      offset = 2\n",
      "      norm = res[1]\n",
      "    outputs = res[offset + 1:offset + 1 + length]\n",
      "    steps = res[offset + 1 + length:]\n",
      "    return res[offset], outputs, norm, steps\n",
      "\n",
      "@@ -1,3 +1,19 @@\n",
      "+# Copyright 2015 Google Inc. All Rights Reserved.\n",
      "+#\n",
      "+# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
      "+# you may not use this file except in compliance with the License.\n",
      "+# You may obtain a copy of the License at\n",
      "+#\n",
      "+#     http://www.apache.org/licenses/LICENSE-2.0\n",
      "+#\n",
      "+# Unless required by applicable law or agreed to in writing, software\n",
      "+# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
      "+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
      "+# See the License for the specific language governing permissions and\n",
      "+# limitations under the License.\n",
      "+#\n",
      "+#==============================================================================\n",
      "+\n",
      " \"\"\"The Neural GPU Model.\"\"\"\n",
      " \n",
      " import time\n",
      "\n",
      "\n",
      "__Before__: \n",
      "\"\"\"Neural GPU for Learning Algorithms.\"\"\"\n",
      "\n",
      "import math\n",
      "import os\n",
      "import random\n",
      "import sys\n",
      "import time\n",
      "\n",
      "import google3\n",
      "\n",
      "import matplotlib.animation as anim\n",
      "import matplotlib.pyplot as plt\n",
      "import numpy as np\n",
      "import tensorflow as tf\n",
      "\n",
      "from google3.third_party.tensorflow.python.platform import gfile\n",
      "import google3.experimental.users.lukaszkaiser.neural_gpu.data_utils as data\n",
      "import google3.experimental.users.lukaszkaiser.neural_gpu.neural_gpu as ngpu\n",
      "\n",
      "tf.app.flags.DEFINE_float(\"lr\", 0.1, \"Learning rate.\")\n",
      "tf.app.flags.DEFINE_float(\"init_weight\", 1.0, \"Initial weights deviation.\")\n",
      "tf.app.flags.DEFINE_float(\"max_grad_norm\", 0.05, \"Clip gradients to this norm.\")\n",
      "tf.app.flags.DEFINE_float(\"cutoff\", 1.2, \"Cutoff at the gates.\")\n",
      "tf.app.flags.DEFINE_float(\"pull\", 0.0005, \"Starting pull of the relaxations.\")\n",
      "tf.app.flags.DEFINE_float(\"pull_incr\", 1.2, \"Increase pull by that much.\")\n",
      "tf.app.flags.DEFINE_float(\"dropout\", 0.2, \"Dropout that much.\")\n",
      "tf.app.flags.DEFINE_float(\"grad_noise_scale\", 1.0, \"Gradient noise scale.\")\n",
      "tf.app.flags.DEFINE_integer(\"batch_size\", 64, \"Batch size.\")\n",
      "tf.app.flags.DEFINE_integer(\"low_batch_size\", 16, \"Low batch size.\")\n",
      "tf.app.flags.DEFINE_integer(\"steps_per_checkpoint\", 100, \"Steps per epoch.\")\n",
      "tf.app.flags.DEFINE_integer(\"nmaps\", 24, \"Number of floats in each cell.\")\n",
      "tf.app.flags.DEFINE_integer(\"niclass\", 14, \"Number of classes (0 is padding).\")\n",
      "tf.app.flags.DEFINE_integer(\"noclass\", 14, \"Number of classes (0 is padding).\")\n",
      "tf.app.flags.DEFINE_integer(\"train_data_size\", 5000, \"Training examples/len.\")\n",
      "tf.app.flags.DEFINE_integer(\"max_length\", 41, \"Maximum length.\")\n",
      "tf.app.flags.DEFINE_integer(\"rx_step\", 6, \"Relax that many recursive steps.\")\n",
      "tf.app.flags.DEFINE_integer(\"random_seed\", 125459, \"Random seed.\")\n",
      "tf.app.flags.DEFINE_integer(\"nconvs\", 2, \"How many convolutions / 1 step.\")\n",
      "tf.app.flags.DEFINE_integer(\"kw\", 3, \"Kernel width.\")\n",
      "tf.app.flags.DEFINE_integer(\"kh\", 3, \"Kernel height.\")\n",
      "tf.app.flags.DEFINE_integer(\"height\", 4, \"Height.\")\n",
      "tf.app.flags.DEFINE_integer(\"forward_max\", 401, \"Maximum forward length.\")\n",
      "tf.app.flags.DEFINE_integer(\"jobid\", -1, \"Task id when running on borg.\")\n",
      "tf.app.flags.DEFINE_integer(\"nprint\", 0, \"How many test examples to print out.\")\n",
      "tf.app.flags.DEFINE_integer(\"mode\", 0, \"Mode: 0-train other-decode.\")\n",
      "tf.app.flags.DEFINE_string(\"task\", \"rev\", \"Which task are we learning?\")\n",
      "tf.app.flags.DEFINE_string(\"train_dir\", \"/tmp/\", \"Directory to store models.\")\n",
      "\n",
      "FLAGS = tf.app.flags.FLAGS\n",
      "\n",
      "\n",
      "def initialize(sess):\n",
      "  \"\"\"Initialize data and model.\"\"\"\n",
      "  if FLAGS.jobid >= 0:\n",
      "    data.log_filename = os.path.join(FLAGS.train_dir, \"log%d\" % FLAGS.jobid)\n",
      "  data.print_out(\"NN \", newline=False)\n",
      "\n",
      "  # Set random seed.\n",
      "  seed = FLAGS.random_seed + max(0, FLAGS.jobid)\n",
      "  tf.set_random_seed(seed)\n",
      "  random.seed(seed)\n",
      "  np.random.seed(seed)\n",
      "\n",
      "  # Check data sizes.\n",
      "  data.forward_max = max(FLAGS.forward_max, data.bins[-1])\n",
      "  assert data.bins\n",
      "  min_length = 3\n",
      "  max_length = min(FLAGS.max_length, data.bins[-1])\n",
      "  assert max_length + 1 > min_length\n",
      "  while len(data.bins) > 1 and data.bins[-2] > max_length + 12:\n",
      "    data.bins = data.bins[:-1]\n",
      "  assert data.bins[0] > FLAGS.rx_step\n",
      "  nclass = min(FLAGS.niclass, FLAGS.noclass)\n",
      "  data_size = FLAGS.train_data_size if FLAGS.mode == 0 else 1000\n",
      "\n",
      "  # Initialize data for each task.\n",
      "  tasks = FLAGS.task.split(\"-\")\n",
      "  for t in tasks:\n",
      "    for l in xrange(max_length + 11):\n",
      "      data.init_data(t, l, data_size, nclass)\n",
      "    data.init_data(t, data.bins[-2], data_size, nclass)\n",
      "    data.init_data(t, data.bins[-1], data_size, nclass)\n",
      "    end_size = 4 * 1024 if FLAGS.mode > 0 else 1024\n",
      "    data.init_data(t, data.forward_max, end_size, nclass)\n",
      "\n",
      "  # Print out parameters.\n",
      "  curriculum = 0.12\n",
      "  fin = (\"cv %d kw %d h %d kh %d rxr %d bs %d ns %.2f t %s\"\n",
      "         % (FLAGS.nconvs, FLAGS.kw, FLAGS.height, FLAGS.kh, FLAGS.rx_step,\n",
      "            FLAGS.batch_size, FLAGS.grad_noise_scale, FLAGS.task))\n",
      "  fin = \"data %d %s\" % (FLAGS.train_data_size, fin)\n",
      "  tag = (\"df %.2f p %.3f lr %.2f iw %.2f cr %.2f nm %d d%.4f gn %.2f %s\" %\n",
      "         (FLAGS.cutoff, FLAGS.pull_incr, FLAGS.lr, FLAGS.init_weight,\n",
      "          curriculum, FLAGS.nmaps, FLAGS.dropout, FLAGS.max_grad_norm, fin))\n",
      "  data.print_out(tag)\n",
      "\n",
      "  # Create checkpoint directory if it does not exist.\n",
      "  checkpoint_dir = os.path.join(FLAGS.train_dir, \"neural_gpu%s\"\n",
      "                                % (\"\" if FLAGS.jobid < 0 else str(FLAGS.jobid)))\n",
      "  if not gfile.IsDirectory(checkpoint_dir):\n",
      "    data.print_out(\"Creating checkpoint directory %s.\" % checkpoint_dir)\n",
      "    gfile.MkDir(checkpoint_dir)\n",
      "\n",
      "  # Create model and initialize it.\n",
      "  tf.get_variable_scope().set_initializer(\n",
      "      tf.uniform_unit_scaling_initializer(factor=1.8 * FLAGS.init_weight))\n",
      "  model = ngpu.NeuralGPU(\n",
      "      FLAGS.nmaps, FLAGS.nmaps, FLAGS.niclass, FLAGS.noclass, FLAGS.dropout,\n",
      "      FLAGS.rx_step, FLAGS.max_grad_norm, FLAGS.cutoff, FLAGS.nconvs,\n",
      "      FLAGS.kw, FLAGS.kh, FLAGS.height, FLAGS.mode, FLAGS.lr,\n",
      "      FLAGS.pull, FLAGS.pull_incr, min_length + 3)\n",
      "  data.print_out(\"Created model.\")\n",
      "  sess.run(tf.initialize_all_variables())\n",
      "  data.print_out(\"Initialized variables.\")\n",
      "\n",
      "  # Load model from parameters if a checkpoint exists.\n",
      "  ckpt = tf.train.get_checkpoint_state(checkpoint_dir)\n",
      "  if ckpt and gfile.Exists(ckpt.model_checkpoint_path):\n",
      "    data.print_out(\"Reading model parameters from %s\"\n",
      "                   % ckpt.model_checkpoint_path)\n",
      "    model.saver.restore(sess, ckpt.model_checkpoint_path)\n",
      "\n",
      "  # Return the model and needed variables.\n",
      "  return (model, min_length, max_length, checkpoint_dir, curriculum)\n",
      "\n",
      "\n",
      "def single_test(l, model, sess, task, nprint, batch_size, print_out=True,\n",
      "                offset=None):\n",
      "  \"\"\"Test model on test data of length l using the given session.\"\"\"\n",
      "  inpt, target = data.get_batch(l, batch_size, False, task, offset)\n",
      "  _, res, _, steps = model.step(sess, inpt, target, False)\n",
      "  errors, total, seq = data.accuracy(inpt, res, target, batch_size, nprint)\n",
      "  seq = float(seq) / batch_size\n",
      "  if total > 0:\n",
      "    errors = float(errors) / total\n",
      "  if print_out:\n",
      "    data.print_out(\"  %s len %d errors %.2f sequence-errors %.2f\"\n",
      "                   % (task, l, 100*errors, 100*seq))\n",
      "  return errors, seq, (steps, inpt, [np.argmax(o, axis=1) for o in res])\n",
      "\n",
      "\n",
      "def multi_test(l, model, sess, task, nprint, batch_size, offset=None):\n",
      "  \"\"\"Run multiple tests at lower batch size to save memory.\"\"\"\n",
      "  errors = 0.0\n",
      "  seq = 0.0\n",
      "  to_print = nprint\n",
      "  low_batch = FLAGS.low_batch_size\n",
      "  low_batch = min(low_batch, batch_size)\n",
      "  for mstep in xrange(batch_size / low_batch):\n",
      "    cur_offset = None if offset is None else offset + mstep * low_batch\n",
      "    err, sq, _ = single_test(l, model, sess, task, to_print, low_batch, False,\n",
      "                             cur_offset)\n",
      "    to_print = max(0, to_print - low_batch)\n",
      "    errors += err\n",
      "    seq += sq\n",
      "    if FLAGS.mode > 0:\n",
      "      cur_errors = float(low_batch * errors) / ((mstep+1) * low_batch)\n",
      "      cur_seq = float(low_batch * seq) / ((mstep+1) * low_batch)\n",
      "      data.print_out(\"    %s multitest current errors %.2f sequence-errors %.2f\"\n",
      "                     % (task, 100*cur_errors, 100*cur_seq))\n",
      "  errors = float(low_batch) * float(errors) / batch_size\n",
      "  seq = float(low_batch) * float(seq) / batch_size\n",
      "  data.print_out(\"  %s len %d errors %.2f sequence-errors %.2f\"\n",
      "                 % (task, l, 100*errors, 100*seq))\n",
      "  return errors, seq\n",
      "\n",
      "\n",
      "def train():\n",
      "  \"\"\"Main training function.\"\"\"\n",
      "  batch_size = FLAGS.batch_size\n",
      "  tasks = FLAGS.task.split(\"-\")\n",
      "  with tf.Session() as sess:\n",
      "    model, min_length, max_length, checkpoint_dir, curriculum = initialize(sess)\n",
      "    max_cur_length = min(min_length + 3, max_length)\n",
      "    prev_acc_perp = [1000000 for _ in xrange(3)]\n",
      "    prev_sq = 1.0\n",
      "\n",
      "    while True:\n",
      "      global_step, pull, max_cur_length, learning_rate = sess.run(\n",
      "          [model.global_step, model.pull, model.cur_length, model.lr])\n",
      "      ep = global_step / FLAGS.steps_per_checkpoint\n",
      "      acc_loss, acc_total, acc_errors, acc_seq = 0.0, 0, 0, 0\n",
      "      acc_grad_norm, step_count, step_time = 0.0, 0, 0.0\n",
      "      for _ in xrange(FLAGS.steps_per_checkpoint):\n",
      "        global_step += 1\n",
      "        task = random.choice(tasks)\n",
      "        l1 = np.random.randint(max_cur_length - min_length + 1) + min_length\n",
      "        l = l1\n",
      "        if np.random.randint(10) > 3:  # Prefer longer stuff 60% of time.\n",
      "          l = np.random.randint(max_cur_length - min_length+1) + min_length\n",
      "          l = max(l, l1)\n",
      "        if np.random.randint(4) < 1:  # Mixed learning: once in a while big.\n",
      "          l = np.random.randint(max_length - min_length + 1) + min_length\n",
      "          l = max(l, l1)\n",
      "        start_time = time.time()\n",
      "        inp, target = data.get_batch(l, batch_size, True, task)\n",
      "        stepp = math.pow(global_step, -0.55)\n",
      "        noise_param = math.sqrt(stepp * 20 * prev_sq) * FLAGS.grad_noise_scale\n",
      "        loss, res, gnorm, _ = model.step(sess, inp, target, True, noise_param)\n",
      "        step_time += time.time() - start_time\n",
      "        acc_grad_norm += float(gnorm)\n",
      "        if l < max_cur_length + 1:\n",
      "          step_count += 1\n",
      "          acc_loss += loss\n",
      "          errors, total, seq = data.accuracy(inp, res, target,\n",
      "                                             batch_size, 0)\n",
      "          acc_total += total\n",
      "          acc_errors += errors\n",
      "          acc_seq += seq\n",
      "      acc_loss /= step_count\n",
      "      step_time /= FLAGS.steps_per_checkpoint\n",
      "      acc_seq = float(acc_seq) / (step_count * batch_size)\n",
      "      prev_sq = acc_seq\n",
      "      acc_errors = float(acc_errors) / acc_total if acc_total > 0 else 1.0\n",
      "      msg1 = \"ep %d st %.2f lr %.8f\" % (ep, step_time, learning_rate)\n",
      "      msg2 = \"pl %.3f cme %.3f\" % (pull, curriculum)\n",
      "      msg = (\"%s %s gn %.8f\"\n",
      "             % (msg1, msg2, acc_grad_norm / FLAGS.steps_per_checkpoint))\n",
      "      data.print_out(\"%s len %d ppx %.8f errs %.2f sq %.2f\" %\n",
      "                     (msg, max_cur_length, data.safe_exp(acc_loss),\n",
      "                      100*acc_errors, 100*acc_seq))\n",
      "      if curriculum > acc_seq:\n",
      "        prev_acc_perp.append(1000000)\n",
      "        do_incr = True\n",
      "        while do_incr and max_cur_length < max_length:\n",
      "          sess.run(model.cur_length_incr_op)\n",
      "          for t in tasks:\n",
      "            if data.train_set[t]: do_incr = False\n",
      "        if pull < 1:\n",
      "          sess.run(model.pull_incr_op)\n",
      "        else:\n",
      "          data.print_out(\"  Averaging parameters.\")\n",
      "          sess.run([model.avg_op, model.lr_decay_op])\n",
      "      else:\n",
      "        acc_perp = data.safe_exp(acc_loss)\n",
      "        if acc_perp > max(prev_acc_perp[-3:]):\n",
      "          sess.run(model.lr_decay_op)\n",
      "        prev_acc_perp.append(acc_perp)\n",
      "      checkpoint_path = os.path.join(checkpoint_dir, \"neural_gpu.ckpt\")\n",
      "      model.saver.save(sess, checkpoint_path,\n",
      "                       global_step=model.global_step)\n",
      "      # Run evaluation.\n",
      "      should_exit = True\n",
      "      bound = data.bins[-1] + 1\n",
      "      for t in tasks:\n",
      "        l = min_length\n",
      "        while l < max_length + 12 and l < bound:\n",
      "          _, sq, _ = single_test(l, model, sess, t, FLAGS.nprint, batch_size)\n",
      "          l += 1\n",
      "          while l < bound + 1 and not data.test_set[t][l]:\n",
      "            l += 1\n",
      "        if sq < 0.5:\n",
      "          _, sq = multi_test(data.forward_max, model, sess, t, FLAGS.nprint,\n",
      "                             batch_size * 4)\n",
      "        if sq > 0.001: should_exit = False\n",
      "      if should_exit:\n",
      "        if data.forward_max > 4000 and len(tasks) == 1:\n",
      "          multi_test(data.forward_max, model, sess, tasks[0], FLAGS.nprint,\n",
      "                     batch_size * 16, 0)\n",
      "\n",
      "\n",
      "def animate(l, test_data, anim_size):\n",
      "  \"\"\"Create animation for the given data (hacky matplotlib use).\"\"\"\n",
      "  xf = 12\n",
      "  fps = 2\n",
      "  fig = plt.figure(figsize=(16, 9), facecolor=\"white\")\n",
      "  ax = fig.add_axes([0, 0, 1, 1], frameon=False, zorder=2)\n",
      "  ax.set_xticks([i * 24-0.5 for i in xrange(4)])\n",
      "  ax.set_xticklabels([])\n",
      "  ax.set_yticks([i - 0.5 for i in xrange(l+1)])\n",
      "  ax.grid(which=\"major\", axis=\"both\", linestyle=\"-\", color=\"black\")\n",
      "  text_fields = []\n",
      "  text_size = 24*32/l\n",
      "  for y in xrange(l):\n",
      "    text_fields.append(ax.text(\n",
      "        11.25, y + 0.15, \"\", color=\"g\", ha=\"center\", va=\"center\",\n",
      "        bbox={\"facecolor\": \"b\", \"alpha\": 0.01, \"pad\": 24 * text_size},\n",
      "        size=text_size - (4 * 32 / l), animated=True))\n",
      "  im = ax.imshow(np.zeros_like(test_data[0][0][0]), vmin=-1.0,\n",
      "                 vmax=1.0, cmap=\"gray\", aspect=\"auto\", origin=\"upper\",\n",
      "                 interpolation=\"none\", animated=True)\n",
      "  im.set_zorder(1)\n",
      "  def to_symbol(i):\n",
      "    if i == 0: return \"\"\n",
      "    if i == 11: return \"+\"\n",
      "    if i == 12: return \"*\"\n",
      "    return str(i-1)\n",
      "  def animation_update(frame_no, test_data, xf, im, text_fields):\n",
      "    \"\"\"Update an animation frame.\"\"\"\n",
      "    steps, inpt, out_raw = test_data\n",
      "    length = len(steps)\n",
      "    batch = frame_no / (fps * (l+4*xf))\n",
      "    index = int((frame_no % (fps * (l+4*xf))) / fps)\n",
      "    # Cut output after first padding.\n",
      "    out = [out_raw[i][batch] for i in xrange(len(text_fields))]\n",
      "    if 0 in out:\n",
      "      i = out.index(0)\n",
      "      out = out[0:i] + [0 for _ in xrange(len(out) - i)]\n",
      "    # Show the state after the first frames.\n",
      "    if index >= 2*xf:\n",
      "      im.set_array(steps[min(length - 1, index - 2*xf)][batch])\n",
      "      for i, t in enumerate(text_fields):\n",
      "        if index - 2*xf < length:\n",
      "          t.set_text(\"\")\n",
      "        else:\n",
      "          t.set_text(to_symbol(out[i]))\n",
      "    else:\n",
      "      for i, t in enumerate(text_fields):\n",
      "        t.set_text(to_symbol(inpt[i][batch]) if index < xf else \"\")\n",
      "      if index < xf:\n",
      "        im.set_array(np.zeros_like(steps[0][0]))\n",
      "      else:\n",
      "        im.set_array(steps[0][batch])\n",
      "    return im,\n",
      "  animation = anim.FuncAnimation(\n",
      "      fig, animation_update, blit=True, frames=(l+4*xf)*anim_size*fps,\n",
      "      interval=500/fps, fargs=(test_data, xf, im, text_fields))\n",
      "  animation.save(\"/tmp/neural_gpu.mp4\", writer=\"mencoder\", fps=4*fps, dpi=3*80)\n",
      "\n",
      "\n",
      "def evaluate():\n",
      "  \"\"\"Evaluate an existing model.\"\"\"\n",
      "  batch_size = FLAGS.batch_size\n",
      "  tasks = FLAGS.task.split(\"-\")\n",
      "  with tf.Session() as sess:\n",
      "    model, min_length, max_length, _, _ = initialize(sess)\n",
      "    bound = data.bins[-1] + 1\n",
      "    for t in tasks:\n",
      "      l = min_length\n",
      "      while l < max_length + 12 and l < bound:\n",
      "        _, sq, _ = single_test(l, model, sess, t, FLAGS.nprint, batch_size)\n",
      "        l += 1\n",
      "        while l < bound + 1 and not data.test_set[t][l]:\n",
      "          l += 1\n",
      "      # Animate.\n",
      "      anim_size = 2\n",
      "      _, _, test_data = single_test(l, model, sess, t, 0, anim_size)\n",
      "      animate(l, test_data, anim_size)\n",
      "      # More tests.\n",
      "      _, sq = multi_test(data.forward_max, model, sess, t, FLAGS.nprint,\n",
      "                         batch_size * 4)\n",
      "    if sq < 0.01:  # More tests.\n",
      "      if data.forward_max > 4000 and len(tasks) == 1:\n",
      "        multi_test(data.forward_max, model, sess, tasks[0], FLAGS.nprint,\n",
      "                   batch_size * 64, 0)\n",
      "\n",
      "\n",
      "def interactive():\n",
      "  \"\"\"Interactively probe an existing model.\"\"\"\n",
      "  with tf.Session() as sess:\n",
      "    model, _, _, _, _ = initialize(sess)\n",
      "    sys.stdout.write(\"> \")\n",
      "    sys.stdout.flush()\n",
      "    inpt = sys.stdin.readline()\n",
      "    while inpt:\n",
      "      ids = [int(c) for c in inpt.strip()]\n",
      "      inpt, target = data.get_batch(len(ids), 1, False, \"\",\n",
      "                                    preset=(ids, [0 for _ in ids]))\n",
      "      _, res, _, _ = model.step(sess, inpt, target, False)\n",
      "      res = [np.argmax(o, axis=1) for o in res]\n",
      "      print \" \".join([str(output[0]) for output in res])\n",
      "      sys.stdout.write(\"> \")\n",
      "      sys.stdout.flush()\n",
      "      inpt = sys.stdin.readline()\n",
      "\n",
      "\n",
      "def main(_):\n",
      "  if FLAGS.mode == 0:\n",
      "    train()\n",
      "  elif FLAGS.mode == 1:\n",
      "    evaluate()\n",
      "  else:\n",
      "    interactive()\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "  tf.app.run()\n",
      "\n",
      "__After__: \n",
      "# Copyright 2015 Google Inc. All Rights Reserved.\n",
      "#\n",
      "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
      "# you may not use this file except in compliance with the License.\n",
      "# You may obtain a copy of the License at\n",
      "#\n",
      "#     http://www.apache.org/licenses/LICENSE-2.0\n",
      "#\n",
      "# Unless required by applicable law or agreed to in writing, software\n",
      "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
      "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
      "# See the License for the specific language governing permissions and\n",
      "# limitations under the License.\n",
      "#\n",
      "#==============================================================================\n",
      "\n",
      "\"\"\"Neural GPU for Learning Algorithms.\"\"\"\n",
      "\n",
      "import math\n",
      "import os\n",
      "import random\n",
      "import sys\n",
      "import time\n",
      "\n",
      "import google3\n",
      "\n",
      "import matplotlib.animation as anim\n",
      "import matplotlib.pyplot as plt\n",
      "import numpy as np\n",
      "import tensorflow as tf\n",
      "\n",
      "from google3.third_party.tensorflow.python.platform import gfile\n",
      "import google3.experimental.users.lukaszkaiser.neural_gpu.data_utils as data\n",
      "import google3.experimental.users.lukaszkaiser.neural_gpu.neural_gpu as ngpu\n",
      "\n",
      "tf.app.flags.DEFINE_float(\"lr\", 0.1, \"Learning rate.\")\n",
      "tf.app.flags.DEFINE_float(\"init_weight\", 1.0, \"Initial weights deviation.\")\n",
      "tf.app.flags.DEFINE_float(\"max_grad_norm\", 0.05, \"Clip gradients to this norm.\")\n",
      "tf.app.flags.DEFINE_float(\"cutoff\", 1.2, \"Cutoff at the gates.\")\n",
      "tf.app.flags.DEFINE_float(\"pull\", 0.0005, \"Starting pull of the relaxations.\")\n",
      "tf.app.flags.DEFINE_float(\"pull_incr\", 1.2, \"Increase pull by that much.\")\n",
      "tf.app.flags.DEFINE_float(\"dropout\", 0.2, \"Dropout that much.\")\n",
      "tf.app.flags.DEFINE_float(\"grad_noise_scale\", 1.0, \"Gradient noise scale.\")\n",
      "tf.app.flags.DEFINE_integer(\"batch_size\", 64, \"Batch size.\")\n",
      "tf.app.flags.DEFINE_integer(\"low_batch_size\", 16, \"Low batch size.\")\n",
      "tf.app.flags.DEFINE_integer(\"steps_per_checkpoint\", 100, \"Steps per epoch.\")\n",
      "tf.app.flags.DEFINE_integer(\"nmaps\", 24, \"Number of floats in each cell.\")\n",
      "tf.app.flags.DEFINE_integer(\"niclass\", 14, \"Number of classes (0 is padding).\")\n",
      "tf.app.flags.DEFINE_integer(\"noclass\", 14, \"Number of classes (0 is padding).\")\n",
      "tf.app.flags.DEFINE_integer(\"train_data_size\", 5000, \"Training examples/len.\")\n",
      "tf.app.flags.DEFINE_integer(\"max_length\", 41, \"Maximum length.\")\n",
      "tf.app.flags.DEFINE_integer(\"rx_step\", 6, \"Relax that many recursive steps.\")\n",
      "tf.app.flags.DEFINE_integer(\"random_seed\", 125459, \"Random seed.\")\n",
      "tf.app.flags.DEFINE_integer(\"nconvs\", 2, \"How many convolutions / 1 step.\")\n",
      "tf.app.flags.DEFINE_integer(\"kw\", 3, \"Kernel width.\")\n",
      "tf.app.flags.DEFINE_integer(\"kh\", 3, \"Kernel height.\")\n",
      "tf.app.flags.DEFINE_integer(\"height\", 4, \"Height.\")\n",
      "tf.app.flags.DEFINE_integer(\"forward_max\", 401, \"Maximum forward length.\")\n",
      "tf.app.flags.DEFINE_integer(\"jobid\", -1, \"Task id when running on borg.\")\n",
      "tf.app.flags.DEFINE_integer(\"nprint\", 0, \"How many test examples to print out.\")\n",
      "tf.app.flags.DEFINE_integer(\"mode\", 0, \"Mode: 0-train other-decode.\")\n",
      "tf.app.flags.DEFINE_string(\"task\", \"rev\", \"Which task are we learning?\")\n",
      "tf.app.flags.DEFINE_string(\"train_dir\", \"/tmp/\", \"Directory to store models.\")\n",
      "\n",
      "FLAGS = tf.app.flags.FLAGS\n",
      "\n",
      "\n",
      "def initialize(sess):\n",
      "  \"\"\"Initialize data and model.\"\"\"\n",
      "  if FLAGS.jobid >= 0:\n",
      "    data.log_filename = os.path.join(FLAGS.train_dir, \"log%d\" % FLAGS.jobid)\n",
      "  data.print_out(\"NN \", newline=False)\n",
      "\n",
      "  # Set random seed.\n",
      "  seed = FLAGS.random_seed + max(0, FLAGS.jobid)\n",
      "  tf.set_random_seed(seed)\n",
      "  random.seed(seed)\n",
      "  np.random.seed(seed)\n",
      "\n",
      "  # Check data sizes.\n",
      "  data.forward_max = max(FLAGS.forward_max, data.bins[-1])\n",
      "  assert data.bins\n",
      "  min_length = 3\n",
      "  max_length = min(FLAGS.max_length, data.bins[-1])\n",
      "  assert max_length + 1 > min_length\n",
      "  while len(data.bins) > 1 and data.bins[-2] > max_length + 12:\n",
      "    data.bins = data.bins[:-1]\n",
      "  assert data.bins[0] > FLAGS.rx_step\n",
      "  nclass = min(FLAGS.niclass, FLAGS.noclass)\n",
      "  data_size = FLAGS.train_data_size if FLAGS.mode == 0 else 1000\n",
      "\n",
      "  # Initialize data for each task.\n",
      "  tasks = FLAGS.task.split(\"-\")\n",
      "  for t in tasks:\n",
      "    for l in xrange(max_length + 11):\n",
      "      data.init_data(t, l, data_size, nclass)\n",
      "    data.init_data(t, data.bins[-2], data_size, nclass)\n",
      "    data.init_data(t, data.bins[-1], data_size, nclass)\n",
      "    end_size = 4 * 1024 if FLAGS.mode > 0 else 1024\n",
      "    data.init_data(t, data.forward_max, end_size, nclass)\n",
      "\n",
      "  # Print out parameters.\n",
      "  curriculum = 0.12\n",
      "  fin = (\"cv %d kw %d h %d kh %d rxr %d bs %d ns %.2f t %s\"\n",
      "         % (FLAGS.nconvs, FLAGS.kw, FLAGS.height, FLAGS.kh, FLAGS.rx_step,\n",
      "            FLAGS.batch_size, FLAGS.grad_noise_scale, FLAGS.task))\n",
      "  fin = \"data %d %s\" % (FLAGS.train_data_size, fin)\n",
      "  tag = (\"df %.2f p %.3f lr %.2f iw %.2f cr %.2f nm %d d%.4f gn %.2f %s\" %\n",
      "         (FLAGS.cutoff, FLAGS.pull_incr, FLAGS.lr, FLAGS.init_weight,\n",
      "          curriculum, FLAGS.nmaps, FLAGS.dropout, FLAGS.max_grad_norm, fin))\n",
      "  data.print_out(tag)\n",
      "\n",
      "  # Create checkpoint directory if it does not exist.\n",
      "  checkpoint_dir = os.path.join(FLAGS.train_dir, \"neural_gpu%s\"\n",
      "                                % (\"\" if FLAGS.jobid < 0 else str(FLAGS.jobid)))\n",
      "  if not gfile.IsDirectory(checkpoint_dir):\n",
      "    data.print_out(\"Creating checkpoint directory %s.\" % checkpoint_dir)\n",
      "    gfile.MkDir(checkpoint_dir)\n",
      "\n",
      "  # Create model and initialize it.\n",
      "  tf.get_variable_scope().set_initializer(\n",
      "      tf.uniform_unit_scaling_initializer(factor=1.8 * FLAGS.init_weight))\n",
      "  model = ngpu.NeuralGPU(\n",
      "      FLAGS.nmaps, FLAGS.nmaps, FLAGS.niclass, FLAGS.noclass, FLAGS.dropout,\n",
      "      FLAGS.rx_step, FLAGS.max_grad_norm, FLAGS.cutoff, FLAGS.nconvs,\n",
      "      FLAGS.kw, FLAGS.kh, FLAGS.height, FLAGS.mode, FLAGS.lr,\n",
      "      FLAGS.pull, FLAGS.pull_incr, min_length + 3)\n",
      "  data.print_out(\"Created model.\")\n",
      "  sess.run(tf.initialize_all_variables())\n",
      "  data.print_out(\"Initialized variables.\")\n",
      "\n",
      "  # Load model from parameters if a checkpoint exists.\n",
      "  ckpt = tf.train.get_checkpoint_state(checkpoint_dir)\n",
      "  if ckpt and gfile.Exists(ckpt.model_checkpoint_path):\n",
      "    data.print_out(\"Reading model parameters from %s\"\n",
      "                   % ckpt.model_checkpoint_path)\n",
      "    model.saver.restore(sess, ckpt.model_checkpoint_path)\n",
      "\n",
      "  # Return the model and needed variables.\n",
      "  return (model, min_length, max_length, checkpoint_dir, curriculum)\n",
      "\n",
      "\n",
      "def single_test(l, model, sess, task, nprint, batch_size, print_out=True,\n",
      "                offset=None):\n",
      "  \"\"\"Test model on test data of length l using the given session.\"\"\"\n",
      "  inpt, target = data.get_batch(l, batch_size, False, task, offset)\n",
      "  _, res, _, steps = model.step(sess, inpt, target, False)\n",
      "  errors, total, seq = data.accuracy(inpt, res, target, batch_size, nprint)\n",
      "  seq = float(seq) / batch_size\n",
      "  if total > 0:\n",
      "    errors = float(errors) / total\n",
      "  if print_out:\n",
      "    data.print_out(\"  %s len %d errors %.2f sequence-errors %.2f\"\n",
      "                   % (task, l, 100*errors, 100*seq))\n",
      "  return errors, seq, (steps, inpt, [np.argmax(o, axis=1) for o in res])\n",
      "\n",
      "\n",
      "def multi_test(l, model, sess, task, nprint, batch_size, offset=None):\n",
      "  \"\"\"Run multiple tests at lower batch size to save memory.\"\"\"\n",
      "  errors = 0.0\n",
      "  seq = 0.0\n",
      "  to_print = nprint\n",
      "  low_batch = FLAGS.low_batch_size\n",
      "  low_batch = min(low_batch, batch_size)\n",
      "  for mstep in xrange(batch_size / low_batch):\n",
      "    cur_offset = None if offset is None else offset + mstep * low_batch\n",
      "    err, sq, _ = single_test(l, model, sess, task, to_print, low_batch, False,\n",
      "                             cur_offset)\n",
      "    to_print = max(0, to_print - low_batch)\n",
      "    errors += err\n",
      "    seq += sq\n",
      "    if FLAGS.mode > 0:\n",
      "      cur_errors = float(low_batch * errors) / ((mstep+1) * low_batch)\n",
      "      cur_seq = float(low_batch * seq) / ((mstep+1) * low_batch)\n",
      "      data.print_out(\"    %s multitest current errors %.2f sequence-errors %.2f\"\n",
      "                     % (task, 100*cur_errors, 100*cur_seq))\n",
      "  errors = float(low_batch) * float(errors) / batch_size\n",
      "  seq = float(low_batch) * float(seq) / batch_size\n",
      "  data.print_out(\"  %s len %d errors %.2f sequence-errors %.2f\"\n",
      "                 % (task, l, 100*errors, 100*seq))\n",
      "  return errors, seq\n",
      "\n",
      "\n",
      "def train():\n",
      "  \"\"\"Main training function.\"\"\"\n",
      "  batch_size = FLAGS.batch_size\n",
      "  tasks = FLAGS.task.split(\"-\")\n",
      "  with tf.Session() as sess:\n",
      "    model, min_length, max_length, checkpoint_dir, curriculum = initialize(sess)\n",
      "    max_cur_length = min(min_length + 3, max_length)\n",
      "    prev_acc_perp = [1000000 for _ in xrange(3)]\n",
      "    prev_sq = 1.0\n",
      "\n",
      "    while True:\n",
      "      global_step, pull, max_cur_length, learning_rate = sess.run(\n",
      "          [model.global_step, model.pull, model.cur_length, model.lr])\n",
      "      ep = global_step / FLAGS.steps_per_checkpoint\n",
      "      acc_loss, acc_total, acc_errors, acc_seq = 0.0, 0, 0, 0\n",
      "      acc_grad_norm, step_count, step_time = 0.0, 0, 0.0\n",
      "      for _ in xrange(FLAGS.steps_per_checkpoint):\n",
      "        global_step += 1\n",
      "        task = random.choice(tasks)\n",
      "        l1 = np.random.randint(max_cur_length - min_length + 1) + min_length\n",
      "        l = l1\n",
      "        if np.random.randint(10) > 3:  # Prefer longer stuff 60% of time.\n",
      "          l = np.random.randint(max_cur_length - min_length+1) + min_length\n",
      "          l = max(l, l1)\n",
      "        if np.random.randint(4) < 1:  # Mixed learning: once in a while big.\n",
      "          l = np.random.randint(max_length - min_length + 1) + min_length\n",
      "          l = max(l, l1)\n",
      "        start_time = time.time()\n",
      "        inp, target = data.get_batch(l, batch_size, True, task)\n",
      "        stepp = math.pow(global_step, -0.55)\n",
      "        noise_param = math.sqrt(stepp * 20 * prev_sq) * FLAGS.grad_noise_scale\n",
      "        loss, res, gnorm, _ = model.step(sess, inp, target, True, noise_param)\n",
      "        step_time += time.time() - start_time\n",
      "        acc_grad_norm += float(gnorm)\n",
      "        if l < max_cur_length + 1:\n",
      "          step_count += 1\n",
      "          acc_loss += loss\n",
      "          errors, total, seq = data.accuracy(inp, res, target,\n",
      "                                             batch_size, 0)\n",
      "          acc_total += total\n",
      "          acc_errors += errors\n",
      "          acc_seq += seq\n",
      "      acc_loss /= step_count\n",
      "      step_time /= FLAGS.steps_per_checkpoint\n",
      "      acc_seq = float(acc_seq) / (step_count * batch_size)\n",
      "      prev_sq = acc_seq\n",
      "      acc_errors = float(acc_errors) / acc_total if acc_total > 0 else 1.0\n",
      "      msg1 = \"ep %d st %.2f lr %.8f\" % (ep, step_time, learning_rate)\n",
      "      msg2 = \"pl %.3f cme %.3f\" % (pull, curriculum)\n",
      "      msg = (\"%s %s gn %.8f\"\n",
      "             % (msg1, msg2, acc_grad_norm / FLAGS.steps_per_checkpoint))\n",
      "      data.print_out(\"%s len %d ppx %.8f errs %.2f sq %.2f\" %\n",
      "                     (msg, max_cur_length, data.safe_exp(acc_loss),\n",
      "                      100*acc_errors, 100*acc_seq))\n",
      "      if curriculum > acc_seq:\n",
      "        prev_acc_perp.append(1000000)\n",
      "        do_incr = True\n",
      "        while do_incr and max_cur_length < max_length:\n",
      "          sess.run(model.cur_length_incr_op)\n",
      "          for t in tasks:\n",
      "            if data.train_set[t]: do_incr = False\n",
      "        if pull < 1:\n",
      "          sess.run(model.pull_incr_op)\n",
      "        else:\n",
      "          data.print_out(\"  Averaging parameters.\")\n",
      "          sess.run([model.avg_op, model.lr_decay_op])\n",
      "      else:\n",
      "        acc_perp = data.safe_exp(acc_loss)\n",
      "        if acc_perp > max(prev_acc_perp[-3:]):\n",
      "          sess.run(model.lr_decay_op)\n",
      "        prev_acc_perp.append(acc_perp)\n",
      "      checkpoint_path = os.path.join(checkpoint_dir, \"neural_gpu.ckpt\")\n",
      "      model.saver.save(sess, checkpoint_path,\n",
      "                       global_step=model.global_step)\n",
      "      # Run evaluation.\n",
      "      should_exit = True\n",
      "      bound = data.bins[-1] + 1\n",
      "      for t in tasks:\n",
      "        l = min_length\n",
      "        while l < max_length + 12 and l < bound:\n",
      "          _, sq, _ = single_test(l, model, sess, t, FLAGS.nprint, batch_size)\n",
      "          l += 1\n",
      "          while l < bound + 1 and not data.test_set[t][l]:\n",
      "            l += 1\n",
      "        if sq < 0.5:\n",
      "          _, sq = multi_test(data.forward_max, model, sess, t, FLAGS.nprint,\n",
      "                             batch_size * 4)\n",
      "        if sq > 0.001: should_exit = False\n",
      "      if should_exit:\n",
      "        if data.forward_max > 4000 and len(tasks) == 1:\n",
      "          multi_test(data.forward_max, model, sess, tasks[0], FLAGS.nprint,\n",
      "                     batch_size * 16, 0)\n",
      "\n",
      "\n",
      "def animate(l, test_data, anim_size):\n",
      "  \"\"\"Create animation for the given data (hacky matplotlib use).\"\"\"\n",
      "  xf = 12\n",
      "  fps = 2\n",
      "  fig = plt.figure(figsize=(16, 9), facecolor=\"white\")\n",
      "  ax = fig.add_axes([0, 0, 1, 1], frameon=False, zorder=2)\n",
      "  ax.set_xticks([i * 24-0.5 for i in xrange(4)])\n",
      "  ax.set_xticklabels([])\n",
      "  ax.set_yticks([i - 0.5 for i in xrange(l+1)])\n",
      "  ax.grid(which=\"major\", axis=\"both\", linestyle=\"-\", color=\"black\")\n",
      "  text_fields = []\n",
      "  text_size = 24*32/l\n",
      "  for y in xrange(l):\n",
      "    text_fields.append(ax.text(\n",
      "        11.25, y + 0.15, \"\", color=\"g\", ha=\"center\", va=\"center\",\n",
      "        bbox={\"facecolor\": \"b\", \"alpha\": 0.01, \"pad\": 24 * text_size},\n",
      "        size=text_size - (4 * 32 / l), animated=True))\n",
      "  im = ax.imshow(np.zeros_like(test_data[0][0][0]), vmin=-1.0,\n",
      "                 vmax=1.0, cmap=\"gray\", aspect=\"auto\", origin=\"upper\",\n",
      "                 interpolation=\"none\", animated=True)\n",
      "  im.set_zorder(1)\n",
      "  def to_symbol(i):\n",
      "    if i == 0: return \"\"\n",
      "    if i == 11: return \"+\"\n",
      "    if i == 12: return \"*\"\n",
      "    return str(i-1)\n",
      "  def animation_update(frame_no, test_data, xf, im, text_fields):\n",
      "    \"\"\"Update an animation frame.\"\"\"\n",
      "    steps, inpt, out_raw = test_data\n",
      "    length = len(steps)\n",
      "    batch = frame_no / (fps * (l+4*xf))\n",
      "    index = int((frame_no % (fps * (l+4*xf))) / fps)\n",
      "    # Cut output after first padding.\n",
      "    out = [out_raw[i][batch] for i in xrange(len(text_fields))]\n",
      "    if 0 in out:\n",
      "      i = out.index(0)\n",
      "      out = out[0:i] + [0 for _ in xrange(len(out) - i)]\n",
      "    # Show the state after the first frames.\n",
      "    if index >= 2*xf:\n",
      "      im.set_array(steps[min(length - 1, index - 2*xf)][batch])\n",
      "      for i, t in enumerate(text_fields):\n",
      "        if index - 2*xf < length:\n",
      "          t.set_text(\"\")\n",
      "        else:\n",
      "          t.set_text(to_symbol(out[i]))\n",
      "    else:\n",
      "      for i, t in enumerate(text_fields):\n",
      "        t.set_text(to_symbol(inpt[i][batch]) if index < xf else \"\")\n",
      "      if index < xf:\n",
      "        im.set_array(np.zeros_like(steps[0][0]))\n",
      "      else:\n",
      "        im.set_array(steps[0][batch])\n",
      "    return im,\n",
      "  animation = anim.FuncAnimation(\n",
      "      fig, animation_update, blit=True, frames=(l+4*xf)*anim_size*fps,\n",
      "      interval=500/fps, fargs=(test_data, xf, im, text_fields))\n",
      "  animation.save(\"/tmp/neural_gpu.mp4\", writer=\"mencoder\", fps=4*fps, dpi=3*80)\n",
      "\n",
      "\n",
      "def evaluate():\n",
      "  \"\"\"Evaluate an existing model.\"\"\"\n",
      "  batch_size = FLAGS.batch_size\n",
      "  tasks = FLAGS.task.split(\"-\")\n",
      "  with tf.Session() as sess:\n",
      "    model, min_length, max_length, _, _ = initialize(sess)\n",
      "    bound = data.bins[-1] + 1\n",
      "    for t in tasks:\n",
      "      l = min_length\n",
      "      while l < max_length + 12 and l < bound:\n",
      "        _, sq, _ = single_test(l, model, sess, t, FLAGS.nprint, batch_size)\n",
      "        l += 1\n",
      "        while l < bound + 1 and not data.test_set[t][l]:\n",
      "          l += 1\n",
      "      # Animate.\n",
      "      anim_size = 2\n",
      "      _, _, test_data = single_test(l, model, sess, t, 0, anim_size)\n",
      "      animate(l, test_data, anim_size)\n",
      "      # More tests.\n",
      "      _, sq = multi_test(data.forward_max, model, sess, t, FLAGS.nprint,\n",
      "                         batch_size * 4)\n",
      "    if sq < 0.01:  # More tests.\n",
      "      if data.forward_max > 4000 and len(tasks) == 1:\n",
      "        multi_test(data.forward_max, model, sess, tasks[0], FLAGS.nprint,\n",
      "                   batch_size * 64, 0)\n",
      "\n",
      "\n",
      "def interactive():\n",
      "  \"\"\"Interactively probe an existing model.\"\"\"\n",
      "  with tf.Session() as sess:\n",
      "    model, _, _, _, _ = initialize(sess)\n",
      "    sys.stdout.write(\"> \")\n",
      "    sys.stdout.flush()\n",
      "    inpt = sys.stdin.readline()\n",
      "    while inpt:\n",
      "      ids = [int(c) for c in inpt.strip()]\n",
      "      inpt, target = data.get_batch(len(ids), 1, False, \"\",\n",
      "                                    preset=(ids, [0 for _ in ids]))\n",
      "      _, res, _, _ = model.step(sess, inpt, target, False)\n",
      "      res = [np.argmax(o, axis=1) for o in res]\n",
      "      print \" \".join([str(output[0]) for output in res])\n",
      "      sys.stdout.write(\"> \")\n",
      "      sys.stdout.flush()\n",
      "      inpt = sys.stdin.readline()\n",
      "\n",
      "\n",
      "def main(_):\n",
      "  if FLAGS.mode == 0:\n",
      "    train()\n",
      "  elif FLAGS.mode == 1:\n",
      "    evaluate()\n",
      "  else:\n",
      "    interactive()\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "  tf.app.run()\n",
      "\n",
      "@@ -1,3 +1,19 @@\n",
      "+# Copyright 2015 Google Inc. All Rights Reserved.\n",
      "+#\n",
      "+# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
      "+# you may not use this file except in compliance with the License.\n",
      "+# You may obtain a copy of the License at\n",
      "+#\n",
      "+#     http://www.apache.org/licenses/LICENSE-2.0\n",
      "+#\n",
      "+# Unless required by applicable law or agreed to in writing, software\n",
      "+# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
      "+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
      "+# See the License for the specific language governing permissions and\n",
      "+# limitations under the License.\n",
      "+#\n",
      "+#==============================================================================\n",
      "+\n",
      " \"\"\"Neural GPU for Learning Algorithms.\"\"\"\n",
      " \n",
      " import math\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "seen = 0\n",
    "for commit in RepositoryMining(\"/workspace/tmp/ast_test/repositories/tensorflow@models/\").traverse_commits():\n",
    "    if seen >= 3:\n",
    "        break\n",
    "    \n",
    "    print(f\"commit #{seen + 1}, \\nmessage: {commit.msg}\")\n",
    "    for mod in commit.modifications:\n",
    "        print(f\"__Before__: \\n{mod.source_code_before}\")\n",
    "        print(f\"__After__: \\n{mod.source_code}\")\n",
    "        print(mod.diff)\n",
    "        print()\n",
    "        \n",
    "    seen += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
