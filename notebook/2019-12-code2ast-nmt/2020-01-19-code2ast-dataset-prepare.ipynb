{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code2AST dataset prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/workspace\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def name_to_url(name: str) -> str:\n",
    "    return f\"https://github.com/{name}.git\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steps to prepare a parallel dataset for code2ast:\n",
    "\n",
    "1. Make up a list of repositories to clone\n",
    "2. Clone selected repositories\n",
    "3. Parse every .py file (returning a pair of .src and .ast files) for every cloned repository\n",
    "4. Merge parsed pairs into two large files (train.src, train.ast)\n",
    "5. Remove duplicate lines in .src file along with aligned lines in .ast file\n",
    "6. Train a BPE tokenizer model on both files (model_src, model_ast)\n",
    "7. Apply tokenization for all lines in the files and filter out ones which are longer than the threshold value (512 tokens).\n",
    "(This will result in making two tokenized files with lines of a length not greater than the threshold value)\n",
    "8. Detokenize files using trained BPE models and write results to updated files\n",
    "9. Train new BPE tokenization models on updated files\n",
    "10. Tokenize updated files using new BPE models\n",
    "11. Split tokenized files into train/valid/test subsets\n",
    "12. Preprocess prepared subsets using fairseq-preprocess utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extensions = (\"src\", \"ast\")\n",
    "language = \"python\"\n",
    "language_ext = \"py\"\n",
    "library_path = \"/workspace/data/langs.so\"\n",
    "exp_name = \"/code2ast/code2ast_pretraining_mlm\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: \n",
    "Make up a list of repositories to clone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repositories = pd.read_json(\"/workspace/data/repositories/top_18k.jsonl\", lines=True)\n",
    "unique_repositories = repositories.drop_duplicates(\"full_name\")\n",
    "urls = [name_to_url(name) for name in unique_repositories[\"full_name\"]]\n",
    "urls = \"\\n\".join(urls)\n",
    "\n",
    "repo_filepath = \"/workspace/data/repo_list.txt\"\n",
    "repo_output = \"/workspace/tmp/repositories\"\n",
    "\n",
    "with open(repo_filepath, mode=\"w\") as file:\n",
    "    file.write(urls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: \n",
    "Clone selected repositories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd /workspace && python -m src.clone_repository \\\n",
    "    --repo_file $repo_filepath \\\n",
    "    --output $repo_output \\\n",
    "    --clear_before 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: \n",
    "Parse every .py file (returning a pair of .src and .ast files) for every cloned repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_dataset_path = f\"/workspace/tmp/{exp_name}_datset_parsed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd /workspace && python -m src.ast_dataset_prepare parse-nodes --rule-all \\\n",
    "    --library-path=$library_path \\\n",
    "    --language=$language \\\n",
    "    --language-ext=$language_ext \\\n",
    "    --root-input-path=$repo_output \\\n",
    "    --output-path=$parsed_dataset_path\\\n",
    "    --extensions=\"{extensions[0]}, {extensions[1]}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: \n",
    "Merge parsed pairs into two large files (train.src, train.ast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_dataset_path_prefix = f\"/workspace/tmp/{exp_name}_datset_merged/all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd /workspace && python -m src.merge_files merge-pairs \\\n",
    "    --input-path=$parsed_dataset_path \\\n",
    "    --output-prefix=$merged_dataset_path_prefix \\\n",
    "    --extensions=\"{extensions[0]}, {extensions[1]}\" \\\n",
    "    --remove-files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5:\n",
    "Remove duplicate lines in .src file along with aligned lines in .ast file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deduplicated_dataset_path =  f\"/workspace/tmp/{exp_name}_dataset_dedup\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd /workspace && python -m src.remove_duplicates \\\n",
    "    --reference-filepath={merged_dataset_path_prefix + \".src\"} \\\n",
    "    --aligned-filepath={merged_dataset_path_prefix + \".ast\"} \\\n",
    "    --destination-path=$deduplicated_dataset_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean tmp directories & files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf {os.path.dirname(merged_dataset_path_prefix)} \\\n",
    "    {parsed_dataset_path} \\"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: \n",
    "Train a BPE tokenizer model on both files (model_src, model_ast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merged_dataset_prefix = \"/workspace/tmp/ast_test/code2ast_medium/train\"\n",
    "merged_prefix = os.path.join(deduplicated_dataset_path, os.path.basename(merged_dataset_path_prefix))\n",
    "source_input_path = merged_prefix + \".\" + extensions[0]\n",
    "target_input_path = merged_prefix + \".\" + extensions[1]\n",
    "\n",
    "source_vocab_size = 32_000\n",
    "target_vocab_size = 32_000\n",
    "source_model_name = \"src_model\"\n",
    "target_model_name = \"ast_model\"\n",
    "source_model_path = os.path.join(\"/workspace\", source_model_name + \".model\")\n",
    "target_model_path = os.path.join(\"/workspace\", target_model_name + \".model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!cd /workspace && python -m src.tokenize train \\\n",
    "        --source-input-path=$source_input_path \\\n",
    "        --source-model-name=$source_model_name \\\n",
    "        --source-vocab-size=$source_vocab_size \\\n",
    "        --target-input-path=$target_input_path \\\n",
    "        --target-model-name=$target_model_name \\\n",
    "        --target-vocab-size=$target_vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: \n",
    "Apply tokenization for all lines in the files and filter out ones which are longer than the threshold value (512 tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dest_source_path = f\"/workspace/tmp/{exp_name}_dataset_tokenized/all.src\"\n",
    "dest_target_path = f\"/workspace/tmp/{exp_name}_dataset_tokenized/all.ast\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd /workspace && python -m src.tokenize tokenize-bpe \\\n",
    "        --task=code2ast \\\n",
    "        --source-model=$source_model_path \\\n",
    "        --source-path=$source_input_path \\\n",
    "        --target-model=$target_model_path \\\n",
    "        --target-path=$target_input_path \\\n",
    "        --dest-source-path=$dest_source_path \\\n",
    "        --dest-target-path=$dest_target_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 8: \n",
    "Detokenize files using trained BPE models and write results to updated files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detokenized_source_path = f\"/workspace/tmp/{exp_name}_dataset_detokenized/all.src\"\n",
    "detokenized_target_path = f\"/workspace/tmp/{exp_name}_dataset_detokenized/all.ast\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd /workspace && python -m src.tokenize detokenize-bpe \\\n",
    "        --source-model=$source_model_path \\\n",
    "        --source-path=$dest_source_path \\\n",
    "        --target-model=$target_model_path \\\n",
    "        --target-path=$dest_target_path \\\n",
    "        --dest-source-path=$detokenized_source_path \\\n",
    "        --dest-target-path=$detokenized_target_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 9: \n",
    "Train new BPE tokenization models on updated files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detokenized_source_vocab_size = 32_000\n",
    "detokenized_target_vocab_size = 32_000\n",
    "detokenized_source_model_name = \"detokenized_src_model\"\n",
    "detokenized_target_model_name = \"detokenized_ast_model\"\n",
    "detokenized_source_model_path = os.path.join(\"/workspace\", detokenized_source_model_name + \".model\")\n",
    "detokenized_target_model_path = os.path.join(\"/workspace\", detokenized_target_model_name + \".model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!cd /workspace && python -m src.tokenize train \\\n",
    "        --source-input-path=$detokenized_source_path \\\n",
    "        --source-model-name=$detokenized_source_model_name \\\n",
    "        --source-vocab-size=$detokenized_source_vocab_size \\\n",
    "        --target-input-path=$detokenized_target_path \\\n",
    "        --target-model-name=$detokenized_target_model_name \\\n",
    "        --target-vocab-size=$detokenized_target_vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 10: \n",
    "Tokenize updated files using new BPE models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepared_source_path = f\"/workspace/tmp/{exp_name}_dataset_prepared/all.src\"\n",
    "prepared_target_path = f\"/workspace/tmp/{exp_name}_dataset_prepared/all.ast\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd /workspace && python -m src.tokenize tokenize-bpe \\\n",
    "        --task=code2ast \\\n",
    "        --source-model=$detokenized_source_model_path \\\n",
    "        --source-path=$detokenized_source_path \\\n",
    "        --target-model=$detokenized_target_model_path \\\n",
    "        --target-path=$detokenized_target_path \\\n",
    "        --dest-source-path=$prepared_source_path \\\n",
    "        --dest-target-path=$prepared_target_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 11: \n",
    "Split tokenized files into train/valid/test subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepared_dataset_prefix = os.path.splitext(prepared_source_path)[0]\n",
    "splitted_dataset_path = f\"/workspace/tmp/{exp_name}_dataset_splitted\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd /workspace && python -m src.split_dataset split \\\n",
    "            --dataset_prefix=$prepared_dataset_prefix \\\n",
    "            --exts=\".{extensions[0]}, .{extensions[1]}\" \\\n",
    "            --split-ratio='0.8, 0.15, 0.05' \\\n",
    "            --dest-path=$splitted_dataset_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 12: \n",
    "Preprocess prepared subsets using fairseq-preprocess utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!apt-get update \\\n",
    "    && apt-get -y install build-essential \\\n",
    "    && pip install fairseq sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pref = os.path.join(splitted_dataset_path, \"train\")\n",
    "valid_pref = os.path.join(splitted_dataset_path, \"valid\")\n",
    "test_pref = os.path.join(splitted_dataset_path, \"test\")\n",
    "preprocessed_path = f\"/workspace/tmp/{exp_name}_dataset_splitted.src-ast\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf $preprocessed_path && fairseq-preprocess \\\n",
    "    --source-lang src --target-lang ast \\\n",
    "    --trainpref $train_pref \\\n",
    "    --validpref $valid_pref \\\n",
    "    --testpref $test_pref \\\n",
    "    --destdir $preprocessed_path \\\n",
    "    --nwordssrc 32000 --nwordstgt 32000 \\\n",
    "    --bpe sentencepiece \\\n",
    "    --workers 60"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clear temp paths and BPE models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf {os.path.dirname(dest_source_path)} \\\n",
    "    {os.path.dirname(detokenized_source_path)} \\\n",
    "    {os.path.dirname(prepared_source_path)} \\\n",
    "    {splitted_dataset_path} \\\n",
    "    {deduplicated_dataset_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf {os.path.splitext(source_model_path)[0] + \".*\"} \\\n",
    "    {os.path.splitext(target_model_path)[0] + \".*\"} \\\n",
    "    {os.path.splitext(detokenized_source_model_path)[0] + \".*\"} \\\n",
    "    {os.path.splitext(detokenized_target_model_path)[0] + \".*\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
