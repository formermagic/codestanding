#!/bin/bash

# TODO: 
# 1) Train a sentencepiece model on a shared dataset
# 2) Tokenize dataset with the trained model